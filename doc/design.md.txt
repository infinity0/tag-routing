% Design and architecture
% Ximin Luo

# Introduction

## Background

Searching for information is an essential component of any network. Without it,
there might as well not be a network in the first place.

The world wide web is the largest information network ever created; currently,
search is a service; providers employ crawlers to navigate this network, and
extract and summarise information from the documents visited, into a form which
is suitable for insertion into a database. "Searching the web" usually means
querying these pre-built databases, rather than dynamically routing your way
through the web's content.

Preparing and maintaining the database is extremely resource-intensive for a
large network, which results in high barriers to entry. The utility of a search
service increases with how much of the network it covers; new providers must
recreate a database of comparable size before clients will switch, or else use
indexing algorithms that produce decidedly better results than alternatives.

This creates conditions of oligopoly, which is inherently vulnerable to both
abuse and attack. Large providers are trusted by a great number of clients, so
more people are affected when this trust is broken. We have already seen cases
of providers censoring their search results, both voluntarily[^censor1] and
under coercion[^censor2]. Privacy is also a concern: providers can monitor
client usage of the service, and build up a profile of personal activities.

[^censor1]: Google stopped returning results to CNET's website, because they
published an article that the CEO disliked. [REF]

[^censor2]: Google was also forced by the Chinese government to censor many
search results from its service in China. [REF]

Another issue is the depth and granularity of search topics. Most of us don't
use a search provider for every item of information we need; instead, we often
issue a query that gives us a selection of related sites from all over the web,
then manually browse within these sites to target our needs more precisely. In
addition, some websites have non-public information, or specialist knowledge
that generic search algorithms aren't able to index effectively.

An alternative approach is to perform dynamic routing using query semantics.
Instead of a simple client-provider model where a single query is a single
transaction, we propose a co-operative model where queries are routed between
autonomous providers, and results aggregated for the end user. Small providers
can index their own local sections of the network, and access and results can
be fine-tuned using local information. In addition, clients will have a wider
choice of who to trust.

Admittedly, decentralised systems have their own issues, and these are briefly
discussed [below](#potential-issues). However, unlike the inherent problems of
centralisation, these are open research questions. Once a problem is solved,
future generations may reap the benefits without having to expend the initial
development cost.

Imagine your browser acting like a router; you type in a search query and it
automatically follows links between pages to reach what you want. Of course,
this is a long way off, and it may well be beyond the capabilities of current
hardware and networks, but hopefully this project makes a useful contribution
in that direction.

## Related systems

There are many existing systems which use decentralised, co-operative, dynamic
routing algorithms, such as the internet and various peer-to-peer overlay
networks. There are a great variety of different objectives, approaches, and
models, but some common themes include:

Key-based routing
:	The network defines an address space and a distance metric, where each
	address is represented by a binary key. (DHTs, Freenet, GNUnet)
Mesh networks
:	Various heuristics are used to maintain structure and performance, such as
	random walks, bandwidth detection, index delegation, etc. (Gnutella)
Social relationships
:	Nodes prefer to peer with trusted friends. This provides better security
	properties, and a more predictable network structure. (OneSwarm, Freenet)

A more comprehensive survey of peer-to-peer search systems is presented in
[^surv1]. Using the terminology of that paper, this project develops and
evaluates a _probability-based model_ of information retrieval intended to
support _comprehensive keyword search_[^surv2].

[^surv1]: John Risson, Tim Moors, Survey of research towards robust
peer-to-peer networks: Search methods, _Computer Networks_, **Volume 50**,
**Issue 17**, 5 December 2006, Pages 3485-3521, ISSN 1389-1286, DOI:
[10.1016/j.comnet.2006.02.001](http://www.sciencedirect.com/science/article/B6VRG-4JD0XYW-1/2/07e1ec0ba8cbe65f8f094cd99612b149).

[^surv2]: as opposed to _partial keyword search_

There is some existing research into p2p semantic search systems, eg. [^sem1]
[^sem2]. However, none of these are currently deployed in real systems, and we
were unaware of them during the initial stages of our project. A discussion of
these systems and how they relate to our project is given in sections further
below, after our system has been described first. [MORE]

[^sem1]: [REF] REMINDIN'
[^sem2]: [REF] Harnessing.


# Preparation

## Objectives

We intend to build a system that can at least offer similar functionality to
existing search providers, if not as sophisticated or efficient. Core aspects
of this functionality include:

Semantics
:	Query subjects have semantic relevance to the results, so the routing
	algorithm and address scheme must reflect this.
Reach
:	It should be feasible to locate all the data matching a given query on the
	entire network (or connected component).
Robustness
:	Query paths and returned results should be resistant against subversion,
	such as spam floods or data poisoning.

Most major currently-deployed systems have at least one incompatibility with
the above. For example, DHTs are scalable, and will reach data if it exists on
the network, but addresses have no relation to the semantics of the data. Many
mesh networks (eg. Gnutella) can perform keyword search, but do not attempt to
reach all relevant data on the network.

## Potential issues

_Here, we briefly discuss issues surrounding a decentralised architecture, and
present ways of addressing these issues, or else explain why we think they are
not a significant problem in the context of this project._

A system which must route queries between autonomous providers will obviously
be slower than a system that only needs to query a single provider. At present,
this is noticeable and signficiant - eg. Google returns results for the entire
web almost instantaneously, whereas DHT queries on a medium-sized network might
take a minute to complete. However, systems only get better, not worse, and
hardware will improve in the years to come. By the time research in this area
is well-developed, it's entirely possible that performance will have improved
beyond the limit of human perceptibility.

Existing large service providers might have little incentive to participate in
a co-operatiave search system, since they are each competing for control over
the market. However, this is less of a factor for smaller providers who would
otherwise be unable to attract many users, since their intention is only to
provide search capabilities. In principle, this is no different from displaying
links to other websites to help your visitors find what they want - linking to
useful content increases your own utility, even if this is not reciprocated.

It is widely thought that decentralised systems are hard to design to be both
secure and scalable, but nothing indicates that this impossible. Recent
approaches based on social networks have been promising.[MORE]

## Initial observations

Our first ideas drew upon our experiences and pre-conceptions on how we humans
try to locate things. Two themes stand out:

- keeping knowledge on who knows what, and use this to direct query routes.
- shifting the query subject to increase recall or precision - eg. broadening
  increases the recall, and re-specialising increases the precision

We also drew from our existing background knowledge on decentralised storage
networks, such as Freenet and other Distributed Hash Tables. DHTs are very
efficient and scalable, most systems giving O(log n) performance[^dhtperf] in
the size of the network. These generally have a well-defined address space with
a distance metric, where each address can be represented as a binary key. This
allows for fairly simple, yet effective, routing algorithms.

[^dhtperf]: eg. expected number of hops for a lookup

A basic reason for this is that a numerical address space can be partitioned
hierarchically, eg. by taking successively larger prefixes of an address. This
allows greedy routing to work effectively, ie. by finding the neighbour which
shares the smallest partition with the desired target node (in IP routing, this
is just "longest-prefix-match", which we are all familiar with).

[DIAG] diagram of hierarchical partitioning, [REF] kleinberg "small world"

Two of these ideas seemed to fit well together - if we made the query-shifting
aspect of "human" routing more precise, by using the idea of a structured
address space, then this could also allow for a simple routing algorithm.
Semantic tags are not as naturally structured as numerical addresses however,
and a significant part of our time would be spent in developing a theoretical
model of a partitionable and navigable space over tags.

We did not have any specific initial ideas on how to ensure the global reach of
a search request. However, our DHT-inspired approach of using a global address
scheme should allow further development in this area. (If the relationship from
addresses to objects is deterministic, a global address scheme implies global
reachability, but our final design does not satisfy the former requirement.)

Research has shown that analysing social network structure can be effective in
resisting malicious information. A major reason is because social relationships
are much more expensive to attack than simple naive indexing algorithms, such
as TF-IDF. Existing examples include Google's PageRank algorithm[REF], which
measures a node's importance; trust metrics such as Advogato[REF]; and sybil
detection algorithms such as SybilInfer[REF]. [MORE] restructure

## Working assumptions

Since the aim of the project is fairly ambitious, we want to repeat as little
work as possible. Therefore, we make various assumptions about the environment
that our system will run under. Some of these are reasonable, and some of these
are fairly restrictive; however, we feel they are prudent and necessary in the
context of this project.

Locating known objects is essentially a solved problem: the internet and DHTs
both offer ways to retrieve objects based on a globally unique address. Without
this primitive we cannot continue at all; data has to be stored somewhere, and
it has to be accessible to an arbitrary subset of the network.

Additionally, we assume that objects are always available. In a real network,
this can be implemented with proxy services (eg. servers which hire storage
space out and guarantee continuous access to this), or it might be a property
of the storage network (eg. DHTs). This simplifies our design, since it allows
us to avoid dealing with the issue of churn, by delegating it to an independent
external component.

This also forces us to consider an iterative routing algorithm rather than a
recursive one. Since data cannot forward queries onto other pieces of data, a
lient must process it themselves, determine which objects to retrieve next,
then retrieve those, etc. Hopefully, our design can be adapted into a recursive
one without much effort, but we won't give special consideration to this.

- style of query, keyword search ONLY [MORE]
- nothing more complex, eg. intersections [MORE]

# Theory

_In this chapter we discuss the theoretical ideas behind the system. We
experimented with many different ideas before the code development began, and
during development we weeded out the ones which seemed to lead to a dead end.
Afterwards, the surviving ideas were explored further, and simplified or
generalised.[^simp]_

As such, there are a few aspects of our system design which may seem inelegant
or imperfect in the context of the theories described below. Unfortunatly, we
didn't have enough time to go back and refine them; but we have tried to point
these out in the appropriate parts of the architecture section. None of these
flaws, we think, are serious enough to defeat the basic purpose of the design
that we implemented.

[^simp]: For example, originally the problem of information aggregation was
considered only for the naming and routing layers, and was completely ignored
for the social layer. During implementation it was realised that trust metrics
could not resolve conflicts between `ptable` entries, and after implementation
we formulated the theoretical expression of the general problem, given below.
The discussion on the various issues of aggregation as related to our design
(eg. [LINK]) was actually done during the preparation stage, but it was only
afterwards that we realised how this fit into the general theory.

## System design

_This section describes the overall concepts of our system, and the reasoning
for our design choices. More precise details, such as the exact contents of the
various objects introduced, and how the search algorithm processes these
objects, are given in the [architecture](#architecture) section._

We start off with the analogy of an index being a routing table. Tags are
addresses, documents are hosts, and routes are semantic relationships between
a tag (address) and a document (host). Scalable routing tables must compact
information together. Instead of defining a route to every single host on the
network, it is divided up into subnets, and a single route defined for the
entire subnet. Similarly, indexes shouldn't point to other indexes using all
possible relevant tags, but it should seek to summarise this information.
Instead of listing an index under entries for several dozen instances of (eg.)
strategy games, it could instead list only for the tag "strategy games".

A clear and consistent model for this is developed in [structured address
space](#structured-address-space). From that, we get another type of object, a
tag graph, or tgraph, which defines relationships between tags. Following the
principle of compacting information again, we need some way to route between
these tgraphs. At this point, you might worry that we'll continue with this
ad-infinitum, inventing further layers of meta-objects; but no we stop here,
and actually we use tgraphs to route between themselves. That is, tgraphs also
use tags for addresses, and list related tgraphs under entries for appropriate
tags.

To explain why this isn't a problem:

- The set of documents is much larger than the set of semantically distinct
  tags. Therefore, a map of tags to related _tags_ gives us more information
  about the entire network, than a (similarly-sized) map of tags to related
  _documents_.
- More people are likely to have an idea of what a tag's related tags are,
  since this is part of everyday communication. So a neighbourhood of tgraphs
  will cover a larger section of the network.
- The previous observation also applies to the initiator - most queries are
  made with some understanding of what the tag means. In the worst case, the
  system can just ask the user to supply their own "related tags".

This principle can be summed up as "language unites". Just as the ubiquity of
mathematics allows numerical addresses to be routed over, we exploit the
semi-ubiquity of language to allow tags to be routed over.

Since we are routing between documents, rather than people or hosts, we split
the social network from the indexes network and the tgraphs network. This gives
us several advantages. It allows for space reductions - people often agree
about things, so identical parts of indexes can be spun off into another index
and pointed to instead of repeated. This model is also more general; anything
that applies to it must also apply to the case of one-index-per-person. It's
also arguably more realistic, since we often agree with someone's judgements,
without trusting them as friends.

Each identity on the social network defines a preference table, or ptable,
listing the indexes and tgraphs that it values highly, and other identities
whom it trusts. By exploring this network, we can infer a set of trusted
objects from which to initiate our search.

### Mutability of objects

[MORE] immutable data structures, etc
git submodules vs. URL links

We have implicitly considered objects to be immutable. For example, we intended
the sharing of index objects via ptables to be a space-saving measure, not an
act of delegation. However, this is not enforced by our system, which may
become an issue when a real implementation is deployed.

eg. on freenet, darknet is supposed to be an f2f network, but during the early
stages of the network people added strangers as "friends" in the belief that it
would increase network performance.

### Data collection

[MORE] our project only considers how routing is to be done, it doesn't
consider how the routing tables would actually be created and maintained.
However, model is fairly simple, etc etc [EXTN]

## Information aggregation

Trust metrics and sybil detection algorithms are useful, but cannot support
applications that must obtain value judgements _about resources_ from other
agents. Define:

Agent
:	an entity which can give (and receive) score judgements
Resource
:	an entity which can only receive value judgements

Each agent is associated with two maps: an agent-score map $\{ a : s \}$, and
a resource-value map $\{ r : v \}$, which define their subjective judgements.
The problem of information aggregation can then be formulated as:

Given
:	- a set of agents, with their agent-score and resource-value maps:
	  $$
	  a : (\{ a : s \} , \{ r : v \})
	  $$
	- a seed agent-score map:
	  $$
	  a : s
	  $$
Return
:	- a target resource-value map, aggregated from the given information
	  $$
	  r : v
	  $$

This can be represented as a table. [DIAG] cols = resources, rows = agents

This is a generalisation of the idea of trust metrics, which has no conception
of resources. Most trust metrics instead return an aggregated agent-score map,
with an implicit seed agent-score map of the form $\{ a_0 : s_{max} \}$ (i.e. a
single root agent mapped to the maximum score).

### Simplification

The above formulation of the problem can be usefully divided up into two
components:

**1. Infer scores for all agents**:

Given
:	- a set of agents, with their agent-score maps
	- a seed agent-score map
Return
:	- a target agent-score map, that covers all agents in the input

and

**2. Compose values for all resources, from the scored agents**:

Given
:	- a set of agents, with their inferred score, and resource-value maps
Return
:	- a target resource-value map

We follow this approach in our project. Note however, that even these simpler
problems are fairly deep, and developing robust techniques to solve them could
take up a whole other project. Our prototype implemenation only attempts naive
and simple solutions to each component. [EXTN]

### Use within our design

In the context of our system design:

---------------------------------------------------------------------------
layer       agent                           resource
----------- ------------------------------- -------------------------------
social      identities and their ptables    indexes, tgraphs

naming      tgraphs                         tags, tag-tag relationships

routing     indexes                         tag-document relationships
---------------------------------------------------------------------------

[MORE] discussion here, discrepancy and score/value being mixed up

One can see here that indexes and tgraphs take the position of both agent and
resource. This affects the theoretical model for inferring the scores of those
objects.

[MOVE] "links between objects" here

### Attributes

So far, we haven't discussed what the attributes should actually represent. In
any usage scenario, they should at least satisfy the following properties:

- The attribute should be some well-defined property that can be measured or
  estimated. Any single value of the attribute should mean the same thing to
  all agents, rather than being left up to agents' own interpretation.
- Attribute values should not be treated as authoritative, since in our model
  we only ever receive data from agents, whom we assume to have a limited view
  of the entire network. There will always be inconsistency between different
  agents; the system should account for this.

In the case of perfectly honest agents, it is hoped that inconsistencies will
work to cancel each other out. In the case of a system under attack, this will
not be the case. However, only looking at the received values for attributes
cannot distinguish between an attack and actual value differences; so this must
be detected in some other way. Our project won't present any methods for doing
this, but network structure analysis is a common approach type.[EXTN]

The exact forms that we picked for our attributes are discussed elsewhere.[LINK]
Generally, we use probability-based attributes, which are well-defined and can
be estimated by agents. It also gives a simple way of combining attributes -
multiplication - as well as a simple space of values - $[0, 1]$.

## Structured address space

tgraph semantics, and discussion about it.


# Architecture

## Data structures

### `index`

DIAG

Structure
:	$$
	h = \left[ \begin{array}{lrr} \\
	  E_h \subseteq & T \times D \cup H & \to W \\
	\end{array} \right]
	$$
Description
:	A map of arcs to their weights, where each arc is a relation from a tag to
	a document or another `index`.
Semantics
:	Probability that the arc target will satisfy a query for the source tag.

This data structure contains information on how to satisfy a search request. It
contains mappings from tags to target documents, or to another index to forward
the request onto.

A mapping indicates some semantic relevance between the tag and the document,
and a subjective judgement that a person searching for the tag did intend to
retrieve information provided by the document.

### `tgraph`

DIAG

Structure
:	$$
	g = \left[ \begin{array}{lrr} \\
	  V_g \subseteq & T \cup G & \to W \\
	  E_g \subseteq & T \times T \cup G & \to W \\
	\end{array} \right]
	$$
Description
:	1. A map of nodes to their weights, where each node is a tag; and
	2. A map of arcs to their weights, where each arc is a relation from a tag
	to another tag or `tgraph`.
Semantics
:	1. Saturation of the tag. (see [`tgraph` semantics](#tgraph-semantics))
	2. Relevance of the target tag to the source tag.

This data structure provides information on tags and the relationships between
tags. The presence of a tag indicates an understanding of its meaning; an arc
from a tag to a tag or `tgraph` indicates a semantic relationship between them,
and a judgement that the target can be used in some appropriate way to satisfy
a query for the source tag.

### `ptable`

DIAG

Structure
:	$$
	p = \left[ \begin{array}{lrr} \\
	  V_p \subseteq & G \cup H & \to W \\
	\end{array} \right]
	$$
Description
:	A map of nodes to their weights, where each node is a `tgraph` or an
	`index`. Implicitly, each `ptable` "belongs" to some identity on the
	[underlying social network](#underlying-social-network).
Semantics
:	Probability that a ... TODO

This data structure allows an identity to declare a personal list of preferred
indexes and tgraphs to use for routing purposes.

This is intended to provide an attack-resistant "bootstrapping" path onto the
routing and naming planes.

## Abstraction layers

Since network latency is far greater than processing latency, we design these
algorithms to be inherently concurrent. Operations involving retrieval of
remote objects are non-blocking and allow other work to be done whilst waiting
for its results.

The system is partitioned into several layers, each dependent on the one below
it. These layers (contact, naming, routing) correspond to the data planes
introduced previously.

Each layer follows a basic skeletal structure:

- A set of data sources (agents and their judgements). We allow for these being
  large pieces of data, and so follow a model where only the parts relevant to
  the query are loaded - the "local view" of the data source. [MORE] maybe

- As input from lower layers, a seed map of data sources to their scores. For
  the bottom layer (contact), this is implicitly $\{ a_0 : s_{max} \}$[^layin].
  Other input might also be needed, depending on the specific layer.

- An algorithm for reaching more agents and resources. Generally, this is done
  by traversing out-arcs, starting with our seed data sources and the query
  subject tag. The specific choice of which paths to follow first, depends on
  both the layer and the implementation.

- As output to upper layers, a resource-value map, or an object that is built
  from such a map (in which case the map itself might not be made visible).

- Various algorithmic components for constructing this output. These include at
  least a score-inferer and a value-composer (see [information aggregation -
  simplification](#simplification)). These components are independent from the
  layer itself, and may be replaced with more sophisticated components.

Upper layers can request more input data from lower layers, if needed[^layex].
The top layer (routing) receives requests from the user, and the bottom layer
(contact) uses data from the social network only, independently of any query.
Note that data retrieval can in theory proceed until the entire network has
been searched. In practice, we only automatically send requests for more data
until a set number of results have been returned; after that we wait for user
input before retrieving more.

The interaction between layers is co-ordinated by implementing each layer as a
state machine - it cannot receive requests during a state transition (ie. when
it is currently processing a previous request). This simplifies the execution
logic, and reduces the number of critical objects that need to be synchronised
on. The state machines generally follow the pattern of `[NEW --> AWAIT_INPUT
<-> IDLE]`; specific cases are discussed in the _traversing objects_ sections
of the various layers, below.

[^layin]: as noted in [information aggregation](#information-aggregation)

[^layex]: eg. if all available seed indexes have been exhausted, and nothing
useful returned, we need to ask for more.

To give an overview of the layers and how they fit together, below is a table
of the inputs and outputs of each layer:

-------------------------------------------------------------------------------
layer           requires                        provides
--------------- ------------------------------- -------------------------------
routing         seed indexes $H_s$              results table $\breveh$
                address scheme $\breveT$

naming          seed tgraphs $G_s$              $\breveT$
                query subject tag $t_0$

contact         own identity $z_0$              $G_s$, $H_s$
-------------------------------------------------------------------------------

Table: data dependency between layers [DIAG] pandoc table formatting not-so-good here

### Contact

Requires
:	- implicit $z_0$, from the user
Provides
:	- $G_s$ to [naming](#naming)
	- $H_s$ to [routing](#routing)

#### Aggregating information

Agent
:	social identities
Resource
:	tgraphs, indexes
Seed map
:	the input seed identity $z_0$ mapped to the maximum score.
Score-inferer
:	just a trust metric [MORE]
	Inferring attacks from the link structure of a social network is outside the
	scope of this project. More advanced algorithms exist; a real implementation of
	this system would be able to use the latest available techniques. [REF][EXTN]
Value-composer
:	uses mean value-composer; see "practical details" [LINK]

#### Constructing output

We have two outputs to send to two different layers; we just split up our
aggregate resource-value map by object type.

#### Traversing objects

This is done by the trust metric algorithm. [MORE][REF][EXTN]

### Naming

Requires
:	- $t_0$ from the user
	- $G_s$ from [contact](#contact)
Provides
:	- $\breveT$ to [routing](#routing)

#### Aggregating information

Agent
:	tgraphs
Resource
:	tags (nodes), tag-tag relationships (arcs)
Seed map
:	the input map $G_s$
Score-inferer
:	see [object-object links](#object-object-links) [MORE]
Value-composer
:	uses mean value-composer; see "practical details" [LINK]

#### Constructing output

We have two outputs to send to two different layers; we just split up our
aggregate resource-value map by object type.

[Rewrite]
:	Our prototype uses Dijkstra's algorithm. Each step $n$ adds a node to
	$\breveT$; for this, we need to know the distances to all the out-nodes of
	the shortest-path tree so far, ie. $t_i$ must be complete in $\breveg$ for
	all $i < n$. TODO could split this into another module, allowing other
	methods of ranking to be used.

	1. If we reach a $t$ for which we don't have the data to continue, we pause
	   the algorithm, and give the routing layer the option to request that we
	   add another tag to $\breveT$ (by completing $t$ and continuing).
	2. For each `tgraph` node $g$ in $\breveT$, we give the routing layer the
	   option to request $g$ be added as a data source.

Distance metric

Given
:	- an arc $e = (v_s, v_t)$, and its weight $w_e$
	- a node $v_s$, and its weight $u_s$
	- a node $v_t$, and its weight $u_t$
Return
:	- a "distance" metric $d(v_s, v_t)$.

The distance metric should give some indication of how much $v_s$, $v_t$ are
related to each other, with lower values representing a closer relationship.
Furthermore, this should be additive, meaning that given $e_1 = (v_0, v_1)$,
$e_2 = (v_1, v_2)$, and no further information (ie. assuming that $(e_2, v_0)$,
$(e_1, v_2)$ are pairwise independent), then $d(v_0, v_2) = d(e_0) + d(e_1)$.

see [`tgraph` semantics](#tgraph-semantics) - want to maximise $u_t.w_e$, etc

so for any $e = (t_0, t)$, let $d(e) = -\log(w_e.u_t)$. MORE pad this out

#### Traversing objects

see "practical details" [LINK]

### Routing

Requires
:	- $H_s$ from [contact](#contact)
	- $\breveT$ from [naming](#naming)
Provides
:	- $\breveh$ to the user

#### Aggregating information

Agent
:	indexes
Resource
:	tag-document relationships (arcs)
Seed map
:	the input map $H_s$
Score-inferer
:	see [object-object links](#object-object-links) [MORE]
Value-composer
:	uses mean value-composer; see "practical details" [LINK]

#### Constructing output

ranking...

Given
:	- $\breveg = \{ t \mapsto w \}$
	- $\breveH = \{ h \mapsto w \}$
	- $\breveR = \{ h \mapsto \{ t \mapsto \{ d \mapsto w \} \} \}$
Return
:	- $\breveh = \{(d, w_d)\}$, a results map of documents to their weights.

TODO

#### Traversing objects

see "practical details" [LINK]


# Practical details

## System components

### Using the naive value-composer

#### On ptables

Define $G_s = \dom p_s \cap G$ and $H_s = \dom p_s \cap H$. (Note that $G_s +
H_s = \dom p_s$).

A basic implementation is just to calculate the score-weighted average of the
weight of each node. Then, to generate $p_s[v]$ for any $v \in \bigcup \dom p$,
we just pass $P_s$, $v$ and some appropriate $\alpha_1 : P \times V \to [0, 1]$
through the [general algorithm](#mean-weight-of-a-node).

To recap, $\alpha_1(p, v)$ is the probability of $v \notin \dom p$ being due to
"zero-weight". In other words, $p$ does know about $v$ and has judged it to be
worthless. We need to make a rough estimate for $\alpha_1$.

Various factors that could affect $\alpha_1$ include:

A.  Maintainers of larger `ptable`s are likely to have come across more objects
    than those of smaller `ptable`s, and so $\alpha_1$ will be larger.
B.  The more of our friends that know about a particular item, the more likely
    it is that we also know about it. The probability is proportionally greater
    at lower saturations, since it only takes one friend to pass on knowledge.
C.  On the other hand, we might choose _not_ point to an item that our friends
    already point to, even if we like it. To do so would be somewhat redundant;
    we could just get the information from our friends instead.

It is not immediately obvious how these factors interact with each other, and
how they affect the final $\alpha_1$. Even if we can produce a global estimate,
it is uncertain whether it would actually be accurate or useful when applied to
an individual's local situation. [EXTN]

Also, any estimate depending on social factors (eg. (B), (C)) would require
retrieving the `ptable`s of all the friends of $p$, which is more costly than
estimates based on an individual `ptable`.

Selecting trustworthy nodes from a social network is a deeper and more general
problem which would provide resistance to attacks in this layer. With a good
algorithm, $\alpha_1$ could then be reduced to near-zero.

Due to these reasons, for our prototype we simply use a low constant, $2^{-4}$.
This is **not** theoretically sound, but should work adequately in the majority
of cases, where there are few disagreements or malicious attacks.

#### On tgraphs

Given
:	- $\breveG$
	- a node $v$, or an arc $e$
Return
:	- the combined weight $w_v$ or $w_e$, to be used in $\breveg$.

A basic implementation is just to calculate the score-weighted average value of
$v$ or $t$.

TODO semantics of tag-tgraph arcs

There are two cases to consider:

Weight of a node $v$
:	With `tgraph`s, the absolute weight of a single node does not determine
	its usefulness to the naming layer, but the "distance" between that node
	and the given query tag $t_0$. To this end, `tgraph`s are *supposed* to
	include any tag it knows about, including low-weight ones. Therefore, we
	assume that the absence of a tag *always* means "missing information", and
	let $\alpha_1(g, v) = 0$.

	We need to justify that the distribution of weight estimates is roughly
	even. TODO (see incorporate notes from below)

Weight of an arc $e$
:	We use the standard [mean weight](#mean-weight-of-an-arc) algorithm. This
	requires us to provide an $\alpha_1, \alpha_2$; these take low, high values
	respectively - see the [analysis](#zero-weight-attributes) for details.

	We need to justify that the distribution of weight estimates is roughly
	even. TODO

For node-weights, the "important" thing is not the value itself, but rather
where the most significant bit occurs. In other words, the outcome-entropy of
(D tagged by v), equal to -log(P(D tagged by v)).

When we combine saturations from various sources, then I think it's more
"correct" to take the mean of the entropy, and not the saturation. I'm not sure
how to justify this rigorously, but intuitively:

- if we have 3 sources giving saturations of (1, epsilon, epsilon), the mean
saturation is 1/3, whereas the actual value is likely to be nearer epsilon.

Is there a more formal way of saying this?

Also, "mean entropy" is theoretically open to manipulation, since entropy can
go all the way to infinity. But I don't think this is too much of a problem,
because:

- there is no incentive to manipulate these node-weights, since a tag must be
well-related to other tags for the search algorithm to be affected; and
arc-weights are not susceptible to this kind of manipulation
- CPUs practically have finite-size representations of numbers, which limits
the entropy to be proportional to the width (in bits) of the representation.
Also, we could do this manually.

#### On indexes

...[MORE]

### Traversing objects

#### Naming layer

[REWRITE] this section

$\breveG_* = \{ g \mapsto \ddotg \}$
:	- a map of **data sources**, to the local view of each source.
	- Initialised with the keys of $G_s$, each mapped to an empty local view.
	- For any $g$, let $t$ is _complete_ in $\ddotg$ mean that enough data has
	  been retrieved to calculate any [distance](#distance-between-tags) metric
	  between $t$ and any of its out-nodes - ie. the weights of (itself, its
	  out-arcs, and its out-nodes). `tgraph` nodes have no out-arcs, so this
	  will just be its own weight.
	- Let $t$ is _complete_ in $\breveG_*$ mean that $t$ is complete in all
	  $\ddotg \in \img \breveG_*$.
	- Let $\com \breveG_*$ refer to the largest set of nodes for which every
	  node is complete in $\breveG_*$. This contains enough information to
	  generate the theoretically largest $\breveT$ for the given $\breveG_*$,
	  without waiting for more data from the network, or designing some extra
	  workaround for the missing data.
$\breveG = \{ g \mapsto w_g \}$
:	- a map of **data sources** to their [scores](#weight-of-new-tgraph).
	- Initially empty.
$\breveg \in G$
:	- a **combined `tgraph`**, [constructed](#combining-tgraph-objects) from
	  $\breveG_*$.
	- Initialised with $t_0$ having weight 1. This is just a temporary dummy
	  value to allow the routing layer to do an initial scan using just the
	  root tag, when naming information is still unavailable.
	- This holds all information related to $\com \breveG_*$; except for arcs
	  to `tgraph`s already being used as a data source (ie. all $g \in \dom
	  \breveG$).
$\breveT \subseteq \breveg$
:	- an **acyclic subgraph** of $\breveg$, representing a **routing scheme**.
	- Initially empty.
	- Nodes are ranked in order of its shortest distance to $t_0$, and each
	  node only has in-arcs from nodes nearer to $t_0$ than itself.
	  (TODO could rank these arcs too..)
	- This provides a quick way to obtain the set of short (ie. greedy) paths
	  from $t$ back to $t_0$. MORE explain better

Implementations may merge $(\breveG_*, \breveG)$ and $(\breveg, \breveT)$ into
the same objects; however, they should ensure that only the parts specified
here are exported to other layers.

DIAG data flow diagram

Receiving data from lower layers

:	We start from our seed `tgraph` set $G_s$. If $t_0$ does not appear in $G_s$,
	we have a few backup options:

	- ask the user to supply some related tags. This should not be a significant
	  problem; people usually "have an idea" of what they are looking for.
	- ask more nodes in the social graph for their `ptable`s. This could be done
	  automatically, which makes things more convenient for a user. However, it
	  increases the risk of an attack through the social network.

	TODO rewrite this so it's more relevant to the new layer-based architecture.

Evt+Async: $\breveG_* \to \breveG$

:	The events that can trigger this are:

	0. After receiving $G_s$ from [contact](#contact), in which case $G_s$ is
	   just copied to $\breveG$.
	1. A request from [routing](#routing) to add a tag to $\breveT$, in which
	   case we complete the last element of $\breveT$ in $\breveG_*$, and
	   continue our calculation of $\breveT$ (see below).
	2. A request from [routing](#routing) to add `tgraph` $g \in \breveT$ as
	   a data source, in which case we add $g$ to $\dom \breveG_*$, and
	   retrieve enough data to maintain $\com \breveG_*$.

	In each case, additional data may be need to be retrieved from the network.
	When this is complete, the updates can be pushed atomically (from the other
	layers' perspective) to $\breveG$, $\breveg$, $\breveT$.

	**(3 only)** This may require first completing $g$ in $\breveG_*$, if the
	weighting algorithm needs such data. Since this is costly and our prototype
	does not need it, we omit this step.

	**(3 only)** This may cause some $t \in \com \breveG_*$ to no longer be
	part of the shortest-path tree, and hence $\breveT$, which will cause
	future additions to retrieve unnecessary data from the network. In theory,
	we could check this and prune them; however this would be quite complex and
	should be rarely needed, so we ignore it in our prototype.

Sync: $\breveG \to^{\breveG_*} \breveg$

:	For each tag $t$ in $\com \breveG_*$, we calculate the combined weights for
	all of its out-arcs and out-nodes (from every data source), ignoring the
	`tgraph` nodes that have already been added to $\dom \breveG$ (so that they
	are not added to $\breveT$ and be made available for addition again).

Sync: $\breveg \to \breveT$

	(The numberings correspond to the "event trigger" specs above.)

#### Routing layer

$\breveH_* = \{ h \mapsto \ddoth_H \}$
:	- a map of **data sources** to a partial local view of each source.
	- Initialised with the keys of $H_s$, each mapped to an empty local view.
	- This only holds arcs to other `index`s; arcs to documents are stored in
	  $\breveR$ (see below).^[Contrast this with $\breveG_*$ from naming, which
	  holds all retrieved data, due to the different structure of a `tgraph`.]
	- TODO explore and explain the reasons why we have a $\com \breveG_*$ but
	  not a $\com \breveH_*$.
$\breveH = \{ h \mapsto w_h \}$
:	- a map of **data sources** to their [scores](#weight-of-new-index).
	- Initially empty.
$\breveQ = \{ h \mapsto T_h \}$
:	- a map of **data sources** to sets of tags, representing the currently
	  **active lookup** operations.
	- Initially empty.
	- Here, we distinguish between the _query_ for tag $t_0$, which is the
	  objective of this entire process , and the simpler operation _lookup_,
	  which simply returns the results for a tag in a single `index`.
$\breveR = \{ h \mapsto \ddoth_D \}$
:	- a map of **data sources** to a partial local view of each source,
	  representing the **results** of lookup operations.
	- Initially empty.
	- This holds the entire local view, except for arcs to `index`s already
	  used as a data source (ie. all $h \in \dom \breveH$).
	- Let $\breveD = \dst \bigcup \ddoth_D$ be the set of all results (from
	  every data source).

Implementations may choose to merge any of these objects together; however,
they should ensure that only the parts specified here are exported to other
layers.

DIAG data flow diagram

Evt: $\breveH_* \to \breveH$

:	The events that can trigger this are:

	0. After receiving $H_s$ from [contact](#contact), in which case $H_s$ is
	   just copied to $\breveH$.
	1. After adding $h \to \dom \breveH_*$ (see below), and its lookups have
	   all completed, in which case we update $\breveH$ by calling the scoring
	   algorithm.

	TODO explain this much much better... maybe waiting is not necessary and we
	can just update $\breveH$ every time we receive data from $\breveR$...

Evt: $\breveH_* \to^\breveT \breveQ$

:	The events that can trigger this are:

	1. A request from the user to add a `index` $h \in \breveD$ as a data
	   source, in which case we add $h$ to $\dom \breveH_*$, and update
	   $\breveQ$, etc... TODO explain this much better....
	2. Receive an update to $\breveT$ from [naming](#naming), in
	   which case we update $\breveQ$ etc.

	For each `index` $h \in \breveH$, we want to select the tags to lookup.

	- if $h$ is from $H_s$, select all tags in $\breveT$.
	- otherwise, use $\breveH_*$ to find all tags $t$ than we reached $h$ by,
	  and then use $\breveT$ to select all "short" paths from $t$ to $t_0$.
	  TODO clarify this, description already in naming

	Do this when $\breveT$ is updated from [naming](#naming).

Async: $\breveQ \to^\breveP \breveR$

:	We run the lookups of $\breveQ$ in some [order](#deciding-lookup-order),
	and update $\breveR$ with the results. We can execute these in parallel.

Sync: $\breveR \to \breveH_*$

:	When a lookup $(h, t)$ completes, we scan the results returned, and add
	any arcs to `index`s to the relevant $\ddoth \in \breveH_*$. We also remove
	arcs to any `index` nodes that have already been added to $\dom \breveH$
	(so that they are no longer available for addition again).

	TODO explain this better

#### Deciding lookup order

Given
:	- $\breveQ$
	- $\breveT$
	- $\breveH_*$
Return
:	- an ordered list $\breveP = [(h_i, t_i)]$, where $(h, t) \in \breveP \iff
	  h \in \dom \breveQ \wedge t \in \breveQ[h]$.

Tags further away from the root are less "related" to the original query, but
searches for them are more likely to succeed (for rare searches), so the
algorithm should proceed by searching near the root, then further up the tree.

However, note also that tags with higher $u_t$ are more likely to have their
search queries succeed, independent of $w_e.u_t$.

TODO rewrite this...

## Optimisation

### Data structures

- `ptable`
	- quick partition of `index` vs `tgraph` nodes [$G_s$, $H_s$]
	- optionally order these by their score [possible future use]

- `tgraph`, `index`
	- where applicable:
	- quick lookup of node (and weight)
	- quick lookup of node's out-arcs (and weight) [routing, naming]

- `index`
	- quick partition of tag's to-`index` vs to-document arcs
	- optionally order these by their score [routing]

- $\breveg$
	- same as `tgraph`

- $p_s$, $\breveg$, $\breveG$, $\breveH$
	- might want to make these use CombinedWeight objects instead of a float
	  "weight", which in the future could be expanded to include a variance...

- $\breveT$
	- quick lookup of node (and weight)
	- quick iteration through all nodes [$\breveQ$]
	- quick comparison of nodes by their distance ordering [$\breveP$]
	- quick lookup of node's in-arcs (and weight) [$\breveQ$]

- $\ddotg \in \img \breveG_*$, $\ddoth \in \img \breveH_*$
	- quick lookup of node (and weight)
	- quick iteration through all nodes, arcs [$\breveg$]
	- quick lookup of node`s in-arcs (and weight) [routing, naming]
	- quick lookup of node's out-arcs (and weight) [maybe needed by some
	  scoring modules]
	- quick one-time check that all of a node's out-arcs (and weight) have
	  been retrieved from the network [routing, naming]

- $\breveQ$, $\breveR$
	- quick iteration of all lookups/results [$\breveP$, $\breveh$]
	- an advanced implementation would allow items to be added and dynamically
	  ordered in priority, bypassing the need to have $\breveP$, $\breveh$.


### Retrieval of remote objects

Usually we only need to retrieval part of a `tgraph` or `index`, eg. the weight
of a single node, or its out-arcs.

- eg. for quick "no" answer on lookups of storage objects - bloom filters

### Caching storage objects

- eg. cache commonly-retrieved objects like `ptable`s

The contact layer is independent of any query, so this can be done in the background
at any time. We can cache data for the layers above, which will help to
increase performance for future queries. etc...

MORE on incremental updates etc.

### Incremental state updates

- eg. when updating $\breveG$ from $\breveG_*$, we should only need to
  recalculate the parts that are affected by the updated....

# Design

## `tgraph` semantics

The routing algorithm needs some way of inferring which tags (out of a related
set) are more "general" or "specific".

Our first thought was to have arcs represent a relationship from a more
"specific" tag to a more "general" tag. However, this is problematic since:

- The relationship is supposed to be transitive, but our graph can potentially
  contain cycles, especially when pulling in data from several sources. We
  don't want to be tied up devising a complex algorithm to resolve this.
- We potentially would like to traverse from general to specific tags, as well
  as vice versa; and arc targets are the only mechanism of doing this between
  `tgraph`s.^[Within a `tgraph`, arc sources ("reverse" pointers) could in
  theory be calculated quickly, depending on the implementation.]

The analogy to typical navigable networks is that each tag is like an address;
and each `index` is like a routing table, mapping addresses to target entities.
^[Most "addresses" in this network will have multiple targets, but this is an
unimportant detail; eg. the internet has multicast addresses too.] So, we'll
call the network of `index` objects the "routing" plane.
(this is actually an important detail and why global reach is not guaranteed
for our system).

...

The following model addresses these shortcomings.

For a given tag $t$, let $D(t) = \{ d \in D : d \textrm{ tagged with } t \}$.
Define the "saturation" of $t$ to be $\frac{|D(t)|}{|D|}$, and the "relevance"
of $t$ to $t_0$ to be $\frac{|D(t) \cap D(t_0)|}{|D(t)|}$. An equivalent model
is to treat $D$ as a random variable, evenly distributed over all documents in
the network. For a given tag $t$, the saturation of $t$ is equivalent to $P(D
\textrm{ tagged with } t)$, which we shorten to $P(t)$ for convenience.
Likewise, the relevance of $t$ to $t_0$ is equivalent to $P(t_0|t)$.

We then use these to define:

- the weight $u_t$ of a node $t$ to be $P(t)$, its saturation.
- the weight $w_e$ of an arc $e = (t_0, t)$ to be $P(t_0|t)$, the relevance of
  the target tag to the source tag.

These values are straightforward to calculate, so this process can (in theory)
be automated. Note also that given $u_{t_0}$, $u_t$ and $w_e$, the weight of
$e^- = (t, t_0)$ can be derived using Bayes' theorem, as $w_e.u_t/u_{t_0}$. If
the `tgraph` implementation allows quick lookups of an arc both by its source
node and its target node, then it might be sensible to not explicitly define
the weight of the reverse arc.

We can think of `tgraph` $g$ (containing tags $t_i$), as a single tag over all
documents tagged by any $t_i$, and define saturation and relevance accordingly.
That is, $P(g) = P(\bigcup t_i)$ and $P(t_0|g) = P(t_0|\bigcup t_i)$. Note that
these values give no information on how well-connected a `tgraph` is within the
naming network. However, a `tgraph` consists of highly compressed data on a
large section of the storage network, so in practice the vast majority of our
naming needs should be satisfied by a small neighbourhood around our seed set.
Choosing between `tgraph`s probably won't be very important, so for now we
ignore this deficiency.[EXTN]

Now for the analogy to typical navigable networks: a tag $t$ can be thought of
as defining a subnetwork - the set of documents tagged with $t$. $P(t)$ tells
us the size of this subnetwork, and $P(t_0|t)$ tells us the proximity between
two subnetworks.

For example, $P(t)$ roughly corresponds to the CIDR routing prefix used by the
current internet. In ring-topology networks, such as some DHTs, neighbours are
matched by the numerical value of the network address; by contrast, internet
routers update each other with their nearest neighbours; and here, we provide
this information in the form of $P(t_0|t)$.

In routing terms, if our end target is $t_0$, then we want to pick a $t$ such
that $P(t_0 \cap t)$ is maximised - for the above model, this is $u_t.w_e$.

This raises the question of why we don't just define $w_e$ as $P(t_0 \cap t)$,
which would also remove the need to define $u_t$. However, this would destroy
our ability to quickly look up $P(t)$ and $P(t_0|t)$. On a distributed network,
we need to be able to combine data from multiple sources; algorithms to do this
may well need knowledge of the separate values.

## Links between objects

### Identity-object links

vs. all-data-in-identity and identity-identity pointers

### Object-object links

vs. identity-object links

Scalability - decreases the number of identity-object pointers

- requires less effort than forming a social link
- has less implications than a social link - only endorse information contained
  there, and possibly a small number of steps away

MORE

whether to use social link structure - flawed because:

- MORE using two different semantic meanings for weight attributes being used
  here (relevancy vs reliability) TODO explore this and its potential pitfalls.
- would need to expand `tgraph` to provide this information more efficiently
  (ie. have a "referents" table)

TODO talk about endorser of a node vs maintainer of a node - two different
things (here "node" refers to `tgraph` or `index`)

- **maintainer** of a node defines its nodes and its out-arcs, and provides no
  information about any other nodes beyond its out-neighbours
- **endorser** of a node points to it in its `ptable`, and may follow out-arcs
  to potentially reach _any_ other nodes in its connected component of the data
  network

In other words, when scoring nodes not in our seed set, we must **not** use
weight information from nodes themselves (including the seed nodes), since this
information comes from the _maintainer_ and not the _endorser_. It might be
feasible to extend `ptable` to allow endorsers to provide extra information to
help the scoring process [EXTN].

Below, we explore a rough and basic heuristic to infer scores using only the
link structure of the data network.

MORE

Let $x$ be the node (here, a `tgraph` or `index`) we want to infer a score for.
For each seed node $s$, let $w_{p(s)}$ be the probability that $x$ will satisfy
a query, by traversing some path from $s$ to $x$. (If there is no path, the
probability is obviously 0). The weight of $x$ we might then calculate as:

$$
1 - \prod_s (1 - w_{p(s)})
$$

This is just the union (over $s$) of all path weights, assuming that paths are
independent. A rough justification for this is that we only need to traverse
one path to satisfy a query; we can pick any one we wish.

This is open to attack by multiple colluding seed nodes (which breaks the
assumption of independence), but in such cases, worse attacks are possible.
TODO explore more...

This leaves us to derive a model for $w_{p(s)}$. A simple one is:

- $w_{p(s)} = w_s.k^i$, where $k$ is some constant, and $i$ is the number of
  steps in the shortest path from $s$ to $x$.

$k^i$ represents the probability that the people who endorse $x$, also endorse
an object $i$ steps away from it. Suppose that the endorser follows a node's
out-arcs with probability $k$ (ie. the endorsement also covers the out-node).
Traversing $i$ steps involves following each arc in the path, and therefore has
probability $k^i$.

This ignores several important factors. For example:

- for nodes with larger out-degree, a typical endorser will follow a smaller
  proportion of its out-arcs.
- an endorser is more likely to visit a node that can be reached in more ways
  from $s$ (ie. has more distinct paths to $s$). This is essentially the same
  point as our "union" method above for all $s$.
- an endorser is less likely to traverse another step, the more steps they
  already are from $s$.

Some of these suggest potential vectors of attack upon the $k^i$ scheme. For
example, the assumption that the proportion of out-arcs followed is constant,
could encourage a malicious seed node to define huge numbers of out-arcs, whose
referents will then enjoy a larger path weight than appropriate. (This attack
is easy to detect, but there may be more effective ones.)

This problem can be stated more generally as: _Given an endorsement of a node
in a (social) network, what other nodes can I infer that the endorsement also
covers, and to what degree?_ [EXTN]

For our prototype, we stick to $k^i$. Due to the security concerns, we try to
give an underestimate for $k$. This should not affect the reachability of the
nodes in the network; only that near nodes (to our seed set) will have a much
greater priority than far nodes. Attacks from far nodes can only affect other
far nodes; and we also have the option of constructing local overrides against
the attacks that are detected.

TODO make an actual estimate... $2^{-4}$. [EXTN]

## Weights

### Zero weight-attributes

The current data structures can only represent the presence of an attribute,
and not its absence. This is a problem: any distributed system has to be able
to deal with incomplete information, but here there is no way to tell if the
absence of an object from a collection is due to incomplete information, or an
explicit rejection based on the semantics of the object attributes. This is
important when (eg.) we combine weights from multiple sources - we need to know
whether others disagree with a recommendation, or merely don't know about it.

We can either require that explicit "zero" weight-attributes are defined for
every item encountered, no matter how insignificant; or we can try to generate
heuristics for resolving this ambiguity, based on other implicit information.

**For a node-map**, the former results in linear data growth in the number
of items (nodes) encountered. This is not too bad, but can become inefficient
if the vast majority of items encountered are assigned to zero weights, and the
algorithms that process the data are designed to ignore them. With regards to
our system, this probably affects `ptable`s more than `tgraph`s, where the
algorithmic usefulness of a tag is largely unrelated to its weight.

Within a single node-map, there is no information that could be used to perform
any heuristic: a node not included in the map has no information relating to it
in the map. However, it may be possible to extract information from other
sources, such as the friends for a `ptable`. These are context-specific and
are explored in the appropriate sections. [TODO add link]

**For an arc-map**, the former approach results in quadratic data growth in the
number of items (nodes) encountered, since we need to define _something_
(either a relationship or its absence) between all _pairs_ of nodes. This is
not scalable; fortunately an arc-map holds more information than a node-map,
which allows for better ambiguity-resolution heuristics.

Since the meaning of $e = (v_s, v_t)$ is fully determined by $\{v_s$, $v_t\}$,
the maintainer has enough information to decide whether to include $e$ in $E$
if it "knows about" both $v_s, v_t$. Roughly, we can represent this notion as
$v \in \rft E$. In other words, if $e \notin \dom E$ but $v_s, v_t \in \rft E$,
then either the maintainer has overlooked the relation between $(v_s, v_t)$, or
there is no such relation (ie. a "zero weight"). The latter is more likely,
since people generally review things before publishing, and the mistakes that
do slip through are fixed over time.

Specific examples of this general principle, as well as applications of it,
are explored further in relevant sections of the [Algorithms](#algorithms)
chapter.

### Negative weight-attributes

A related idea is negative weight-attributes, which would represent a judgement
that the subject is malicious (in some sense), rather than neutral or "useless".
In a network where it's possible to act aggressively towards particular nodes,
these weights could provide information on who to attack (or equivalently, set
up defences against), and in what way.

As it stands, our search system is a "dumb" network of data; the only way of
interacting with other nodes is by publishing data objects for other nodes to
read. We are unable to see other nodes' read requests, so it's impossible to
attack a particular node. In this case then, negative weights are not useful.

There are other complications with negative weights in general. These would be
relevant if our search system is adapted into a form where one *can* perform
targeted acts of aggression.

- If positive weights are taken to be probabilities, it's not obvious what
  negative weights should mean. If the semantics of the data structure are not
  well-defined, the algorithms that use it cannot be, either.
- Depending on the algorithm used to combine weights from multiple sources,
  negative weights may allow the weight system itself to be used as an attack
  vector. (Otherwise, this was already possible with positive-only weights and
  the introduction of negative weights makes no difference.)
- MORE

## Combining data from multiple sources

Below, we describe algorithms for combining data from multiple sources. Our
model is relatively simple: we have a list of sources with a weight judgement
on how "good" each source is; in turn, each source specifies their own weight
judgement for each object that it refers to. To avoid confusion, we will call
source-weights "scores" and object-weights "values". Given a target object, we
want to derive a "combined" value for that object, using the given information.

TODO discuss more complex schemes that return "variance" for values. This would
allow further refinement, eg. the more data sources that give a value judgement
for some object $x$, the smaller the variance would be. [EXTN].

For our prototype, we use a very basic algorithm - the score-weighted mean
value of the object. Generally, the arithmetic mean is susceptible to attacks
and distortions. Let $k$ be the proportion of the total score that the attacker
controls. Weights are [limited](#weight-attributes) to $[0, 1]$, so this will
roughly be the same as the proportion of compromised nodes.

If the value is evenly distributed, the effect of an attack will be limited in
direct proportion to $k$ - the worst thing they can do is give a single value
with $k$ influence in the final result. However, if the value is far from being
evenly distributed, it would be possible for an attacker to introduce a value
that has an extremely small probability $\epsilon$ of occuring, yet which will
still have a $k$ share of influence in the final combined value.

TODO - explain this better.

So the types of weights that are input to this algorithm must be roughly evenly
distributed in order to be secure. Most distributions probably will not cause
too many problems; however, negative-log distributions will.

The algorithms below make use of several $\alpha_i$ functions, which fine-tune
the output based knowledge of on the context of the inputs. There are various
ways of producing such functions; one method is to extract realistic values
for some constant, by collecting training data from a prototype network. More
advanced techniques include collecting data directly from the active network,
to adjust the values dynamically. [EXTN]

However, such methods are likely to be quite complex, and so this project does
not go into them. Instead, we only explore heuristics that give a rough
approximation, based on our intuitions. TODO rewrite...

### Mean weight of a node

Given
:	- a map $M = \{ m \mapsto u_m \in W \}$ of data sources to their scores,
	  where each data source $m = \{ v \in V \mapsto w_{mv} \in W \}$ is a map
	  from nodes to their values.
	- a given node $v \in \bigcup \dom m$
Return
:	- the map-weighted mean node-weight $\bar w_v$ of $v$

The mean weight for node $v$, over all maps, is $\frac{\sum_{m} w_{mv}}{|M|}$.
If we take into account the weights of each map, we have $\frac{\sum_{m} u_m
w_{mv}}{\sum_m u_m}$. However, not every map will necessarily contain $v$, so
some $w_{mv}$ may be undefined. In such cases, we estimate a weight instead;
see the section on [zero weights](#zero-weight-attributes) for more details.
Following on from that, we reach:

$$
\bar w_v = \frac{\sum_{m} u_m \hat w_{mv}}{\sum_m u_m}
\quad ; \quad
\hat w_{mv} = \left\{ \begin{array}{llr} \\
  w_{mv} & : v \in \dom m & (0) \\
  \alpha_1(m,v).0 + (1 - \alpha_1(m,v)).\bar w_v & : v \notin \dom m & (1) \\
\end{array} \right
$$

where $\alpha_i(m, v)$ is the probability that the author of $m$ has judged $v$
to be worthless, given $(i)$.

The above definition has $\bar w_v$ on the RHS; after rearranging, we get:

$$
% should be \dfrac but LaTeXMathML doesn't support amsmath commands...
\bar w_v = \frac{\sum_{m:(0)} u_m w_{mv}}{\sum_m u_m \alpha(m,v)}
\quad ; \quad
\alpha(m,v) = \left\{ \begin{array}{ll} \\
  1 & : (0) \\
  \alpha_1(m,v) & : (1) \\
\end{array} \right
$$

Generally, the behaviour of $\alpha_i$ are likely to depend highly on the use
context, so this should be an additional parameter to the algorithm.

### Mean weight of an arc

Given
:	- a map $M = \{ m \mapsto u_m \in W \}$ of data sources to their scores,
	  where each data source $m = \{ e \in E \mapsto w_{me} \in W \}$ is a map
	  from arcs to their values.
	- a given arc $e = (v_s, v_t) \in \bigcup \dom m$
Return
:	- the map-weighted mean arc-weight $\bar w_e$ of $e$

The formula is similar to the one from the previous section, but here we are
dealing with arcs, so we can refine our estimate somewhat further; see the
section on [zero weights](#zero-weight-attributes) for more details. Following
on from that, we reach:

$$
\bar w_e = \frac{\sum_{m} u_m \hat w_{me}}{\sum_m u_m}
\quad ; \quad
\hat w_{me} = \left\{ \begin{array}{llr} \\
  w_{me} & : e \in \dom m & (0) \\
  (1 - \alpha_1(m,e)).\bar w_e & : e \notin \dom m \;\wedge\; \{ v_s, v_t \}
    \not\subseteq \rft m & (1) \\
  (1 - \alpha_2(m,e)).\bar w_e & : e \notin \dom m \;\wedge\; \{ v_s, v_t \}
    \subseteq \rft m & (2) \\
\end{array} \right
$$

where $\alpha_i(m, e)$ is the probability that the author of $m$ has judged $e$
to be worthless, given $(i)$.

The above definition has $\bar w_e$ on the RHS; after rearranging, we get:

$$
% should be \dfrac but LaTeXMathML doesn't support amsmath commands...
\bar w_e = \frac{\sum_{m:(0)} u_m w_{me}}{\sum_m u_m \alpha(m,e)}
\quad ; \quad
\alpha(m,e) = \left\{ \begin{array}{ll} \\
  1 & : (0) \\
  \alpha_1(m,e) & : (1) \\
  \alpha_2(m,e) & : (2) \\
\end{array} \right
$$

Generally, the behaviour of $\alpha_i$ are likely to depend highly on the use
context, so this should be an additional parameter to the algorithm.


# Comparison

Our design differs considerably from these; for example, REMINDIN's design is
more ontology focused, whereas ours draws from existing non-semantic routing
principles, DHTs in particular. Our design also has a well-defined (albeit
underdeveloped) model of the address space (unlike Harnessing). [MORE] also
move this to a better place...


