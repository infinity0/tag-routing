% Design and architecture
% Ximin Luo

# Introduction

## Background

Searching for information is an essential component of any network. Without it,
there might as well not be a network in the first place.

The world wide web is the largest information network ever created; currently,
search is a service; providers employ crawlers to navigate this network, and
extract and summarise information from the documents visited, into a form which
is suitable for insertion into a database. "Searching the web" usually means
querying these pre-built databases, rather than dynamically routing your way
through the web's content.

Preparing and maintaining the database is extremely resource-intensive for a
large network, which results in high barriers to entry. The utility of a search
service increases with how much of the network it covers; new providers must
recreate a database of comparable size before clients will switch, or else use
indexing algorithms that produce decidedly better results than alternatives.

This creates conditions of oligopoly, which is inherently vulnerable to both
abuse and attack. Large providers are trusted by a great number of clients, so
more people are affected when this trust is broken. We have already seen cases
of providers censoring their search results, both voluntarily[^censor1] and
under coercion[^censor2]. Privacy is also a concern: providers can monitor
client usage of the service, and build up a profile of personal activities.

[^censor1]: Google stopped returning results to CNET's website, because they
published an article that the CEO disliked. [REF]

[^censor2]: Google was also forced by the Chinese government to censor many
search results from its service in China. [REF]

Another issue is the depth and granularity of search topics. Most of us don't
use a search provider for every item of information we need; instead, we often
issue a query that gives us a selection of related sites from all over the web,
then manually browse within these sites to target our needs more precisely. In
addition, some websites have non-public information, or specialist knowledge
that generic search algorithms aren't able to index effectively. In these
cases, central index databases are inadequate.

An alternative approach is to perform dynamic routing using query semantics.
Instead of a simple client-provider model where a single query is a single
transaction, we propose a co-operative model where queries are routed between
autonomous providers, and results aggregated for the end user. Small providers
can index their own local sections of the network, and access and results can
be fine-tuned using local information. In addition, clients will have a wider
choice of who to trust.

Of course, decentralised systems have their own issues, and these are briefly
discussed [below](#potential-issues). We believe that these are all practical
issues that are ultimately solvable in the long run, whereas centralisation is
an inherent problem in itself.

Imagine your browser acting like a router; you type in a search query and it
automatically follows links between pages to reach what you want. Of course,
this is a long way off, and it may well be beyond the capabilities of current
hardware and networks, but hopefully this project makes a useful contribution
in that direction.

## Related systems

There are many existing systems which use decentralised, co-operative, dynamic
routing algorithms, such as the internet and various peer-to-peer overlay
networks. There are a great variety of different objectives, approaches, and
models, but some common themes include:

Key-based routing
:	The network defines an address space and a distance metric, where each
	address is represented by a binary key. (DHTs, Freenet, GNUnet)
Mesh networks
:	Various heuristics are used to maintain structure and performance, such as
	random walks, bandwidth detection, index delegation, etc. (Gnutella)
Social relationships
:	Nodes prefer to peer with trusted friends. This provides better security
	properties, and a more predictable network structure. (OneSwarm, Freenet)

A more comprehensive survey of peer-to-peer searching is presented in [^surv1].
Using the terminology of that paper, this project develops and evaluates a
_probability-based model_ of information retrieval intended to support
_comprehensive keyword search_ (as opposed to _partial keyword search_).

[^surv1]: John Risson, Tim Moors, Survey of research towards robust
peer-to-peer networks: Search methods, _Computer Networks_, **Volume 50**,
**Issue 17**, 5 December 2006, Pages 3485-3521, ISSN 1389-1286, DOI:
[10.1016/j.comnet.2006.02.001](http://www.sciencedirect.com/science/article/B6VRG-4JD0XYW-1/2/07e1ec0ba8cbe65f8f094cd99612b149).

Existing research into peer-to-peer semantic search includes [^sem1][^sem2].
None of these are currently deployed in real systems, and we were unaware of
them during the initial stages of our project. A discussion of these systems
and how they relate to our project is given [later](#comparison), after our
system has been described first.

[^sem1]: [REF] REMINDIN'
[^sem2]: [REF] Harnessing.


# Preparation

## Objectives

We intend to build a system that can potentially offer a similar functionality
to existing search providers, although obviously our prototype will be nowhere
near as sophisticated nor efficient. Core functional aspects include:

Semantics
:	Query subjects have semantic relevance to the results, so the routing
	algorithm and address scheme must reflect this.
Reach
:	It should be feasible to locate all the data matching a given query on the
	entire network (or connected component).
Robustness
:	Query paths and returned results should be resistant against subversion,
	such as spam floods or data poisoning.

Most major currently-deployed systems have at least one incompatibility with
the above. For example, DHTs are scalable, and will reach data if it exists on
the network, but addresses have no relation to the semantics of the data. Many
mesh networks (eg. Gnutella) can perform keyword search, but do not attempt to
reach all relevant data on the network.

## Potential issues

_Here, we briefly discuss issues surrounding a decentralised architecture, and
present ways of addressing these issues, or else explain why we think they are
not a significant problem in the context of this project._

A system which must route queries between autonomous providers will obviously
be slower than a system that only needs to query a single provider. At present,
this is noticeable and signficiant - eg. Google returns results for the entire
web almost instantaneously, whereas DHT queries on a medium-sized network might
take a minute to complete. However, systems only get better, not worse, and
hardware will improve in the years to come. By the time research in this area
is well-developed, it's entirely possible that performance will have improved
beyond the limit of human perceptibility.

Existing large service providers might have little incentive to participate in
a co-operatiave search system, since they are each competing for control over
the market. However, this is less of a factor for smaller providers who would
otherwise be unable to attract many users, since their intention is only to
provide search capabilities. In principle, this is no different from displaying
links to other websites to help your visitors find what they want - linking to
useful content increases your own utility, even if this is not reciprocated.

As for constructing a decentralised system in the first place, it's widely
acknowledged that building these to be both scalable and secure is a difficult
problem. However, nothing suggests that this is an inherently impossible task;
and once a problem is solved, future generations may reap the benefits without
having to expend the initial development cost. Recent designs based on social
networks have been promising, and we will use this as an inspiration.

## Initial observations

Our first ideas drew upon our experiences and pre-conceptions on how we humans
try to locate things. Two themes stand out:

- keeping knowledge on who knows what, and use this to direct query routes.
- shifting the query subject to increase recall or precision - eg. broadening
  increases the recall, and re-specialising increases the precision

We also drew from our existing background knowledge on decentralised storage
networks, such as Freenet and other Distributed Hash Tables. DHTs are very
efficient and scalable, most systems giving O(log n) performance[^dhtperf] in
the size of the network. These generally have a well-defined address space with
a distance metric, where each address can be represented as a binary key. This
allows for fairly simple, yet effective, routing algorithms.

[^dhtperf]: eg. expected number of hops for a lookup

An informal explanation is that a numerical address space can be partitioned
hierarchically, eg. by taking successively larger prefixes of an address. This
allows greedy routing to work effectively, ie. by finding the neighbour which
shares the smallest partition with the desired target node (in IP routing, this
is just "longest-prefix-match", which we are all familiar with).

[DIAG] diagram of hierarchical partitioning, [REF] kleinberg "small world"

Two of these ideas seemed to fit well together - if we made the query-shifting
aspect of "human" routing more precise, by using the idea of a structured
address space, then this could also allow for a simple routing algorithm.
Semantic tags are not as naturally structured as numerical addresses however,
and a significant part of our time would be spent in developing a theoretical
model of a partitionable and navigable space over tags.

We did not have any specific initial ideas on how to ensure the global reach of
a search request. Using tags as addresses means that each address holds many
resources; being able to reach all the resources for a single tag is equivalent
to being able to reach any arbitrary resource for that tag. Our DHT-inspired
design aims to support the latter; however, bounding the cost of finding extra
resources is a more complex problem, which we didn't get the chance to explore.
[EXTN]

Finally, we chose to base our system upon a foundation of social relationships.
Recent research has shown that analysing network structure can be effective in
resisting malicious information. A major reason is because social relationships
are much more expensive to attack than simple indexing algorithms that make
naive assumptions about input data. Examples include Google's PageRank[REF],
trust metrics such as Advogato[REF], and sybil detection algorithms such as
SybilInfer[REF].

## Working assumptions

Since the aim of the project is fairly ambitious, we want to repeat as little
work as possible. Therefore, we make various assumptions about the environment
that our system will run under. Some of these are reasonable, and some of these
are fairly restrictive; however, we feel they are prudent and necessary in the
context of this project.

Locating known objects is essentially a solved problem: the internet and DHTs
both offer ways to retrieve objects based on a globally unique address. Without
this primitive we cannot continue at all; data has to be stored somewhere, and
it has to be accessible to an arbitrary subset of the network.

Additionally, we assume that objects are always available. In a real network,
this can be implemented with proxy services (servers that lend out storage and
guarantee continuous access to this), or it might be a property of the storage
network (eg. DHTs). This simplifies our design, since it allows us to avoid
dealing with the issue of churn, by delegating it to an external component.

This also forces us to consider an iterative routing algorithm rather than a
recursive one. Since data cannot forward queries onto other pieces of data, a
lient must process it themselves, determine which objects to retrieve next,
then retrieve those, etc. Hopefully, our design can be adapted into a recursive
one without much effort, but we won't give special consideration to this.

Importantly, we will only consider single-phrase queries, where each "phrase"
corresponds to a single entry in a lookup table. We exclude compound queries,
ie. multiple phrases composed with operations such as intersections, unions,
differences, etc. This helps to keep the basic problem simple; compound query
methods can be developed later, and arguably on top of a solution to this
simpler problem.

# Theory

In this chapter, we discuss the theoretical ideas behind the system. We
experimented with many different ideas before the code development began, and
during development we weeded out the ones which seemed to lead to a dead end.
Afterwards, the surviving ideas were explored further, and simplified or
generalised.[^simp]

As such, there are a few aspects of our system design which may seem inelegant
or imperfect in the context of the theories described below. Unfortunatly, we
didn't have enough time to go back and refine them; but we have tried to point
these out in the appropriate parts of the architecture section. None of these
flaws, we think, are serious enough to defeat the basic purpose of the design
that we implemented.

[^simp]: For example, originally the problem of information aggregation was
considered only for the naming and routing layers, and was completely ignored
for the social layer. During implementation it was realised that trust metrics
could not resolve conflicts between ptable entries, and after implementation we
formulated the theoretical expression of the general problem, given below. The
discussion on the various issues of [aggregation](#information-aggregation) as
related to our design was actually done during the preparation stage, but it
was only afterwards that we realised how this fit into the general theory.

## System design

_This section describes the overall concepts of our system, and the reasoning
for our design choices. More precise details, such as the exact contents of the
various objects introduced, and how the search algorithm processes these
objects, are given in the [architecture](#architecture) section._

We start off with the analogy of an **index** being a routing table. Tags are
addresses, documents are hosts, and routes are semantic relationships between
a tag (address) and a document (host). Scalable routing tables must compact
information together. Instead of defining a route to every single host on the
network, it is divided up into subnets, and a single route defined for the
entire subnet. Similarly, indexes shouldn't point to other indexes using all
possible relevant tags, but it should summarise this information. Instead of
listing a neighbouring index under entries for several dozen instances of e.g.
strategy games, it could instead list it only under "strategy games".

A consistent model for this is developed in [address space](#address-space).
That gives us another type of object, a tag graph, or **tgraph**, which defines
relationships between tags. Following the principle of compacting information
again, we need some way to route between these tgraphs. At this point, you
might worry that we'll continue with this ad-infinitum, inventing more layers
of meta-objects; but no, we stop here, and actually we use tgraphs to route
between themselves. That is, tgraphs also use tags for addresses, and list
related tgraphs under entries for the appropriate tags.

To explain why this isn't a problem:

- The set of documents is much larger than the set of semantically distinct
  tags. Therefore, a map of tags to related _tags_ gives us more information
  about the entire network, than a (similarly-sized) map of tags to related
  _documents_.
- More people are likely to have an idea of what a tag's related tags are,
  since this is part of everyday communication. So a neighbourhood of tgraphs
  will cover a larger section of the network.
- The previous observation also applies to the initiator - most queries are
  made with some understanding of what the tag means. In the worst case, the
  system can just ask the user to supply their own "related tags".

This principle can be summed up as "language unites". Just as the ubiquity of
mathematics allows numerical addresses to be routed over, we exploit the
semi-ubiquity of language to allow tags to be routed over.

Since we are routing between documents, rather than people or hosts, we split
the social network from the indexes network and the tgraphs network. This gives
us several advantages. It allows for space reductions - people often agree
about things, so identical parts of indexes can be spun off into another index
and pointed to instead of repeated. This model is more general; anything that
applies to it must also apply to the case of one-index-per-person. It's also
arguably more realistic, since we often agree with someone's judgements,
without trusting them as friends.

Each identity on the social network defines a preference table, or **ptable**,
listing the indexes and tgraphs that it values highly, and other identities it
trusts. By exploring this network, we can infer a set of trusted objects from
which to initiate our search.

### Mutability of objects

[MOVE] probably to "working assumptions"

We consider objects to be immutable. For instance, we intended the sharing of
indexes via ptable pointers to be a space-saving measure, instead of a gesture
of delegation. In this respect, our objects are more like git commits rather
than web pages, and object links are more like git submodule pointers rather
than web hyperlinks.

This data model is more secure, and in our system it also keeps the issue of
trust contained in the social layer, a property that some components depend on.
However, the immutability is an implicit assumption, which is neither enforced
nor verified in any system component. The intention was to leave this to the
storage layer; real deployments of the system should keep this in mind.

### Data collection

[MOVE] probably to "working assumptions"

Finally, our project only considers how we navigate the networks of objects to
satisfy a search query. It ignores the problem of how these objects would be
constructed and maintained. However, the data models are fairly simple, and is
heavily inspired by ordinary human linking habits. So creating these objects
should be both straightforward and natural.

## Structured address space

As discussed earlier, many routing schemes use a structured address space that
has two (sometimes implicit) properties over its subspaces:

- A measure of the size of the subspace
- A measure of the overlap or similarity between two subspace.

In CIDR, "size" is given by the subnet mask, and "similarity" is given by
matching the subnet prefix against the entries in the routing table. For DHTs,
both "size" and "similarity" are defined with the choice of address space. In
the case of semantic routing, a tag's "size" corresponds to how "general" it
is, and its "similarity" to another tag corresponds to how related they are.

There are various methods of defining a mathematical space over semantic tags.
In the end, we settled on a simple probabilistic model. We interpret the set
of all documents as a probability space:

- Each document $d \in D$ is an outcome, all equally likely
- Each tag $t \in T$ is an event, consisting of all documents tagged with $t$.
  (interpreted as a set, $t = \{ d \in D : d \textrm{ tagged with } t \}$.)

We can interpret $P(t)$ as the size of $t$, and $P(t_0|t)$ as the similarity of
$t$ to $t_0$. These are fairly straightforward to calculate (ie. count the
relevant documents and normalise), so this process can in theory be automated.

This information can be stored as a graph, where each tag is a node and each
tag-tag relationship is an arc. Node attributes are sizes, and arc attributes
are similarity scores[^addrat]. We'll call this a tag-graph, or tgraph for short.

We are distributing this information across many objects, so each tgraph must
be able to point to other tgraphs. As with indexes, we need to guide traversal
between these objects. A simple model for this is to interpret a tgraph $g$ as
the union event of all its tags, ie. $g = \bigcup_{t \in g} t$[^addrlk]. Any
tgraph that wishes to refer to $g$ can represent it as a node in the graph,
with size and similarity attributes following as for single tags.

[^addrat]: In retrospect, it would probably be neater to use $P(t_0, t)$ as arc
attributes and store an undirected graph; they contain the same information.

[^addrlk]: This interpretation gives no information on how well-connected a
tgraph is, which would also affect its usefulness. However, in general, tgraphs
are intended to hold highly compressed summaries on significant sections of the
network, and the vast majority of naming needs should be satisfiable by a small
neighbourhood around a seed set. In other words, we assume that routing between
tgraphs won't be a major practical issue, and brush this deficiency under the
carpet.[EXTN]

### Distance relation

Let $(M, \circ)$ be a **monoid** with a _linear ordering_ over $\sqsubseteq$,
_identity element_ $I$, and which only contains _non-negative_ elements, ie.
$\forall m \in M: I \sqsubseteq m$. A **distance relation** over a set $S$ is a
partial function $D \subseteq S \times S \to M$ satisfying:

Identity
:	$\forall a,b \in S : D(a,b) = I \iff a = b$

This is consistent with the intuitive notion that "adding components to a path
never makes it shorter":

$$
\begin{array} {rcll}
\forall a,b,c \in S :\\
             I & \sqsubseteq & D(b,c)              & \quad M \textrm{ non-negative } \\
D(a,b) \circ I & \sqsubseteq & D(a,b) \circ D(b,c) & \quad M \textrm{ linearly ordered } \\
        D(a,b) & \sqsubseteq & D(a,b) \circ D(b,c) & \quad \textrm{ identity element } \\
\end{array}
$$

A **distance metric** also satisfies $\forall a,b,c \in S : D(a,c) \sqsubseteq
D(a,b) \circ D(c,b)$, ie. _symmetry_ ("forwards backwards are equally long")
and _triangle inequality_ ("direct path is shortest"); we won't consider these
restrictions here.

We refer to the tuple $(M, I, \sqsubseteq, \circ)$ as the _relation type_. A
probability-based relation of type $((0, 1], 1, \geq, \times)$ can be converted
into an entropy-based relation of type $([0, \infty), 0, \leq, +)$ by taking
the negative-log of each value, and vice-versa by taking the inverse-exponent.

We want to define a distance relation over tags, so that we can construct a
routing scheme over them. One simple approach is $D[t_0, t_1] = P(t_1 | t_0)$,
which forms a valid distance relation with the probability-based relation type
$((0, 1], 1, \geq, \times)$, giving a path-distance formula of $D[t_i]_0^n =
D[t_0, t_1].\cdots.D[t_{n-1}, t_n]$.

There are various flaws with this. In our view, a significant flaw is the lack
of a "natural" interpretation as to what this thing represents. It also gives
no information about tag triples, and hence cannot distinguish between cases
where triple-intersections differ greatly; although this is really a problem
with the tgraph design.[^addrdm]

[^addrdm]: Note also that it isn't a distance _metric_, as it satisfies neither
symmetry nor the triangle inequality. We were (and still are) unaware of any
theory that comments on if this is a bad thing, from a routing perspective.
Dijkstra's algorithm, which we use to construct a routing scheme, only requires
a distance relation rather than a metric.

Unfortunately, we didn't have enough time to explore these issues very deeply,
or to come up with better alternatives. It does satisfy the distance relation
axioms, so we decided to stick with it.

## Information aggregation

Trust metrics and sybil detection algorithms are useful, but cannot support
applications that must obtain value judgements _about resources_ from other
agents. Define:

Agent
:	an entity which can give (and receive) score judgements
Resource
:	an entity which can only receive value judgements

Each agent is associated with two maps: an agent-score map $\{ a : s \}$, and
a resource-value map $\{ r : v \}$, which define their subjective judgements.
A score represents how much an agent's judgements can be trusted; the meaning
of a resource value is context-specific. The problem of information aggregation
can then be formulated as:

Given
:	- a set of agents, with their agent-score and resource-value maps:
	  $$
	  a : (\{ a : s \} , \{ r : v \})
	  $$
	- a seed agent-score map:
	  $$
	  a : s
	  $$
Return
:	- a target resource-value map, aggregated from the given information
	  $$
	  r : v
	  $$

This can be represented as a table. [DIAG] cols = resources, rows = agents

### Simplification

The above formulation of the problem can be usefully divided up into two
components:

**1. Infer scores for all agents**:

Given
:	- a set of agents, with their agent-score maps
	- a seed agent-score map
Return
:	- a target agent-score map, that covers all agents in the input

and

**2. Compose values for all resources, from the scored agents**:

Given
:	- a set of agents, with their inferred score, and resource-value maps
Return
:	- a target resource-value map

We follow this approach in our project. Even these simpler problems, however,
are fairly deep, and developing robust techniques to solve them could take up a
whole other project. Our prototype implemenation only attempts crude and simple
solutions to each component. [EXTN]

(Note that the first component is a generalisation of trust metrics, which use
an implicit seed agent-score map of the form $\{ a_0 : s_{max} \}$, i.e. a
single root agent mapped to the maximum score.)

### Use contexts

In the context of our system design:

---------------------------------------------------------------------------
layer       agent                           resource
----------- ------------------------------- -------------------------------
social      identities and their ptables    indexes, tgraphs

naming      tgraphs                         tags, tag-tag relationships

routing     indexes                         tag-document relationships
---------------------------------------------------------------------------

One can see here that indexes and tgraphs take the position of both agent and
resource. The output of the social layer is a resource-value map; the naming
and routing layers both use this as an agent-score map.

As agents, neither tgraphs nor indexes define explicit agent-score judgements.
In addition to this, they represent immutable information, rather than social
identities. These were conscious design choices - to encapsulate the problem of
trust and authentication into a social layer, away from the problem of naming
and routing.

This does mean that the problem is different from that of a trust metric. We
won't attempt to formalise this, but intuitively, for a given non-seed agent
$a$, we want to infer its score based on how much arbitrary endorsements of the
seed agents _extend_ to it, rather than how much the seed agents should _trust_
it. A crude attempt at a solution is [score-inferer](#score-inferer).

### Attributes

So far, we haven't discussed what the attributes should actually represent. In
any usage scenario, they should at least satisfy the following properties:

- The attribute should be some well-defined property that can be measured or
  estimated. Any single value of the attribute should mean the same thing to
  all agents, rather than being left up to agents' own interpretation.
- Attribute values should not be treated as authoritative, since in our model
  we only ever receive data from agents, whom we assume to have a limited view
  of the entire network. There will always be inconsistency between different
  agents; the system should account for this.

In the case of perfectly honest agents, it is hoped that inconsistencies will
work to cancel each other out. In the case of a system under attack, this will
not be the case. However, only looking at the received values for attributes
cannot distinguish between an attack and actual value differences; so this must
be detected in some other way. Our project won't present any methods for doing
this, but network structure analysis is a common approach type.[EXTN]

The exact attribute types we use are discussed [elsewhere](#data-structures).
Generally, we use probability-based attributes, which are well-defined and can
be estimated by agents. It also gives a simple way of combining attributes -
multiplication - as well as a simple space of values - $[0, 1]$.[^attrva]

[^attrva]: More sophisticated attributes might describe a belief distribution
over its possible values. When aggregating multiple judgements that agree with
each other, we could reduce the variance of the aggregated attribute.[EXTN]

In some contexts, the set of useful resources is sparse over the set of all
possible resources[^attrsp] - e.g. if the attribute represents an endorsement,
or a non-trivial binary relationship. In such cases, we'd like to not define
the non-useful resource at all, implicitly marking it with a neutral or "zero"
attribute, to save space and redundancy.

However, this raises a problem when aggregating information - we now can't
distinguish whether an agent implicitly regards a resource as useless, or just
that it doesn't know about it. In our crude [value-composer](#value-composer),
we address this issue by requiring an input heuristic that can help to resolve
this ambiguity, based on other information.

[^attrsp]: from a given agent's point of view

A related idea is negative attributes, which would represent a judgement that
the resource is malicious or dangerous in some sense, rather than neutral or
"useless". In a system where it's possible to act aggressively, these weights
could provide information on who to attack (or set up defences against).

It's not clear how this applies to our system, which is a network of data and
so only supports passive traps rather than active attacks. It's also unclear
how probability-based attributes could be extended to work this way. So for
now, we ignore this possibility.[EXTN]


# Architecture

## Data structures

_In this section we present a specification of the data objects that our system
uses, their structure, semantics, and expected contents._

### ptable

DIAG

Structure
:	$$
	p = \left[ \begin{array}{lrl} \\
	  V_p =&   g|h &: u \\
	\end{array} \right]
	$$
Description
:	A map $V_p$ of nodes to attributes, where each node is a tgraph or an
	index.
Semantics
:	Probability that the node can satisfy a query for an arbitrary tag.

Each ptable is associated with some identity $z$ on the social network; the
friend table of $z$, together with $V_p$, make up the judgement table for $z$
as an agent.

We assume that the set of useful resources is sparse over the set of possible
resources ($G \cup H$), and so resources with near-neutral attribute values may
be omitted at the identity's discretion, even if they are aware of them.
Algorithms that process this data structure should be aware of this.

### tgraph

DIAG

Structure
:	$$
	g = \left[ \begin{array}{lrl} \\
	  V_g =&    t|g &: u \\
	  E_g =& t, t|g &: w \\
	\end{array} \right]
	$$
Description
:	1. A map $V_g$ of nodes to attributes, where each node is a tag; and
	2. A map $E_g$ of arcs to attributes, where each arc is a (tag, tgraph) or
	   (tag, tag) relation.
Semantics
:	1. Size of the tag. (see [address space](#address-space))
	2. Similarity of the target tag or tgraph, to the source tag.

We assume that the size of a tag is useful information, irrespective of the
actual value. There is no such thing as a "neutral" size, and maintainers are
expected to define all tag sizes that they have knowledge of.

We assume that the set of useful relations is sparse over the set of possible
relations ($T \times (T \cup G)$), and so relations with near-neutral attribute
values may be omitted at the maintainer's discretion, even if they are aware of
them. Algorithms that process this data structure should be aware of this.

### index

DIAG

Structure
:	$$
	h = \left[ \begin{array}{lrl} \\
	  E_h =& t, d|h &: w \\
	\end{array} \right]
	$$
Description
:	A map $E_h$ of arcs to attributes, where each arc is a (tag, index) or
	(tag, document) relation.
Semantics
:	Probability that the arc target will satisfy a query for the source tag.

We assume that the set of useful relations is sparse over the set of possible
relations ($T \times (D \cup H)$), and so relations with near-neutral attribute
values may be omitted at the maintainer's discretion, even if they are aware of
them. Algorithms that process this data structure should be aware of this.

## Abstraction layers

Our search application is partitioned into layers (contact, naming, routing),
each encapsulating components that process information from the corresponding
data planes. This allows all the layers to run concurrently; operations that
retrieve remote objects do not block other layers, so that can do other work
whilst waiting for the results.

Each layer follows a basic structural template:

- A set of data sources (agents and their judgements). These may be large data
  structures, so we follow a model that supports partial loading - the "local
  view" of the object. This can be visualised as a table, where the rows are
  agents, and columns are resources. Each cell can hold an attribute, or a
  sentinel value ("not yet loaded" or "attribute doesn't exist").[^laytb]

- As input from lower layers, a seed map of data sources to their scores. Other
  input might also be needed, depending on the specific layer.

- An algorithm for reaching more agents and resources. Generally, this is done
  by traversing out-arcs, starting with our seed data sources and the query
  subject tag. The specific choice of which paths to follow first, depends on
  both the layer and the implementation.

- As output to upper layers, a resource-value map, or an object that is built
  from such a map (in which case the map itself might not be made visible).

- Various algorithmic components for constructing this output. These include at
  least a score-inferer and a value-composer (see [information aggregation -
  simplification](#simplification)). These components are independent from the
  layer itself, and may be replaced with more sophisticated components.

Upper layers can request more input data from lower layers, if needed[^layex].
The top layer (routing) receives requests from the user, and the bottom layer
(contact) uses data from the social network only, independently of any query.
Note that data retrieval can in theory proceed until the entire network has
been searched. In practice, we only automatically send requests for more data
until a set number of results have been returned; after that we wait for user
input before retrieving more.

The interaction between layers is co-ordinated by implementing each layer as a
state machine - it cannot receive requests during a state transition (ie. when
it is currently processing a previous request). This simplifies the execution
logic, and reduces the number of critical objects that need to be synchronised
on. The state machines generally follow the pattern of `[NEW → AWAIT_INPUT ↔
IDLE]`; specific cases are discussed in the _traversing objects_ sections of
the various layers, below.

[^laytb]: Our implementation doesn't actually use a table structure, but this
is a simple and useful description, which is basically equivalent.

[^layex]: eg. if all available seed indexes have been exhausted, and nothing
useful returned, we need to ask for more.

To give an overview of the layers and how they fit together, below is a table
of the inputs and outputs of each layer:

-------------------------------------------------------------------------------
layer           requires                        provides
--------------- ------------------------------- -------------------------------
routing         seed indexes $H_s$              results table $\bar h$
                address scheme $\bar T$

naming          seed tgraphs $G_s$              $\bar T$
                query subject tag $t_0$

contact         own identity $z_0$              $G_s$, $H_s$
-------------------------------------------------------------------------------

Table: data dependency between layers [DIAG] pandoc table formatting not-so-good here

### Contact

Requires
:	- implicit $z_0$, from the user
Provides
:	- $G_s$ to [naming](#naming)
	- $H_s$ to [routing](#routing)

#### Aggregating information

Agent
:	social identities
Resource
:	tgraphs, indexes
Seed map
:	implicitly, the input seed identity $z_0$ mapped to the maximum score.
Score-inferer
:	Here, the problem reduces down to that of a trust metric. Developing one
	is outside the scope of this project, and various algorithms exist already
	[REF]. A real deployment of this system would use the latest available
	techniques; our prototype only returns an identity's immediate neighbours.
	[EXTN]
Value-composer
:	Our prototype uses the crude [mean-based](#value-composer) value-composer.
	For a discussion on what would be an appropriate $\alpha$ function to use,
	see [using the value-composer: on ptables](#on-ptables).

#### Constructing output

We have two outputs to send to two different layers; we just split up our
aggregate resource-value map by object type, to get $G_s$, $H_s$.

#### Traversing objects

This would normally be part of the trust metric component. Since our prototype
only returns the direct neighbours of the seed identity, there is nothing to
traverse, so we skip this component for now.

### Naming

Requires
:	- $t_0$ from the user
	- $G_s$ from [contact](#contact)
Provides
:	- $\bar T$ to [routing](#routing)

#### Aggregating information

Agent
:	tgraphs
Resource
:	tags (nodes), tag-tag relationships (arcs)
Seed map
:	the input map $G_s$
Score-inferer
:	Our prototype uses the crude [path-based](#score-inferer) score-inferer. We
	consider a route $g \rightarrow^t g'$ only if the traversal algorithm has
	passed through it, which restricts it to tags relevant to our query.
Value-composer
:	Our prototype uses the crude [mean-based](#value-composer) value-composer.
	For a discussion on what would be an appropriate $\alpha$ function to use,
	see [using the value-composer: on tgraphs](#on-tgraphs).

#### Constructing output

We first construct an aggregate tgraph by passing every node and arc (from all
tgraphs) through the value-composer. If we haven't yet loaded a node or arc in
a tgraph, we treat it as non-existent in that tgraph.

We then construct an address scheme from this, rooted at the source tag for the
query. This is done by applying Dijkstra's algorithm to the aggregate tgraph.
The distance between nodes is given using the intersection-based [distance
relation](#distance-relation).

We don't restrict the arcs to just being the shortest-path-tree; we instead
only restrict them to the extent that nodes only have in-arcs from neighbours
with a smaller "shortest-distance-from-source". This is intended to give some
extra flexibility when routing. So, the resulting address scheme is a directed
acyclic graph, rather than a tree.

#### Traversing objects

see "practical details" [LINK]

### Routing

Requires
:	- $H_s$ from [contact](#contact)
	- $\bar T$ from [naming](#naming)
Provides
:	- $\bar h$ to the user

#### Aggregating information

Agent
:	indexes
Resource
:	tag-document relationships (arcs)
Seed map
:	the input map $H_s$
Score-inferer
:	Our prototype uses the crude [path-based](#score-inferer) score-inferer. We
	consider a route $h \rightarrow^t h'$ only if the traversal algorithm has
	passed through it, which restricts it to tags relevant to our query.
Value-composer
:	Our prototype uses the crude [mean-based](#value-composer) value-composer.
	For a discussion on what would be an appropriate $\alpha$ function to use,
	see [using the value-composer: on indexes](#on-indexes).

#### Constructing output

Aggregating the information from various indexes gives us a map $E_{\barh}$ of
tag-document arcs to attributes. For all $(t, d) : w$, where $w = P(t|d)$, we
want to normalise this to $(t_0, d) : w'$, where $w' = P(t_0|d)$.

For any tag $t$, let $[t_i]_0^n$ be the shortest path from $t_0$ to $t$, where
$t = t_n$. From the address scheme, we have the distance between $t$ and $t_0$:

$$
D(t) = D[t_i]_0^n = \prod_{i=0}^{n-1} P(t_{i+1}|t_i)
$$

If we assume that the pairs $P(t_{i+1}|t_i)$, along with $P(t_n|d)$, are all
independent[^routin] of each other, then we have:

$$
\begin{array}{rl}
D(t)
=& \prod_{i=0}^{n-1} P(t_{i+1}|t_i) \\
=& \prod_{i=0}^{n-1} P(t_{i+1}|t_i, \ldots, t_0) \\
=& P(t_n, \ldots, t_1|t_0) \\
\end{array}
$$

so that:

$$
\begin{array}{rl}
D(t) \frac{P(t_0)}{P(t_n)} P(t_n|d)
=& P(t_n, \ldots, t_1|t_0) \frac{P(t_0)}{P(t_n)} P(t_n|d) \\
=& P(t_{n-1}, \ldots, t_0|t_n) P(t_n|d) \\
=& P(t_n, \ldots, t_0|d) \\
\end{array}
$$

Since $[t_i]$ is the shortest path, we assume that $P(t_n, \ldots, t_0|d)
\approx P(t_0|d)$. So, for each $(t, d) : w$, we have:

$$
w' = D(t) \frac{P(t_0)}{P(t)} w
$$

and generate a results map $d : w'$ for the user. If a document is pointed to
by more than one tag, we get a $w'$ for each tag; we simple pick the most
favourable one and only use that.

[^routin]: This assumption is unlikely to be true, but it's a common assumption
in information retrieval, and appears to give reasonable results.

#### Traversing objects

see "practical details" [LINK]


# Practical details

## System components

### Using the value-composer

To recap, $\alpha(a, r)$ is the probability that $a$ judges $r$ to be worthless
(as opposed to not knowing about it), given that $r \notin A_a$.

#### On ptables

Various factors that could affect $\alpha(a, r)$ include the size of the ptable
(how many resources $a$ knows about in total), and whether their friends also
know about $r$. Developing a satisfactory model of this would be complicated,
and the benefits are unclear. Complex models generally need more information,
which raises the cost of calculation.

Due to these reasons, for our prototype we simply set $\alpha$ to be constant.
As a preliminary estimate, we'll use a low value of $2^{-4}$, intended to
convey the notion that most agents only have a limited view of the network.

#### On tgraphs

Attribute of a node $v$: with tgraphs, node-attributes are expected to be
defined if the agent has information to do this, regardless of what the actual
attribute value is. So we set $\alpha = 0$.

Attribute of an arc $e$: as discussed in [value-composer](#value-composer), we
can make a distinction between arcs whose endpoints are both defined in the
graph, and the opposite case. So we'll use two constants, a high $\alpha_1$ for
the former, and a low $\alpha_2$ for the latter.

A further tweak for node attributes is to take the geometric mean instead of
the arithmetic mean, by converting probabilities to entropies before passing
them through the composer, then reverting the result back. This is because we
expect agents' views of the network to follow a power-law distribution, which
means the judgements for a tag's size will be distributed in the same way.
Taking the arithmetic mean of probabilities would bias the result in favour of
larger sizes.

#### On indexes

As in the case of tgraphs, we use two constants for the two cases of arcs, a
high $\alpha_1$ and a low $\alpha_2$.

### Traversing objects

#### Naming layer

First a definition: we say a tag $t$ is _completely loaded_ (or _complete_ for
short) in $g$ if the attributes for itself, all its out-arcs, and all its
out-nodes have been loaded. This is the maximum amount of information that can
be retrieved from the current set of resources, for calculating the distance
between $t$ and its out-nodes. The address scheme constructor needs this.

Recall that a layer has a table holding the local views of agents' judgements.
We mark a column for a tag-resource $t$ as _complete_ in the table, if $t$ is
_complete_ in all rows $g$ currently in the table.

We initialise the table rows from the seed tgraphs $G_s$, and start with no
columns. The table can be expanded in two possible ways:

Add a row
:	For a new tgraph $g$, _completely load_ all the tags that are currently
	complete in the table, so that the addition of $g$ as a row does not make
	any (previously completed tags) incomplete.

Add a column
:	For a new tag $t$, _completely load_ it in all rows $g$, so that $t$ is
	then complete in the table.

After each wave of new data, the aggregated resource-value map, and then the
address scheme, is recalculated.

We have the address scheme constructor stop when it reaches a tag which is not
complete in the table. Of course we could continue with incomplete information,
but we thought it would be more prudent to wait until all the relevant data
becomes available. This should help to reduce instability in the output, which
is important because another layer (routing) uses it; if it varies wildly
between successive waves then that layer might not work so well.

So the address scheme will have a maximum of one incomplete tag, and possibly
references to tgraphs not yet added to our table. The incomplete tag, along
with the tgraph with the "shortest-distance-to-source", become valid candidates
for being added to the table.

In choosing between the expansion methods, we prefer adding columns to adding
rows; we only add a new row if there is no incomplete tag to add, or if it is
"further" than some predefined threshold, relative to the nearest tgraph. (In
our prototype, we set this to $2^{-2}$.)

This preference is based on the assumption that most naming needs should be
satisfiable by a small neighbourhood around our seeds, and that judgements of
tag-tag relationships should be similar across the entire network. (If someone
claims to know a tag's related tags, then it's likely that this knowledge
covers a large part of all related tags defined by the entire network.)

#### Routing layer

Define the _potential_ of a lookup $(t, h)$ to be $P(t_0, t, h)$. We estimate
this as $P(t_0|t) P(t|h) P(h)$[^trvrpo], which assumes that $P(t_0|t)$, $P(h)$
are independent.

[^trvrpo]: ie. similarity of $t$ to $t_0$, relevance of $h$ to $t$, and score
of $h$, respectively.

As with the other layers, we have a table of loaded attributes. In this layer
however, we can be more focused when searching through indexes. Instead of
looking up all the tags for an entire row, we only partially load each row; all
non-loaded cells are assumed to be non-existent in the index for the row, until
they are loaded.

We initialise the table rows from the seed indexes $H_s$, and an empty address
scheme (ie. no columns). The table can be expanded in two possible ways:

Add a row
:	For a new index $h$, select all tags that lie on a path between $t_s$ and
	$t_0$ in the address scheme, where $t_s$ is any tag that we reached $h$ by.

On address scheme update
:	Our prototype implementation has no way of detecting how the address scheme
	changed, so we just re-add all the rows as above. For seed rows, we select
	_all_ tags in the address scheme, since "reached-by" doesn't apply to seed
	tags. Of course, lookups that have already completed, are skipped.

Unlike naming, no other functional component is dependent on the output of this
layer; the results are only returned to the user. So we shift our focus from
output stability to responsiveness - updating results as soon as possible when
new information arrives.

We retrieve data asynchronously, with pending lookups held in a priority queue,
ordered by lookup potential. When the user asks for more results, we return
immediately after picking an expansion method and adding its relevant lookups
to the queue, instead of waiting for them to finish. The user can poll query
progress; each time this occurs, we generate aggregated results using the data
retrieved so far.

The aggregated results might also contain references to new indexes as well as
documents, in which case the most relevant index (scored in the same way as
documents; this is described elsewhere) becomes a valid candidate for being
added to the table.

The choice of expansion method proceeds as follows. Define the _potential_

- of a not-yet-added tag $t$, to be the independent union of the potentials of
  its lookups over all rows $h$.
- of a not-yet-added index $h$, to be the independent union of the potentials
  of the lookups that _would be selected_ if $h$ were to be added as a row.
- of the current lookups queue, to be the highest potential of any lookup
  currently contained within it.

We compare the potential of the incomplete tag $t$, the new-index candidate
$h$, and the current lookups queue. (If any of these do not exist, they are
ignored.) If the most favourable potential out of these is:

- that of the lookups queue, then we wait for lookups to continue
- that of $t$, then we request an address scheme update
- that of $h$, then we add it as a row

Finally, if all three are non-existent (ie. no pending lookups, no incomplete
tag, and no indexes to add), then we request an address scheme update.

## Optimisation

[MOVE] to appendix

### Data structures

- `ptable`
	- quick partition of `index` vs `tgraph` nodes [$G_s$, $H_s$]
	- optionally order these by their score [possible future use]

- `tgraph`, `index`
	- where applicable:
	- quick lookup of node (and weight)
	- quick lookup of node's out-arcs (and weight) [routing, naming]

- `index`
	- quick partition of tag's to-`index` vs to-document arcs
	- optionally order these by their score [routing]

- $\breveg$
	- same as `tgraph`

- $p_s$, $\breveg$, $\breveG$, $\breveH$
	- might want to make these use CombinedWeight objects instead of a float
	  "weight", which in the future could be expanded to include a variance...

- $\breveT$
	- quick lookup of node (and weight)
	- quick iteration through all nodes [$\breveQ$]
	- quick comparison of nodes by their distance ordering [$\breveP$]
	- quick lookup of node's in-arcs (and weight) [$\breveQ$]

- $\ddotg \in \img \breveG_*$, $\ddoth \in \img \breveH_*$
	- quick lookup of node (and weight)
	- quick iteration through all nodes, arcs [$\breveg$]
	- quick lookup of node`s in-arcs (and weight) [routing, naming]
	- quick lookup of node's out-arcs (and weight) [maybe needed by some
	  scoring modules]
	- quick one-time check that all of a node's out-arcs (and weight) have
	  been retrieved from the network [routing, naming]

- $\breveQ$, $\breveR$
	- quick iteration of all lookups/results [$\breveP$, $\breveh$]
	- an advanced implementation would allow items to be added and dynamically
	  ordered in priority, bypassing the need to have $\breveP$, $\breveh$.


### Retrieval of remote objects

Usually we only need to retrieval part of a `tgraph` or `index`, eg. the weight
of a single node, or its out-arcs.

- eg. for quick "no" answer on lookups of storage objects - bloom filters

### Caching storage objects

- eg. cache commonly-retrieved objects like `ptable`s

The contact layer is independent of any query, so this can be done in the background
at any time. We can cache data for the layers above, which will help to
increase performance for future queries. etc...

[MORE] on incremental updates etc.

### Incremental state updates

- eg. when updating $\breveG$ from $\breveG_*$, we should only need to
  recalculate the parts that are affected by the updated....

# Design

[MOVE] probably to "information aggregation"

## Score-inferer

Here we explore a crude algorithm to infer scores using the link structure of
a data network. This is intended for use by the naming and routing layers, and
as discussed previously, we base our model on the likelihood of reaching a
given agent from the seeds.

Let $x$ be the agent we want to infer a score for. A simple model is to assign
a path-score for the shortest path between $x$ and each seed agent $a$, then
take the independent union of all of these path-scores:

$$
s_x = 1 - \prod_a (1 - p_{ax})
$$

Note that this is open to attack by multiple colluding seed nodes, which breaks
the assumption of independence.

Let $k$ represent the probability that an endorsement of a subject also implies
endorsement of an arbitrary neighbour, and let $i$ be the number of steps in
the shortest path from $a$ to $x$. Then $k^i$ is a rough estimate for the
probability that an endorsement extends to a node $i$ steps away. Combining
this with the weight of the original endorsement gives:

$$
p_{ax} = s_a.k^i
$$

Again, this is a very crude model, which ignores several important factors and
assumes that $k$ is universally constant. However, we weren't able (in the time
given) to develop any significantly better models, so we've stuck with it for
our prototype. We try to given an underestimate of $k$, to give a bias towards
agents nearer our seed set, which hopefully offers slightly more resistance to
simple attacks. Our prototype uses $k = 2^{-4}$.

## Value-composer

Here we explore a crude algorithm to aggregate value judgements from a set of
agents. This is required by all the layers, and the model presented here is
general enough to be applied to all of these.

It's impossible to determine the accuracy of the input data from only the data
itself, since any input is potentially true. We need either a pre-existing
expection of what the information should look like, or a model of how its
meta-information (eg. its source agents) affects its accuracy. Both approaches
are outside of the scope of this project.[EXTN]

For our prototype, we use a very basic algorithm - the score-weighted mean of
each agent's value judgement for that resource. In our case, both the score and
the value are bounded, and it's hoped that this can hinder some basic attacks
against mean-based composers (eg. judging a value to be infinity).

Recall the problem specification:

Given
:	- a set of agents, with their inferred score, and resource-value maps
	$$
	a : (s, \{ r : v \})
	$$
Return
:	- a target resource-value map
	$$
	r : v
	$$

The score-weighted mean value for resource $r$, over all agents, is:

$$
\bar v_r = \frac{\sum_{a} s_a v_{ar}}{\sum_a s_a}
$$

However, not every agent will have a judgement for $r$, so some $v_{ar}$ may be
undefined. In such cases, we make an estimate $\hat v_{ar}$ instead.

$$
\bar v_r = \frac{\sum_{a} s_a \hat v_{ar}}{\sum_a s_a}
\quad ; \quad
\hat v_{ar} = \left\{ \begin{array}{llr} \\
  v_{ar} & : r \in R_a & (0) \\
  \alpha(a,r).0 + (1 - \alpha(a,r)).\bar v_r & : r \notin R_a & (1) \\
\end{array} \right
$$

where $\alpha(a,r)$ is the probability that agent $a$ has judged resource $r$
to be worthless, given $(1)$.

The above definition has $\bar v_r$ on the RHS; after rearranging, we get:

$$
% should be \dfrac but LaTeXMathML doesn't support amsmath commands...
\bar v_r = \frac{\sum_{a:(0)} s_a v_{ar}}{\sum_a s_a \alpha_0(a,r)}
\quad ; \quad
\alpha_0(a,r) = \left\{ \begin{array}{ll} \\
  1 & : (0) \\
  \alpha(a,r) & : (1) \\
\end{array} \right
$$

The behaviour of $\alpha$ will depend greatly on the context in which the above
formula is used. Our system deals with two types of resources, nodes and arcs;
we note the following observations when implementing heuristics for $\alpha$:

**For a node** $v$, there is no information that could be used to perform any
heuristic: a node not in the map, has no information relating to it in the map.
However, it may be possible to extract information from other sources, such as
the friends for a ptable. These are context-specific and are explored in the
appropriate sections.

**For an arc** $e = (v_s, v_t)$, we have some extra information. The meaning of
$e$ is fully determined by $v_s$, $v_t$, which means that the agent is able to
judge $e$ if it "knows about" both $v_s$ and $v_t$. We can estimate this notion
with $v \in \rft E$. That is, if $e \notin \dom E$ but $v_s, v_t \in \rft E$,
then it's likely that there is no such relation.[^vcmped]

[^vcmped]: Or, the agent has overlooked $e$. We assume this is _unlikely_ for
the average agent, which is reasonable since we already trust them enough to
be processing their judgement table.

Brief discussions of specific applications of all of the above is given in the
[relevant sub-sections](#using-the-value-composer).

More sophisticated methods include training on a prototype network, or dynamic
learning from an active network. However, it must be noted that the theoretical
model for this component is not particularly robust anyway, so the benefit-cost
ratio of such methods is likely to be low.


# Comparison

Our design differs considerably from these; for example, REMINDIN's design is
more ontology focused, whereas ours draws from existing non-semantic routing
principles, DHTs in particular. Our design also has a well-defined (albeit
underdeveloped) model of the address space (unlike Harnessing). [MORE] also
move this to a better place...


