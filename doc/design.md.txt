% Design
% Ximin Luo

# Search framework

Here, we present a high-level overview of the framework we developed, and the
reasoning for our design choices.

## Data model

Our design uses a data model based on the analogy of tags being addresses. In
the discussion below, we apply ideas such as _size_ and _similarity_ to tags.
This is developed more precisely in [address space](#address-space); we omit it
here to give an overview of the entire system.

A common theme is to collect information from distributed autonomous objects.
All our objects are structures that map items (nodes, arcs) to attributes. To
avoid potential ambiguity, we refer to such objects as _agents_[^dataob], and
such items as _resources_.

[^dataob]: We also refer to the objects' maintainers as agents; the context
should make it clear what we mean.

### index

An **index** is a set of arcs. Each arc is either a tag-document or tag-index
relation, and its attribute represents their _similarity_.

This is analogous to a routing table: tags are addresses, documents are hosts,
and tag-document relationships are routes. We write $h \rightarrow^t d$ to mean
that $h$ defines a route to $d$ via $t$.

Scalable routing tables must compact information. Instead of defining a route
to every host on a subnet, a single route is defined for the entire subnet.
Likewise, indexes shouldn't point to other indexes using all possible relevant
tags, but it should summarise this information. Rather than listing another
index under entries for several dozen instances of e.g. strategy games, it
could instead list it only under "strategy games".

Along with a routing table, we need an address scheme to know which queries to
make; this is discussed in the next section.

### tgraph

A **tgraph** (tag-graph) is a graph. Each node is a tag or a pointer to another
tgraph, and its attribute represents its _size_. Each arc is either a tag-tag
or tag-tgraph relation, and its attribute represents their _similarity_.

This defines relationships between tags, which gives a precise way to perform
query-shifting (as mentioned earlier). This is similar to a naming service, but
instead of resolving names to an absolute address in a more structured space,
it defines the relative _similarity_ between names.[^namela] (Note that in our
system, names _are_ addresses.)

[^namela]: We'll still call this the naming layer though, since it occupies the
same place in the routing stack.

We define routes between tgraphs in the same way as we do for indexes - using
tags as addresses. This might seem problematic, since the point of tgraphs is
to provide an address scheme over tags, but we need such a scheme to route
between tgraphs. However, we believe that in practice this won't a problem:

- We can try to construct the address scheme as we go along.

- There are many more documents than (semantically distinct) tags. This should
  mean that naming information is much more saturated than routing information,
  and so routing between tgraphs isn't so vital. (Arguably, this is a part of
  our own experience: people are likely to have an idea of what a tag's related
  tags are, since this is part of everyday communication.)

- The previous observation also applies to the query initiator - most people
  will have some understanding of what their own query means. When unable to
  proceed, the system could ask them to supply their own "related tags".

This principle can be summed up as "language unites". Just as the ubiquity of
mathematics allows numerical addresses to be routed over, we exploit the
semi-ubiquity of language to allow tags to be routed over.

### ptable

A **ptable** (preference table) is a set of nodes. Each node is a pointer to an
index or a tgraph, and its attribute represents how likely it can satisfy a
query.

Since we are routing between documents, rather than people, we split the social
network from the tgraphs network and the indexes network. Each identity $z$ has
an associated ptable. By exploring the ptables of our friends and other trusted
peers, we can infer a set of trusted "seed" indexes and tgraphs to initiate our
query from.

This gives several advantages. It allows for space reductions - people often
agree about things, so identical parts of objects can be spun off into another
object, and pointed to instead of repeated. Also, this model is more general;
anything that applies to it, also applies to the case of one-object-per-person.
Finally, it's arguably more realistic, since we often agree with someone's
judgements, without trusting them as friends.

## Architecture

Our search application is partitioned into layers (contact, naming, routing),
each encapsulating components that process information from the corresponding
data planes. This allows all the layers to run concurrently; operations that
retrieve remote objects do not block other layers, so that they can do other
work whilst waiting for the results.

Each layer follows a basic structural template laid out in the diagram below.
Circles represent algorithmic modules, and square blocks represent data
structures that these modules act upon.

<object class="diagram" data="res/layer.svg" type="image/svg+xml" width="560" height="560"></object>

The components are as follows:

- A set of **data sources** (agents). These may be large objects, so we follow
  a model that supports partial loading - the _local view_ of the object. This
  can be visualised as a table, where the rows are agents, and columns are
  resources. Each cell can hold an attribute, or a sentinel value ("not yet
  loaded" or "attribute doesn't exist").

- A **seed map** of agents to their scores, as input from lower layers.

- A **score-inferer**, which uses the seed map and the data sources, to infer a
  score for every agent in use.

- A **value-composer**, which uses the inferred scores and the data sources, to
  compose an aggregated data source.

- An **output-factory**, which uses the aggregated object and additional input
  from lower layers, to construct output for higher layers.

- A **traversal algorithm** for reaching more agents and resources. The general
  approach is to traverse routes, starting from seed agents and the query tag.

[^laytb]: Our implementation doesn't actually use a table structure, but this
is a simple and useful description, which is basically equivalent.

Higher layers can request more input data from lower layers, if needed[^layex].
The top layer (routing) receives requests from the user, and the bottom layer
(contact) uses data from the social network only, independently of any query.
Note that data retrieval can in theory proceed until the entire network has
been searched. In practice, we only automatically send requests for more data
until a set number of results have been returned; after that we wait for user
input before retrieving more.

[^layex]: e.g. if none of the current seed indexes return anything useful, we
need to ask for more.

<object class="diagram" data="res/layers.svg" type="image/svg+xml" width="400" height="400"></object>

The above diagram depicts the inputs and outputs of each layer, and how they
relate to each other and to the subject query. The labels represent outputs,
whereas the arrowheads represent the inputs, divided into two types:

Input seeds
:	The routing and naming layers both receive their seed map from the contact
	layer. The contact layer itself implicitly receives a single seed identity,
	the query initiator's own identity. (This can be thought of as a singleton
	map of the seed identity mapped to the maximum possible score.)
Other input
:	The contact layer accepts no input other than the seed identity; it outputs
	seed indexes and tgraphs. The naming layer accepts the query tag as extra
	input, and outputs an address scheme. The routing layer accepts this
	address scheme, and outputs a results table for the user.

Some components have similar implementations in multiple layers, _in our
prototype_. These are:

Value-composer
:	We use our [mean-based](#value-composer) value-composer in all the layers,
	but with [different parameters](#picking-alpha) for each layer.
Score-inferer
:	For the contact layer, this is the job of the trust metric. For naming and
	routing, we use our [path-based](#score-inferer) score-inferer. We model a
	path $(x, x')$ only if the traversal algorithm has passed through a route
	$x \rightarrow^t x'$[^scorla], and ignore all other routes. This is to
	restrict our path hops only to the tags relevant to our query.

[^scorla]: i.e. if the resource representing the route has been loaded into the
table

The output factories and the traversal algorithms are different for each layer;
we will discuss each separately.

### Contact

This is the simplest layer. The problem of infering scores reduces down to that
of a trust metric. This is an algorithm that assigns an objective score to each
agent, given agents' subjective scores of other agents and a single seed agent.
Developing a trust metric is beyond the scope of our project; there are various
existing algorithms [`04Lev`][`09Da+`]. A real deployment of this system would
use state-of-the-art methods; our prototype only returns the seed identity's
immediate neighbours.

**Constructing output** is straightforward. We have an aggregated ptable; we
just split it up by object type to get two tables, one with only tgraphs in,
and one with only indexes in.

**Traversing objects** would normally be closely linked to the trust metric and
its output. Since our prototype metric only returns the neighbours of the seed
identity, there is nothing to traverse, and we skip this component for now.

### Naming

**Constructing output**. We construct an address scheme rooted at the query
tag, by applying Dijkstra's algorithm to the aggregate tgraph. Node distance
is given by the intersection-based [distance relation](#distance-relation).

We don't use the shortest-path-tree directly as the address scheme; we instead
use it to create a pruned copy of the aggregate tgraph, where nodes only have
in-arcs from neighbours nearer to the root tag. This is intended to give extra
flexibility. The resulting address scheme is then a directed acyclic graph,
rather than a tree.

**Traversing objects**. The address scheme is used by the routing layer for its
operation. We therefore try to reduce instability between successive updates,
by only constructing the address scheme from a self-contained set of data, i.e.
a selection of tags such that all the tags are loaded in all the rows[^nametr].
Output is only returned from the last time the table was in this condition, so
our implementation of this layer effectively blocks on this. Our traversal
algorithm therefore tries to achieve such a condition as quickly as possible.

[^nametr]: which ensures that the part of the address scheme for these tags
will stay the same even if we add another tag, and help to minimise the change
if we add another row.

We initialise the table rows from the seed tgraphs, and start with no tags.
When we receive a request for more data, we can pick one of:

- add a row, by loading all currently-used tags in it
- add a tag, by loading it in all currently-used rows

We choose this by comparing the "distance-to-root" of the next tag-to-be-added
with that of the next tgraph-to-be-added. If both are non-existent, we pass the
request onto the contact layer.

We give preference to adding tags, and only add a row if its distance is
significantly smaller than any tag candidate. This is based on the assumption
that most naming needs should be satisfiable by a small neighbourhood around
our seeds, and that judgements about tags should be similar over the entire
network.

### Routing

**Constructing output**. Let $t_0$ be our query tag. In our aggregate index,
for each $(t, d)$ arc with attribute $w$, we combine $w$ with the similarity
between $t$ and $t_0$. This gives a $w'$ that represents the similarity between
$t_0$ and the document.

We collect all of these $d : w'$ into a results table. If we have more than one
tag for a document $d$, we get more than one $w'$; we simply use the most
favourable one. See [ranking results](appendix.html#ranking-results) for the
specifics of how attributes are combined.

[^routar]: There are also tag-index arcs; we ignore these for simplicity, but
the same applies to these.

**Traversing objects**. No other functional component depends on the output of
this layer; the results are only returned to the user. So we shift our focus
from stability to responsiveness - updating results as soon as possible when
new information arrives. We retrieve data asynchronously, with pending lookups
held in a priority queue, ordered by their "potential", a measure of how likely
the lookup is to succeed based on the information we have so far. When the user
polls for output, it is constructed from the data retrieved so far.

We can also be more focused in what lookups to attempt. Instead of looking up
all the tags in our address scheme for any index, we only lookup the tags that
are related to the ones we originally reached that index by. (If this index is
a seed, then we are forced to lookup all tags.)

We initialise our table rows from the seed indexes, and an empty address scheme
(i.e. no columns). When we receive a request for more data, we can pick one of:

- allow the currently queued lookups to continue, i.e. do nothing.
- add another data source (row in the table), by selecting some lookups and
  pushing this to the pending queue.
- pass the request onto the naming layer, which will eventually update the
  address scheme.

We choose this by comparing the potential of the current lookups queue, and the
potential of the lookups that _would be added_ if we were to add that source,
or that new tag to the address scheme. If these are all empty or non-existent,
we pass the request onto the naming layer.


# Theory

## Address space

As discussed [earlier](intro.html#initial-observations), routing schemes use a
structured address space that has two properties over its addresses:

- A measure of the size of an address[^addrse]
- A measure of the overlap or similarity between two addresses.

[^addrse]: To keep our words simple, we make no distinction between addresses
and sets of addresses.

In CIDR, "size" is given by the subnet mask, and "similarity" is given by
matching the subnet prefix against the entries in the routing table. For DHTs,
both "size" and "similarity" are defined with the choice of address space. In
the case of semantic routing, a tag's "size" can mean how "general" it is, and
its "similarity" to another tag can mean how semantically related they are.

Most information retrieval models [`98Cr+`] focus on associating documents with
tags; we also need to model the relationships _between_ tags. We came up with a
fairly simple probabilistic model for this. Define:

- $d \circ t$ represents the event ($d$ satisfies a query for $t$)
- $R_{d,t}$ is a binary r.v. with $R_{d,t}(1) = P(d \circ t)$
- $D$ is a r.v. evenly distributed over all documents.
- $E_X(f(X))$ is the expected value of $f(X)$ over $X$.

$R_{d,t}$ are the "fundamental" variables of our representation of the world,
that must be measured (somehow); the expressions given below are combinations
of these. We define _similarity_ and _size_ as follows:

- an index arc $(t, d)$ has _similarity_: $P(d \circ t) = R_{d,t}(1)$

- a tgraph arc $(t, t')$ has _similarity_: $E_D(P(D \circ t | D \circ t'))$

- a tgraph node $(t)$ has _size_: $E_D(P(D \circ t))$

We interpret a tgraph $g$ as the union of all its tags, and an index $h$ as the
union of all its documents[^addrlk]. Then:

- an index arc $(t, h)$ has _similarity_: $P(\bigvee_{d \in h} d \circ t)$

- a tgraph arc $(t, g)$ has _similarity_: $E_D(P(D \circ t | \bigvee_{t' \in g} D \circ t'))$

- a tgraph node $(g)$ has _size_: $E_D(P(\bigvee_{t' \in g} D \circ t'))$

[^addrlk]: Note that these ignore how well-connected the objects are, which
would also affect its usefulness. Again, we'll overlook this for the sake of
simplicity.

In the rest of our analysis, we simplify this greatly. Instead of considering
$R_{d,t}$ as a random variable, we take a "typical query" and collapse this
down into either $0$ or $1$, so that either $d \circ t$ or not. Then in the
average case, and defining $P(x) = E_D(P(D \circ x))$:

- a tgraph arc $(t, t')$ has _similarity_: $P(t'|t) = \frac{|\{d \in D | d
  \circ t' \wedge d \circ t\}|}{|\{d \in D | d \circ t\}|}$

- a tgraph node $(t)$ has _size_: $P(t) = \frac{|\{d \in D | d \circ t\}|}{|D|}$

and similarly for $(t, g)$ and $(g)$. This is arguably easier to visualise, and
also gives a simple way to calculate these attributes.

(In retrospect, it might have been better to use $P(t, t')$ for arc attributes
and store an undirected tgraph; they contain the same information.)

### Distance relation

Let $(M, \circ)$ be a **monoid** with a _linear ordering_ over $\sqsubseteq$,
_identity element_ $I$, and which only contains _non-negative_ elements, i.e.
$\forall m \in M: I \sqsubseteq m$. A **distance relation** over a set $S$ is a
partial function $D \subseteq S \times S \to M$ satisfying:

Identity
:	$\forall a,b \in S : D(a,b) = I \iff a = b$

This is consistent with the intuitive notion that "adding components to a path
never makes it shorter".

We refer to the tuple $(M, I, \sqsubseteq, \circ)$ as the _relation type_. A
probability-based relation of type $((0, 1], 1, \geq, \times)$ can be converted
into an entropy-based relation of type $([0, \infty), 0, \leq, +)$ by taking
the negative-log of each value, and vice-versa by taking the inverse-exponent.

We want to define a distance relation over tags, so that we can construct a
routing scheme over them. One simple approach is $D[t_0, t_1] = P(t_1 | t_0)$,
which forms a valid distance relation with the probability-based relation type
$((0, 1], 1, \geq, \times)$, giving a path-distance formula of $D[t_i]_0^n =
D[t_0, t_1].\cdots.D[t_{n-1}, t_n]$.

A significant flaw is the lack of a "natural" interpretation for what this
measure represents. It also gives no information about tag triples, and hence
cannot distinguish between cases where triple intersection sizes differ greatly
- although this is more a problem with the tgraph design.

Unfortunately, we didn't have time to find better alternatives. It does satisfy
the distance relation axioms, meaning that we can use Dijkstra's algorithm to
construct routing schemes, so we decided to stick with it.

## Score-inferer

Here, we explore a crude algorithm to infer score ratings for a set of agents.
This is required by the naming and routing layers. The agents there (tgraphs
and indexes) represent immutable information, rather than social identities.
This means that the problem is different from that of a trust metric. We won't
attempt to formalise this, but intuitively, we infer scores based on how much
the original endorsements of the seed agents _extend_ to agents, rather than
how much the seed agents should _trust_ them.

Given
:	- a set of agents, each with paths to other agents
	- a seed agent-score map
Return
:	- a target agent-score map, that covers all agents in the input

We model paths between agents by selecting routes as discussed in
[routing](#routing).

Let $x$ be the agent we want to infer a score for. A simple model is to assign
a path-score for the shortest path between $x$ and each seed agent $a$, then
take the _independent union_ of all of these path-scores:

$$
s_x = 1 - \prod_a (1 - p_{ax})
$$

Note that this is open to attack by multiple colluding seed nodes, which breaks
the assumption of independence.

Let $k$ represent the probability that an endorsement of a subject also implies
endorsement of an arbitrary neighbour, and let $i$ be the number of steps in
the shortest path from $a$ to $x$. Then $k^i$ is a rough estimate for the
probability that an endorsement extends to a node $i$ steps away. Combining
this with the weight of the original endorsement gives:

$$
p_{ax} = s_a.k^i
$$

Again, this is a very crude model, which simplifies several important factors
and assumes that $k$ is universally constant. However, we weren't able (in the
time given) to develop any significantly better models, so we stick with it for
our prototype. We try to give an underestimate of $k$, to give a bias towards
agents nearer our seed set, which hopefully offers slightly more resistance to
simple attacks. Our prototype uses $k = 2^{-4}$.

## Value-composer

Here, we explore a crude algorithm to aggregate value judgements for a set of
resources. This is required by all the layers, and the model presented here is
general enough to be applied to all of these. The problem can be specified as:

Given
:	- a set of agents, with their inferred score, and resource-value judgements
	$$
	a : (s, \{ r : v \})
	$$
Return
:	- a target resource-value map
	$$
	r : v
	$$

It's impossible to determine the accuracy of the input data from only the data
itself, since any input is potentially true. We need either a pre-existing
expection of what the information should look like, or a model of how its
meta-information (e.g. its source agents) affects its accuracy. Both approaches
are outside of the scope of this project.

We will use a very basic algorithm - the score-weighted mean of each agent's
value judgement for that resource. In our case, both the score and the value
are bounded, and it's hoped that this can hinder some basic attacks against
mean-based composers (e.g. judging a value to be infinity).

The score-weighted mean value for resource $r$, over all agents, is:

$$
\bar v_r = \frac{\sum_{a} s_a v_{ar}}{\sum_a s_a}
$$

However, not every agent will have a judgement for $r$, so some $v_{ar}$ may be
undefined. In such cases, we make an estimate $\hat v_{ar}$ instead.

$$
\bar v_r = \frac{\sum_{a} s_a \hat v_{ar}}{\sum_a s_a}
\quad ; \quad
\hat v_{ar} = \left\{ \begin{array}{llr} \\
  v_{ar} & : r \in a & (0) \\
  \alpha(a,r).0 + (1 - \alpha(a,r)).\bar v_r & : r \notin a & (1) \\
\end{array} \right
$$

where $\alpha(a,r)$ is the probability that $a$ has judged $r$ to be worthless,
given $(1)$.

The above definition has $\bar v_r$ on the RHS; after rearranging, we get:

$$
% should be \dfrac but LaTeXMathML doesn't support amsmath commands...
\bar v_r = \frac{\sum_{a:(0)} s_a v_{ar}}{\sum_a s_a \hat\alpha(a,r)}
\quad ; \quad
\hat\alpha(a,r) = \left\{ \begin{array}{ll} \\
  1 & : (0) \\
  \alpha(a,r) & : (1) \\
\end{array} \right
$$

The form of $\alpha$ will depend on the use context; this is discussed below.

### Picking $\alpha$

In some cases, the set of useful resources is sparse, e.g. if the attribute
represents an endorsement (in ptables), or a non-trivial binary relationship
(routes). We'd like to not define the non-useful resource at all, to avoid
redundancy, and only implicitly mark it with a neutral or "zero" attribute.

In other cases, all attributes are useful (tgraph nodes), and there is no such
thing as a zero attribute. Here, we expect to explicitly mention all nodes that
we are aware of, so $\alpha = 0$.

Most of the $\alpha(a, r)$ estimates we use in our prototype are constant. More
sophisticated estimates would use information about both $a$ and $r$, such as
the size and neighbours of $a$. We largely avoided this approach to keep things
simple; moreover, complex models generally need more information, which raises
the cost of calculation.

For arc resources (e.g. routes), we have a simple non-constant heuristic which
isn't too costly to calculate. Consider an arc $e = (v_s, v_t)$ in a graph $G$.
The meaning of $e$ is fully determined by $v_s$, $v_t$, so any agent that can
judge both $v_s$ and $v_t$ individually, should be able to judge $e$. That is,
if $e \notin G$ but $v_s, v_t \in G$, then $\alpha$ is high[^vcmped], and for
$v_s \notin G$ or $v_t \notin G$, $\alpha$ is low.

[^vcmped]: Either the agent judges $e$ to be not worth mentioning, or it has
overlooked the possibility of $e$. We assume the latter is _unlikely_, which is
reasonable since we already trust them enough to be asking them for judgements.

In summary:

ptable (nodes)
:	We use a low constant of $2^{-4}$, intended to reflect the fact that most
	agents only have a limited view of the network.
tgraph (nodes)
:	We use $\alpha = 0$, as discussed.
tgraph (arcs), index (arcs)
:	We use $\alpha = 2^{-1}$ and $\alpha = 2^{-4}$ for the high and low cases
	respectively, as discussed in the simple heuristic for routes.

A further tweak for tgraph nodes is to take the geometric mean instead of the
arithmetic mean. This is because we expect agents' views of the network to
follow a power-law distribution, which means the judgements for a tag's size
will be distributed in the same way. Taking the arithmetic mean of would bias
the result in favour of larger sizes.

More ambitious methods might include training on a prototype network, or
dynamic learning from an active network. It must be noted however, that the
underlying theoretical model is not particularly robust anyway, so their actual
benefit is unclear.

