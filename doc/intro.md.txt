% Introduction
% Ximin Luo

# Introduction

## Background

Searching for information is an essential component of any network. Without it,
there might as well not be a network in the first place.

The world wide web is the largest information network ever created; currently,
search is a service; providers employ crawlers to navigate this network, and
extract and summarise information from the documents visited, into a form which
is suitable for insertion into a database. "Searching the web" usually means
querying these pre-built databases, rather than dynamically routing your way
through the web's content.

Preparing and maintaining the database is extremely resource-intensive for a
large network, which results in high barriers to entry. The utility of a search
service increases with how much of the network it covers; new providers must
recreate a database of comparable size before clients will switch, or else use
indexing algorithms that produce decidedly better results than alternatives.

This creates conditions of oligopoly, which is inherently vulnerable to both
abuse and attack. Large providers are trusted by a great number of clients, so
more people are affected when this trust is broken. We have already seen cases
of providers censoring their search results, both voluntarily[^censor1] and
under coercion[^censor2]. Privacy is also a concern: providers can monitor
client usage of the service, and build up a profile of personal activities.

[^censor1]: Google stopped returning results to CNET's website, because they
published an article that the CEO disliked. [REF]

[^censor2]: Google was also forced by the Chinese government to censor many
search results from its service in China. [REF]

Another issue is the depth and granularity of search topics. Most of us don't
use a search provider for every item of information we need; instead, we often
issue a query that gives us a selection of related sites from all over the web,
then manually browse within these sites to target our needs more precisely. In
addition, some websites have non-public information, or specialist knowledge
that generic search algorithms aren't able to index effectively. In these
cases, central index databases are inadequate.

An alternative approach is to perform dynamic routing using query semantics.
Instead of a simple client-provider model where a single query is a single
transaction, we propose a co-operative model where queries are routed between
autonomous providers, and results aggregated for the end user. Small providers
can index their own local sections of the network, and access and results can
be fine-tuned using local information. In addition, clients will have a wider
choice of who to trust.

Of course, decentralised systems have their own issues, and these are briefly
discussed [below](#potential-issues). We believe that these are all practical
issues that are ultimately solvable in the long run, whereas centralisation is
an inherent problem in itself.

Imagine your browser acting like a router; you type in a search query and it
automatically follows links between pages to reach what you want. Of course,
this is a long way off, and it may well be beyond the capabilities of current
hardware and networks, but hopefully this project makes a useful contribution
in that direction.

## Related systems

There are many existing systems which use decentralised, co-operative, dynamic
routing algorithms, such as the internet and various peer-to-peer overlay
networks. There are a great variety of different objectives, approaches, and
models, but some common themes include:

Key-based routing
:	The network defines an address space and a distance metric, where each
	address is represented by a binary key. (DHTs, Freenet, GNUnet)
Mesh networks
:	Various heuristics are used to maintain structure and performance, such as
	random walks, bandwidth detection, index delegation, etc. (Gnutella)
Social relationships
:	Nodes prefer to peer with trusted friends. This provides better security
	properties, and a more predictable network structure. (OneSwarm, Freenet)

A more comprehensive survey of peer-to-peer searching is presented in [^surv1].
Using the terminology of that paper, this project develops and evaluates a
_probability-based model_ of information retrieval intended to support
_comprehensive keyword search_ (as opposed to _partial keyword search_).

[^surv1]: John Risson, Tim Moors, Survey of research towards robust
peer-to-peer networks: Search methods, _Computer Networks_, **Volume 50**,
**Issue 17**, 5 December 2006, Pages 3485-3521, ISSN 1389-1286, DOI:
[10.1016/j.comnet.2006.02.001](http://www.sciencedirect.com/science/article/B6VRG-4JD0XYW-1/2/07e1ec0ba8cbe65f8f094cd99612b149).

Existing research into peer-to-peer semantic search includes [^sem1][^sem2].
None of these are currently deployed in real systems, and we were unaware of
them during the initial stages of our project. A discussion of these systems
and how they relate to our project is given [later](#comparison), after our
system has been described first.

[^sem1]: [REF] REMINDIN'
[^sem2]: [REF] Harnessing.


# Preparation

## Objectives

We intend to build a system that can potentially offer a similar functionality
to existing search providers, although obviously our prototype will be nowhere
near as sophisticated nor efficient. Core functional aspects include:

Semantics
:	Query subjects have semantic relevance to the results, so the routing
	algorithm and address scheme must reflect this.
Reach
:	It should be feasible to locate all the data matching a given query on the
	entire network (or connected component).
Robustness
:	Query paths and returned results should be resistant against subversion,
	such as spam floods or data poisoning.

Most major currently-deployed systems have at least one incompatibility with
the above. For example, DHTs are scalable, and will reach data if it exists on
the network, but addresses have no relation to the semantics of the data. Many
mesh networks (eg. Gnutella) can perform keyword search, but do not attempt to
reach all relevant data on the network.

## Potential issues

A system which must route queries between autonomous providers will obviously
be slower than a system that only needs to query a single provider. At present,
this is signficiant - web search engines return results almost instantaneously,
whereas DHT queries on a medium-sized network might take a minute to complete.
However, systems only get better, not worse, and so by the time research in
this area is mature, it's entirely possible that performance will have improved
beyond the limit of human perceptibility.

Existing large service providers might have little incentive to participate in
a co-operatiave search system, since they are each competing for control over
the market. However, this is less of a factor for smaller providers who would
otherwise be unable to attract many users, since their intention is only to
provide search capabilities. In principle, this is no different from displaying
links to other websites to help your visitors find what they want - linking to
useful content increases your own utility, even if this is not reciprocated.

As for constructing a decentralised system in the first place, it's widely
acknowledged that building these to be both scalable and secure is a difficult
problem. However, nothing suggests that this is an inherently impossible task;
and once a problem is solved, future generations may reap the benefits without
having to expend the initial development cost. Recent designs based on social
networks have been promising, and we will use this as an inspiration.

## Initial observations

Our first ideas drew upon our experiences and pre-conceptions on how we humans
try to locate things. Two themes stand out:

- keeping knowledge on who knows what, and use this to direct query routes.
- shifting the query subject to increase recall or precision - eg. broadening
  increases the recall, and re-specialising increases the precision

We also drew from our existing background knowledge on decentralised storage
networks, such as Freenet and other Distributed Hash Tables. DHTs are very
efficient and scalable, most systems giving O(log n) performance[^dhtperf] in
the size of the network. These generally have a well-defined address space with
a distance metric, where each address can be represented as a binary key. This
allows for fairly simple, yet effective, routing algorithms.

[^dhtperf]: eg. expected number of hops for a lookup

An informal explanation is that a numerical address space can be partitioned
hierarchically, eg. by taking successively larger prefixes of an address. This
allows greedy routing to work effectively, ie. by finding the neighbour which
shares the smallest partition with the desired target node (in IP routing, this
is just "longest-prefix-match", which we are all familiar with).

[DIAG] diagram of hierarchical partitioning, [REF] kleinberg "small world"

Two of these ideas seemed to fit well together - if we made the query-shifting
aspect of "human" routing more precise, by using the idea of a structured
address space, then this could also allow for a simple routing algorithm.
Semantic tags are not as naturally structured as numerical addresses however,
and a significant part of our time would be spent in developing a theoretical
model of a partitionable and navigable space over tags.

We did not have specific ideas on how to ensure the global reach of a query.
Using tags as addresses means that each address points to many resources; being
able to reach all the resources for a single tag is equivalent to being able to
reach any arbitrary resource for that tag. Our DHT-inspired design aims to
support the latter; however, bounding the cost of finding extra resources is a
more complex problem, which we didn't have time to explore.[EXTN]

Finally, we based our system on a social foundation. Research has shown that
analysing network structure can offer resistance against malicious information.
A major reason is because social relationships are much more expensive to
attack than simple algorithms that make naive assumptions about input data.
Examples include Google's PageRank[REF], trust metrics such as Advogato[REF],
and sybil detection algorithms such as SybilInfer[REF].

## Working assumptions

Since the aim of the project is fairly ambitious, we want to repeat as little
work as possible. Therefore, we make various assumptions about the environment
that our system will run under. Some of these are reasonable, and some of these
are fairly restrictive; however, we feel they are prudent and necessary given
our resource limits.

Locating known objects is essentially a solved problem: the internet and DHTs
both offer ways to retrieve objects based on a globally unique address. Without
this primitive we cannot continue; data must be stored somewhere, and it must
be accessible to arbitrary agents on the network.

We assume that objects are always available. In a real network, this can be
implemented with proxy services, or it might be a property of the network (e.g.
DHTs). This simplifies our design, since it allows us to avoid dealing with the
issue of churn, by delegating it to an external component.

This assumption also forces us to design an iterative routing algorithm rather
than a recursive one. Since data cannot forward queries onto other pieces of
data, a client must process it themselves, determine which objects to retrieve
next, then retrieve those, etc. Hopefully, our design can be adapted into a
recursive one, but we won't consider this problem.

Finally, we only consider single-phrase queries, that correspond to a single
key in a lookup table. We exclude compound queries (multiple phrases composed
with e.g. intersections, unions, differences). This helps to keep the basic
problem simple; compound query methods can be developed later, and arguably on
top of a solution to this simpler problem.


# Comparison

Our design differs considerably from these; for example, REMINDIN's design is
more ontology focused, whereas ours draws from existing non-semantic routing
principles, DHTs in particular. Our design also has a well-defined (albeit
underdeveloped) model of the address space (unlike Harnessing). [MORE] also
move this to a better place...

