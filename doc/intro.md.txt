% Introduction
% Ximin Luo

# Introduction

## Background

The world wide web is the largest information network ever created; currently,
search is a service; providers employ crawlers to navigate this network, and
extract and summarise information from the documents visited, into a form which
is suitable for insertion into a database. "Searching the web" usually means
querying these pre-built databases, rather than dynamically routing your way
through the web's content.

Preparing and maintaining the search database is extremely resource-intensive
for a large network, which results in high barriers to entry. The utility of a
search service increases with how much of the network it covers; new providers
must recreate a database of comparable size before clients will switch, or else
use indexing algorithms that produce decidedly better results than competitors.

This creates conditions of oligopoly, which is inherently vulnerable to both
abuse and attack. Large providers are trusted by a great number of clients, so
more people are affected when this trust is broken. We have already seen cases
of providers censoring their search results, both voluntarily and being forced
to it[^censor]. Privacy is also a concern: providers can monitor client usage
of the service, and build up a profile of personal activities.

[^censor]: For example, legal pressure in the form of censorship laws and
takedown notices, but also self-censorship over perceptions of sensitivity.

An alternative approach is to perform dynamic routing using query semantics.
Instead of a simple client-provider model where a single query is a single
transaction, we propose a co-operative model where queries are routed between
autonomous providers, and results aggregated for the end user. Small providers
can index their own local sections of the network, and access and results can
be fine-tuned using local information. In addition, clients will have a wider
choice of who to trust.

Of course, decentralised systems have their own issues, and these are briefly
discussed [later](#potential-issues). We believe that these are all practical
issues that are ultimately solvable in the long run, whereas centralisation is
an inherent problem in itself.

## Related systems

There are many existing systems which use decentralised, co-operative, dynamic
routing algorithms, such as the internet and various peer-to-peer overlay
networks. There are a great variety of different objectives, approaches, and
models, but some common themes include:

Key-based routing
:	The network defines an address space and a distance metric, where each
	address is represented by a binary key. For instance, Distributed Hash
	Tables (DHTs) e.g. [`01St+`], Freenet [`wFre`], GNUnet [`wGnN`].
Mesh networks
:	Various heuristics are used to maintain structure and performance, such as
	random walks, bandwidth detection, index delegation, etc. For instance,
	Gnutella [`wGtl`].
Social relationships
:	Nodes prefer to peer with trusted friends. This provides better security
	properties, and a more predictable network structure. For instance,
	OneSwarm [`wOSw`], Freenet.

A more comprehensive survey of peer-to-peer search is presented in [`06Ri+`].
Using the terminology of that paper, this project develops and evaluates a
_probability-based model_ of information retrieval intended to support
_comprehensive keyword search_ (as opposed to _partial keyword search_).

Existing research into peer-to-peer semantic search include [`04Te+`][`07Co+`].
None of these are currently deployed in real systems; we were unaware of them
during the initial stages of our project, and our design differs considerably
from these.


# Preparation

## Objectives

We intend to build a system that can potentially offer a similar functionality
to existing search providers, although obviously our prototype will be nowhere
near as sophisticated nor efficient. Core functional aspects include:

Semantics
:	Query subjects have a dynamic user-supplied meaning, rather than a static
	system-defined meaning; the routing algorithm and address scheme must
	reflect this.
Reach
:	It should be feasible to locate all the data matching a given query on the
	entire network (or connected component).
Robustness
:	Query paths and returned results should be resistant against subversion,
	such as spam floods or data poisoning.

Most major currently-deployed systems have at least one incompatibility with
the above. For example, DHTs are scalable, and will reach data if it exists on
the network, but addresses have no relation to the semantics of the data. Many
mesh networks (eg. Gnutella [`wGtl`]) can perform keyword search, but do not
attempt to reach all relevant data on the network.

## Potential issues

A system which must route queries between autonomous providers will obviously
be slower than a system that only needs to query a single provider. At present,
this is signficiant - web search engines return results almost instantaneously,
whereas DHT queries on a medium-sized network might take a minute to complete.
However, technology only gets better, not worse, and so by the time research in
this area is mature, it's entirely possible that performance will have improved
beyond the limit of human perceptibility.

Existing large service providers might have little incentive to participate in
a co-operatiave search system, since they are each competing for control over
the market. However, this is less of a factor for smaller providers who would
otherwise be unable to attract many users, since their intention is only to
provide search capabilities. In principle, this is no different from displaying
links to other websites to help your visitors find what they want - linking to
useful content increases your own utility, even if this is not reciprocated.

As for constructing a decentralised system in the first place, it's widely
acknowledged that building these to be both scalable and secure is a difficult
problem. However, nothing suggests that this is an inherently impossible task;
and once a problem is solved, future generations may reap the benefits without
having to expend the initial development cost. Recent designs based on social
networks have been promising [`10LLK`], and we take inspiration from these.

## Initial observations

Our first ideas drew upon our experiences and pre-conceptions on how we humans
try to locate things. Two themes stand out:

- keeping knowledge on who knows what, and use this to direct query routes.
- shifting the query subject to increase recall or precision - eg. broadening
  increases the recall, and re-specialising increases the precision

We also drew from our existing background knowledge on decentralised storage
networks, such as Freenet and other Distributed Hash Tables. DHTs are very
efficient and scalable, most systems giving O(log n) performance[^dhtperf] in
the size of the network. These generally have a well-defined address space with
a distance metric, where each address can be represented as a binary key. This
allows for fairly simple, yet effective, routing algorithms.

[^dhtperf]: eg. expected number of hops for a lookup

Informally, a numerical address space can be partitioned hierarchically, e.g.
by taking successively larger prefixes of an address. This allows greedy
routing to work effectively, by finding the neighbour which shares the smallest
partition with the desired target node. (For instance, in IP routing, this is
just the "longest-prefix-match" method.)

[DIAG] diagram of hierarchical partitioning, [`99Kle`] kleinberg "small world"

Two of these ideas seemed to fit well together: we can try to make the
query-shifting aspect of "human" routing more precise, by using the idea of a
structured address space. This could then allow for a simple routing algorithm.
Semantic tags are not as naturally structured as numerical addresses however,
and a significant part of our time would be spent in developing a theoretical
model of a partitionable and navigable space over tags.

As for ensuring the global reach of a query, if we use tags as addresses then
each address will point to many resources. Being able to reach all resources
for a single tag is equivalent to being able to reach any arbitrary resource
for that tag. Our DHT-inspired design aims to support the latter; however,
bounding the cost of finding extra resources is a more complex problem, which
we didn't have time to explore.

Finally, we based our system on a social foundation. Research has shown that
analysing network structure can offer resistance against malicious information.
A major reason is because social relationships are much more expensive to
attack than simple algorithms that make naive assumptions about input data.
Examples include Google's PageRank [`98Br+`], trust metrics such as Advogato
[`wAdv`], and sybil detection algorithms such as SybilInfer [`09Da+`].

## Working assumptions

Since the aim of the project is fairly ambitious, we want to replicate as
little work as possible. Therefore, we make various assumptions about the
environment that our system will run under. Some of these are reasonable, and
some of these are fairly restrictive; however, we feel they are prudent and
necessary given our resource limits.

- **Locating known objects is essentially a solved problem**: the internet and
  DHTs both offer ways to retrieve objects based on a globally unique address.
  Without this primitive we cannot continue; data must be stored somewhere, and
  it must be accessible to arbitrary agents on the network.

- We assume that **objects are always available**. In a real network, this can
  be implemented with proxy services, or it might be a property of the network
  (e.g. DHTs). This simplifies our design, since it allows us to avoid dealing
  with the issue of churn, by delegating it to an external component.

- This assumption also forces us to design an **iterative routing algorithm**
  rather than a recursive one. Since data cannot forward queries onto other
  pieces of data, a client must process it themselves, determine which objects
  to retrieve next, then retrieve those, etc. Hopefully, our design can be
  adapted into a recursive one, but we won't consider this problem.

- Finally, we only consider **single-phrase queries**, that correspond to a
  single key in a lookup table. We exclude compound queries (multiple phrases
  composed with e.g. intersections, unions, differences). This helps to keep
  the basic problem simple; compound query methods can be developed later, and
  arguably on top of a solution to this simpler problem.

_The following assumptions refer to our data model, which is discussed in the
next chapter - refer there if clarification is needed._

- We consider **objects to be immutable**. For example, we intended the sharing
  of indexes via ptable pointers to be a space-saving measure, instead of an
  action of delegation. In this respect, our objects are more like git commits
  [`wGOM`] rather than web pages, and object links are more like git submodule
  pointers rather than web hyperlinks.

  This data model is more secure, and in our system it also keeps the issue of
  trust contained in the social layer, a property that some components depend
  on. However, the immutability is an implicit assumption, which is neither
  enforced nor verified in any system component. The idea was to leave this to
  the storage layer; real deployments of the system should keep this in mind.

- Our project only considers how we **navigate the networks** of objects to
  satisfy a search query. It ignores the problem of how these objects would be
  constructed and maintained. However, the data models are fairly simple, and
  is inspired by ordinary human linking habits. So these tasks should be both
  straightforward and natural.

