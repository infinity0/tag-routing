% Evaluation
% Ximin Luo

# Generating data

[MORE] why we need to generate data.

## Model

We assume the following environment:

- We have a world-set of documents $D$, and a world-set of tags $T$. Each
  document has related tags, each associated with an attribute.
- We have a social network of identities $Z$. Each identity has some known
  friends.

We present a framework for generating an index network and a tgraph network on
top of this. The model is fairly simple; although we model agents as having
limited knowledge of the world, we do not try to model inaccurate or malicious
information.

### Producer

The basic generative object, we call a **producer**. Each producer has:

- a resource set, representing the documents that it knows about
- implicitly, a tag set, containing all the tags associated by any of the
  documents in the resource set
- arcs to other producers; each arc has a source tag, and an attribute.

Producers represent data agents rather than social identities - that is, they
can produce indexes and tgraphs. Producers can link together in a _resource
relationship_; we write $p_0 \rightarrow^t p$ to mean that $p_0$ points to $p$
via a tag $t$. (Note again this is _not_ equivalent to a social relationship.)

Each resource relationship $p_0 \rightarrow^t p$ has an associated attribute
$w$, indicating the similarity of $t$ to $p$. For simplicity's sake, we only
calculate $w$ given $t$ and $p$, and ignore $p_0$. From our specification, an
arc $(t, d)$ should have attribute $w = P(t|d)$. Since $p$ is essentially a set
of documents, we take $(t, p) : w$ with $w$ being the independent union of
$P(t|d)$ over $d \in p$.

### Object production

To generate an index, we simply construct an inverted index from tags to the
resource set of $p$. Attributes for tag-document arcs are taken from the
working environment, as per our assumptions. Attributes for tag-index arcs are
taken from the similarity scores for the index's producer, as calculated above.

Generating a tgraph is more complex; we first construct an estimate $s'$ of
$|D|$ from the point of view of a source producer $p_0$. One approach is to
take the independent union of $P(p) = |p|/|D|$ over $p \in succ(p_0) \cup p_0$.
The resource sets of neighbouring producers should be positively correlated, so
this will give a size that is larger than the actual union of their resource
sets. This is intended to give a result close to what real producers would
estimate the network size to be (from their own view).

We then calculate tag sizes and tag-tag similarities by counting the relevant
sets of documents and normalising by the appropriate factor. We can calculate
sizes for neighbour tgraphs by counting the resource set of its producer and
normalising; and tag-producer similarities we already have. Finally, most tags
are unrelated, so if a similarity for a particular relation is low then we just
ignore this altogether and skip adding the arc to the resulting tgraph.

## Application

Rather than generate entirely new data from scratch, we decided to collect data
from existing resource-sharing services, and process it to fit our data model.

This has several advantages. It saves us from having to develop a model of how
agents interact with regards to resources, which is hard to emulate well. It
also helps us to make a crude evaluation on well the system is working - much
of the theory depends upon concepts like tag "size" and tag-tag "similarity",
which we will have intuitive expectations for if the tags are real phrases, but
not if they are randomly generated data.

We were aware of three online services based around social sharing of content:
Flickr[REF] (photos), Last.fm[REF] (music), and Delicious[REF] (bookmarks). We
briefly investigated each of their APIs to see which would be most suitable to
base our test data on.

Both Last.fm and Flickr have well-documented APIs; the Delicious API is still
under development. Crucially, Flickr groups can hold resources. Last.fm groups
only displays stats for members, and Delicious had no API support for groups at
the time of writing. Therefore, we decided to use Flickr.

### Flickr overview

Flickr is an online content-sharing service. As with any social networking
service, users can add other users as contacts. On Flickr, this does not need
to be reciprocated.

The basic shareable resource on Flickr is a photograph. User can upload their
own photos and associate tags to them. They can also add other users' photos as
personal favorites.

Users can create and join common-interest groups. Each group has a group pool
to hold photos specific to that interest, which members can post to.

Flickr also infers tag _clusters_, which are sets of tags that occur frequently
among common photos. (They seem to use an algorithm which infers each cluster
from on a seed triple of tags.) A tag may belong to more than one cluster; this
often corresponds to its different semantic senses. We do not add this cluster
data directly to producers, but we use it to generate resource relationships.

### Crawl strategy

We don't have enough time or resources to crawl the entire data set of Flickr,
so we need to take a coherent and self-contained subset of it.

We start with a single seed user, then perform breath-first search on the
social network (outgoing contacts), stopping when a predefined number of users
have been met. We then retrieve the groups for each user.

For every user and every group, we create a producer and construct its resource
set as follows: for user-producers, we add their own photos and favourites; for
group-producers, we add photos from its group pool but restricted to the photos
uploaded by the users we just crawled.

We then retrieve the tags for each photo, and the clusters for each tag. At
this point, we have all the data we need for constructing our test sample.

### Processing

We use both user-producers and group-producers to generate indexes. For each
producer, we pre-calculate the similarity for each tag in its tag set. We label
the tags with the highest similarities as representative tags, or _rep-tags_.
We also score documents based on which rep-tags are associated with them; the
highest-scored are labelled as representative documents, or _rep-documents_.

We generate resource relationships as follows: for each producer $p_0$ we select
the producers whose resource sets contain many of its rep-documents. We call
these the _related producers_. For each related producer $p$, we infer tags to
link to it with, by calculating intersections between the rep-tags of $p$, and
each cluster of the rep-tags of $p_0$.

This (arguably convoluted) method is intended to give a wider-ranging and less
predictable selection of tags, than merely taking the intersection of the
rep-tags of $p_0$ and $p$. This was hoped to be "more realistic", though this
is obviously open to considerable debate.

We then produce indexes according to their resource sets and these resource
relationships, using the method described [above](#object-production).

Our data set does not have any natural entities that, we believe, can provide
an adequate naming service, as tgraphs are supposed to. (None of our other
social service candidates had such functionality either.) However, we do have
preconceived ideas of the information tgraphs would contain, and how they would
be structured. So we generate new producers from the existing producers, for
producing tgraphs. Our model aims to satisfy the following properties:

- tgraph producers have a larger view of the network (ie. larger resource set)
  than index producers
- the size of each view follows a power-law distribution (as for indexes)
- the views are interest-oriented (as for indexes)

We generate super-producers by running community detection algorithms on the
indexes network. These were part of the graph library we used, and include
label propagation[REF], greedy max-modularity[REF], and walktrap[REF]. Some of
these return dendrograms rather than membership sets; we just cut these at
various intervals to get multiple membership sets.

Note that this is _not_ intended to have a deep theoretical basis, and we did
not consider the details of each detected algorithm; rather, we only wanted a
quick-and-easy way to achieve the properties listed above.

We construct the resource set of each super-producer from the union of those of
their child producers. We generate resource relationships in a similar way to
our original producers, and produce tgraphs similarly too.

Lastly, we generate ptables. We simply have each user-producer link to the
producers for the groups it belongs to (for indexes) and the super-producers
that they in turn belong to (for tgraphs). Social relationships between users
are taken straight from the unprocessed data set.


# Measurements

_We were far behind schedule by the time we finished coding both the search
application and the sample generator, and so we had very little time to devise
a comprehensive and precise set of tests. What follows is our attempt to derive
some specific measurement metrics that might give a meaningful, if extremely
rough, idea on the performance of our system, in the limited time we had left._

_The actual execution of these measurements on the results we obtained, are
described in [testing - full testing](execute.html#full-testing)._

## Address scheme

- compare inferred address scheme with true address scheme
- test whether these match our expectations (nodes further out are "more general")
[MORE]

Two measures of address scheme "similarity":

Local
:	We calculate the ideal scheme from our crawl data, restricted to only the
	tags that are part of the subject scheme.
World
:	We calculate the ideal scheme from our entire crawl data, limited to $n$
	tags, where $n$ is the number of tags in the subject scheme.

In both cases, we score the subject address scheme using the Jaccard index
(|intersection| / |union|) over the two edge sets (subject vs ideal), which
gives a measure of their similarity (0 is worst, 1 is best).

## Query results

Given a map of queries to their results $\{ (z,t) : R \}$ we want a way of
scoring the result, and comparing this against a measurement of how ideally
"hard" the query would have been to satisfy.

In terms of plotting a graph, the query rating is our independent variable, and
the results score is our dependent variable.

### Results score

There are a few standard ways of scoring a set of results for a query. Let $S$
be the documents associated with tag $t$ (which we know from our data sample),
and $R$ be the results returned by our search application. Then:

Precision
:	$p = |R \cap S| / |R|$
Recall
:	$r = |R \cap S| / |S|$
F1-score
:	$f = 2pr / (p+r) = 2 |R \cap S| / (|S| + |R|)$

It is widely acknowledged that acheiving high $p$ _and_ $r$ is hard, so we will
consider these separately. $f$ is just the harmonic mean of $p$ and $r$, which
will be high only if both $p$ and $r$ are high.

### Query rating

As for the "difficulty" of a query, we were not aware of any existing metric
that could be neatly applied to our system, with all its network components. So
we tried to come up with our own metric.

The "difficulty" can be interpreted as the "closeness" between a seed identity,
and the subject tag for a query. This is a vague concept, so we must find a way
to make it specific. We use the following approach (which is very arbitrary,
but the best we could come up with in the time we had left):

We start by looking at existing measures of closeness. One standard definition
for the closeness of a node $v$ in a graph, is the inverse-sum of the geodesic
distances to all other nodes[REF]. This does not work for disconnected graphs
where distances can be infinite; variants that work around this, include using
the sum-inverse[REF] and sum-inverse-exponent[REF] of the distances instead.

We look at this last version more closely; its advantages include having closed
formulas for closeness in simple graphs (stars, lines, etc). The closeness of a
node $v$ in graph $G = (V, E)$ is defined:

$$
C(v_0, G) = \sum_{v \in V \setminus v_0} 2^{-d_G(v_0, v)}
$$

where $d_G(v_0, v)$ is the geodesic distance from $v_0$ to $v$ in $G$.

A rough interpretation for this is how easily one can get from $v_0$ to all the
other parts of the graph as a whole. Hand-waving, this is along the lines of
what we want to use as the "difficulty" of a query - getting from our source
identity to the relevant target documents.

Before we continue, we should make precise exactly what we want to get from,
and what we want to get to. Our search application starts from a seed identity
$z$, and traverses several networks to reach a set of result documents $R$
hopefully relevant to $t$. We can make our "closeness" measure derivation
easier, by considering only one network plane - the indexes network.

We define "source" nodes $V_s$ to be the indexes that are present in $z$'s own
ptable, plus some of indexes present in the ptables of their friends.[^meassr]
We define "target" nodes $V_t$ to be the indexes that hold documents associated
to $t$. This problem is now reduced to finding the "closeness" from $V_s$ to
$V_t$, in the same graph (i.e. the indexes network).

[^meassr]: for our sample data, we select the "personal index" of each friend,
ie. the index produced from the user-producer of that friend, rather than the
group-producers. This reduces the cost of the calculation, whilst keeping the
basic idea of what we want to measure.

We can tweak the above measure to give the closeness of $v_0$ relative to a
subset $V_t$ of the nodes, rather than the entire graph:

$$
C(v_0, V_t) = \sum_{v \in V_t} 2^{-d_G(v_0, v)}
$$

(We leave out the $\setminus v_0$ requirement; this was only added in [REF] so
the closeness of an isolated graph is $0$. Further, $v_0$ is always present
in $V$, so it always removes a constant value; however in our extension below,
this isn't true, and we mustn't make an arbitrary non-constant adjustment.)

We can tweak this further, to give the closeness of a set of nodes $V_s$
relative to $V_t$, rather than a single source node:

$$
C(V_s, V_t) = \sum_{v \in V_t} \sum_{u \in V_s} 2^{-d_G(u, v)} / |V_s|
$$

We have $|V_s|$ as a normalisation factor because we doesn't need to traverse
through all the seed indexes to reach a target index; on the other hand, we
don't use $min_{u \in V_s}$ either, because having more seed indexes will
result in more work, since we can't predict which index is the minimum.

(Of course, the above definitions all assume an additive distance relation. So
in our calculations, we first convert probability attributes to entropy ones.)

Our intuition is that $C(V_s, V_t)$ will increase linearly [MORE]

A more sophisticated rating metric might be to calculate the smallest multipath
between $V_s$ and $V_t$ (i.e. minimal subgraph that contains all paths from
$V_s$ to $V_t$). However, this will probably be much more complex to calculate
efficiently; we did not have any time to explore this.

### Query steps

A third variable that we have so far ignored above is the number of steps to
run our search application for - recall that there is no termination condition
other than what the user says, and beyond that, searching the entire network.

As the number of steps increases, we expect that recall will increase, whereas
precision will increase until "near" results are exhausted, then hopefully
only decrease slightly. It would be nice to evaluate this more precisely, but
we have no time for this.

However, it's trivial to have the system print out a results report after a
given number of steps $s$, then continue and repeat. We will do this for all
queries, with $s \in \{ 32, 64, 128 \}$, and plot (results score) vs (query
rating) as described above, for the results at each of these steps, and make
brief comments on these sets of evaluations.

