% Evaluation
% Ximin Luo

# Generating data

Since our system is entirely new, there are no existing samples of data which
can be directly for its objects. Rather than generate entirely new data from
scratch, we decided to collect data from existing resource-sharing services,
and process it to fit our data model.

This has several advantages. It saves us from having to develop a model of how
agents interact with regards to resources, which is hard to emulate well. It
also helps us to make a crude evaluation on well the system is working - much
of the theory depends upon concepts like tag "size" and tag-tag "similarity",
which we will have intuitive expectations for if the tags are real phrases, but
not if they are randomly generated data.

## Data model

We assume the following environment:

- A world-set of documents $D$, and a world-set of tags $T$. Each document has
  related tags, each relation associated with an attribute.
- A social network of identities $Z$. Each identity has some known friends.

We present a framework for generating an index network and a tgraph network on
top of this. The model is fairly simple; although we model agents as having
limited knowledge of the world, we do not try to model inaccurate or malicious
information.

### Producer

The basic generative object, we call a **producer**. Each producer has:

- a resource set, representing the documents that it knows about
- implicitly, a tag set, containing all the tags associated by any of the
  documents in the resource set
- arcs to other producers; each arc has a source tag, and an attribute.

Producers represent data agents rather than social identities - that is, they
can produce indexes and tgraphs. Producers can link together in a _resource
relationship_, or _route_; we write $p_0 \rightarrow^t p$ to mean that $p_0$
points to $p$ via a tag $t$. (Note again this is _not_ equivalent to a social
relationship.)

Each route $p_0 \rightarrow^t p$ has an associated attribute $w$, indicating
the similarity of $t$ to $p$. For simplicity's sake, we only calculate $w$
given $t$ and $p$, and ignore $p_0$. From our specification, any arc $(t, d)$
should have attribute $w = P(t|d)$. So given a $(t, p)$, we consider $p$ as a
set of documents (its resource set) and calculate $w$ to be the independent
union of $P(t|d)$ over $d \in p$.

### Object production

To generate an index, we simply construct an inverted index from tags to the
resource set of $p$. Attributes for tag-document arcs are taken from the
working environment, as per our assumptions. Attributes for tag-index arcs are
taken from the similarity scores for the index's producer, as calculated above.

Generating a tgraph is more complex; we first construct an estimate $|\breveD|$
of $|D|$ from the view of a source producer $p_0$. One approach is to take the
independent union of $P(p) = |p|/|D|$ over $p \in succ(p_0) \cup p_0$. The
resource sets of neighbouring producers should be positively correlated, so
this will give a size that is larger than the actual union of their resource
sets. This is intended to give a result close to what real producers would
estimate the network size to be (from their own view).

We then calculate tag sizes and tag-tag similarities by counting the relevant
sets of documents and normalising by the appropriate factor. We can calculate
sizes for neighbour tgraphs by counting the resource set of its producer and
normalising; and tag-producer similarities we already have. Finally, most tags
are unrelated, so if a similarity for a particular relation is low then we just
ignore this altogether and skip adding the arc to the resulting tgraph.

## Application

We were aware of three online services based around social sharing of content:
Flickr (photos [`wFlr`]), Last.fm (music [`wLfm`]), and Delicious (bookmarks
[`wDel`]). We briefly investigated each of their APIs to see which would be
most suitable to base our test data on.

Both Last.fm and Flickr have well-documented APIs; the Delicious API is still
under development. Crucially, Flickr groups can hold resources. Last.fm groups
only displays stats for members, and Delicious had no API support for groups at
the time of writing. Therefore, we decided to use Flickr.

### Flickr overview

Flickr is an online content-sharing service. As with any social networking
service, users can add other users as contacts. On Flickr, this does not need
to be reciprocated.

The basic shareable resource on Flickr is a photograph. User can upload their
own photos and associate tags to them. They can also add other users' photos as
personal favorites.

Users can create and join common-interest groups. Each group has a group pool
to hold photos specific to that interest, which members can post to.

Flickr also infers tag _clusters_, which are sets of tags that occur frequently
among common photos. (They seem to use an algorithm which infers each cluster
from a seed tag-triple.) A tag may belong to more than one cluster; this often
corresponds to its different semantic senses. We do not add this cluster data
directly to producers, but we use it to generate routes.

### Crawl strategy

We don't have enough time or resources to crawl the entire data set of Flickr,
so we need to take a coherent and self-contained subset of it.

We start with a single seed user, then perform breath-first search on the
social network (outgoing contacts), stopping when a predefined number of users
have been met. We then retrieve the groups for each user.

For every user and every group, we create a producer and construct its resource
set as follows: for user-producers, we add their own photos and favourites; for
group-producers, we add photos from its group pool but restricted to the photos
uploaded by the users we just crawled.

We then retrieve the tags for each photo, and the clusters for each tag. At
this point, we have all the data we need for constructing our test sample.

### Processing

We use both user-producers and group-producers to generate indexes. For each
producer, we pre-calculate the similarity for each tag in its tag set. We label
the tags with the highest similarities as representative tags, or _rep-tags_.
We also score documents based on which rep-tags are associated with them; the
highest-scored are labelled as representative documents, or _rep-documents_.

We generate routes as follows: for each producer $p_0$ we select producers
whose resource sets contain many rep-documents of $p_0$. We call these the
_related producers_. For each related producer $p$, we infer tags to link to it
with, by calculating intersections between the rep-tags of $p$, and each
cluster of the rep-tags of $p_0$.

This (arguably convoluted) method is intended to give a wider-ranging and less
predictable selection of tags, than merely taking the intersection of the
rep-tags of $p_0$ and $p$. This was hoped to be "more realistic", though this
is obviously open to considerable debate.

We then produce indexes according to their resource sets and these routes,
using the method described [previously](#object-production).

Our data set does not have any natural entities that, we believe, can provide
an adequate naming service, as tgraphs are supposed to. (None of our other
social service candidates had such functionality either.) However, we do have
preconceived ideas of the information tgraphs would contain, and how they would
be structured. So we generate new producers from the existing producers, for
producing tgraphs. Our model aims to satisfy the following properties:

- tgraph producers have a larger view of the network (ie. larger resource set)
  than index producers
- the size of each view follows a power-law distribution (as for indexes)
- the views are interest-oriented (as for indexes)

We generate super-producers by running community detection algorithms on the
indexes network. These were a part of the graph library we used [`wiGr`], and
include label propagation [`07Ra+`], greedy max-modularity [`04Cl+`], and
walktrap [`05Po+`]. Some of these return dendrograms rather than membership
sets; we just cut these at various intervals to get multiple membership sets.

Note that this is _not_ intended to have a deep theoretical basis, and we did
not consider the details of each detected algorithm; rather, we only wanted a
quick-and-easy way to achieve the properties listed above.

We construct the resource set of each super-producer from the union of those of
their child producers. We generate routes in a similar way to our original
producers, and produce tgraphs similarly too.

Lastly, we generate ptables. We simply have each user-producer link to the
producers for the groups it belongs to (for indexes) and the super-producers
that they in turn belong to (for tgraphs). Social relationships between users
are taken straight from the unprocessed data set.


# Measurements

_We were far behind schedule by the time we finished coding both the search
application and the sample generator, and so we had very little time to devise
a comprehensive and precise set of tests. What follows is our attempt to derive
some specific measurement metrics that might give a meaningful, if extremely
rough, idea on the performance of our system, in the limited time we had left._

_The actual execution of these measurements on the results we obtained, are
described in [testing - full testing](execute.html#full-testing)._

## Address scheme

Our search application infers an address scheme from the tgraphs network. We
want a way of comparing this against an "ideal" address scheme (as implied by
our theoretical model and the actual data sample). We use two different senses
of "ideal":

Local
:	We calculate the ideal scheme from our crawl data, restricted to only the
	tags that are part of the subject scheme.
World
:	We calculate the ideal scheme from our entire crawl data, limited to the
	nearest $n$ tags, where $n$ is the number of tags in the subject scheme.

In both cases, we score the subject address scheme using the Jaccard index
(|intersection| / |union|) over the two edge sets (subject vs ideal), which
gives a measure of their similarity (0 is worst, 1 is best).

## Query results

Given a map of queries to their results $\{ (z,t) : R \}$ we want a way of
scoring the result, and comparing this against a measurement of how ideally
"hard" the query would have been to satisfy.

In terms of plotting a graph, the query difficulty is our independent variable,
and the results score is our dependent variable.

### Results score

There are a few standard ways of scoring a set of results for a query. Let $S$
be the documents associated with tag $t$ (which we know from our data sample),
and $R$ be the results returned by our search application. Then:

Precision
:	$p = |R \cap S| / |R|$
Recall
:	$r = |R \cap S| / |S|$
F1-score
:	$f = 2pr / (p+r) = 2 |R \cap S| / (|S| + |R|)$

It is widely acknowledged that acheiving high $p$ _and_ $r$ is hard, so we will
consider these separately. $f$ is just the harmonic mean of $p$ and $r$, which
will be high only if both $p$ and $r$ are high.

There are also ways to evaluate results' ranking. However, we did not give any
special consideration for ranking documents, nor for ranking our sample data.
In any case, we didn't have the time to explore such evaluation metrics;

### Query difficulty

As for the "difficulty" of a query, we were not aware of any existing metric
that could be neatly applied to our system, with all its network components. So
we tried to come up with our own metric.

To judge "difficulty", we can use some sort of "closeness" measure between a
seed identity and a query tag. This concept is more directly relevant, but is
still vague; we need a specific quantifiable measure for our tests. We came up
with the following:

$$
C(V_s, V_t) = \sum_{v \in V_t} \sum_{u \in V_s} 2^{-d_G(u, v)} / |V_s|
$$

where $V_s$, $V_t$ are the sets of indexes we start from and end at; and
$d_G(u, v)$ is the geodesic distance between two indexes $u$, $v$. For a more
detailed discussion, see [closeness](appendix.html#closeness) in the appendix.

A more sophisticated difficulty metric might be to calculate the total length of
the minimal subgraph which holds all non-redundant paths from $V_s$ to $V_t$.
However, this would be more complex to calculate efficiently.

### Query steps

A third variable that we have so far ignored above is the number of steps to
run our search application for - recall that there is no termination condition
other than what the user says, and beyond that, searching the entire network.

As the number of steps increases, we expect that recall will increase, whereas
precision will increase until "near" results are exhausted, then hopefully
only decrease slightly. It would be nice to evaluate this more precisely, but
we have no time for this.

However, it's trivial to have the system print out a results report after a
given number of steps $s$, then continue and repeat. We will do this for all
queries, with $s \in \{ 16, 32, 64 \}$, and plot (results score) vs (query
difficulty) as described above, for the results at each of these steps, and
make brief comments on these sets of evaluations.

