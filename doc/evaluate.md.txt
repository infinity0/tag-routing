% Evaluation
% Ximin Luo

# Generating data

[MORE] why we need to generate data.

## Model

We assume the following environment:

- We have a world-set of documents $D$, and a world-set of tags $T$. Each
  document has related tags, each associated with an attribute.
- We have a social network of identities $Z$. Each identity has some known
  friends.

We present a framework for generating an index network and a tgraph network on
top of this. The model is fairly simple; although we model agents as having
limited knowledge of the world, we do not try to model inaccurate or malicious
information.

### Producer

The basic generative object, we call a **producer**. Each producer has:

- a resource set, representing the documents that it knows about
- implicitly, a tag set, containing all the tags associated by any of the
  documents in the resource set
- arcs to other producers; each arc has a source tag, and an attribute.

Producers represent data agents rather than social identities - that is, they
can produce indexes and tgraphs. Producers can link together in a _resource
relationship_; we write $p_0 \rightarrow^t p$ to mean that $p_0$ points to $p$
via a tag $t$. (Note again this is _not_ equivalent to a social relationship.)

Each resource relationship $p_0 \rightarrow^t p$ has an associated attribute
$w$, indicating the similarity of $t$ to $p$. For simplicity's sake, we only
calculate $w$ given $t$ and $p$, and ignore $p_0$. From our specification, an
arc $(t, d)$ should have attribute $w = P(t|d)$. Since $p$ is essentially a set
of documents, we take $(t, p) : w$ with $w$ being the independent union of
$P(t|d)$ over $d \in p$.

### Object production

To generate an index, we simply construct an inverted index from tags to the
resource set of $p$. Attributes for tag-document arcs are taken from the
working environment, as per our assumptions. Attributes for tag-index arcs are
taken from the similarity scores for the index's producer, as calculated above.

Generating a tgraph is more complex; we first construct an estimate $s'$ of
$|D|$ from the point of view of a source producer $p_0$. One approach is to
take the independent union of $P(p) = |p|/|D|$ over $p \in succ(p_0) \cup p_0$.
The resource sets of neighbouring producers should be positively correlated, so
this will give a size that is larger than the actual union of their resource
sets. This is intended to give a result close to what real producers would
estimate the network size to be (from their own view).

We then calculate tag sizes and tag-tag similarities by counting the relevant
sets of documents and normalising by the appropriate factor. We can calculate
sizes for neighbour tgraphs by counting the resource set of its producer and
normalising; and tag-producer similarities we already have. Finally, most tags
are unrelated, so if a similarity for a particular relation is low then we just
ignore this altogether and skip adding the arc to the resulting tgraph.

## Application

Rather than generate entirely new data from scratch, we decided to collect data
from existing resource-sharing services, and process it to fit our data model.

This has several advantages. It saves us from having to develop a model of how
agents interact with regards to resources, which is hard to emulate well. It
also helps us to make a crude evaluation on well the system is working - much
of the theory depends upon concepts like tag "size" and tag-tag "similarity",
which we will have intuitive expectations for if the tags are real phrases, but
not if they are randomly generated data.

We were aware of three online services based around social sharing of content:
Flickr[REF] (photos), Last.fm[REF] (music), and Delicious[REF] (bookmarks). We
briefly investigated each of their APIs to see which would be most suitable to
base our test data on.

Both Last.fm and Flickr have well-documented APIs; the Delicious API is still
under development. Crucially, Flickr groups can hold resources. Last.fm groups
only displays stats for members, and Delicious had no API support for groups at
the time of writing. Therefore, we decided to use Flickr.

### Flickr overview

Flickr is an online content-sharing service. As with any social networking
service, users can add other users as contacts. On Flickr, this does not need
to be reciprocated.

The basic shareable resource on Flickr is a photograph. User can upload their
own photos and associate tags to them. They can also add other users' photos as
personal favorites.

Users can create and join common-interest groups. Each group has a group pool
to hold photos specific to that interest, which members can post to.

Flickr also infers tag _clusters_, which are sets of tags that occur frequently
among common photos. (They seem to use an algorithm which infers each cluster
from on a seed triple of tags.) A tag may belong to more than one cluster; this
often corresponds to its different semantic senses. We do not add this cluster
data directly to producers, but we do use it to generate content relationships.

### Crawl strategy

We don't have enough time or resources to crawl the entire data set of Flickr,
so we need to take a coherent and self-contained subset of it.

We start with a single seed user, then perform breath-first search on the
social network (outgoing contacts), stopping when a predefined number of users
have been met. We then retrieve the groups for each user.

For every user and every group, we create a producer and construct its resource
set as follows: for user-producers, we add their own photos and favourites; for
group-producers, we add photos from its group pool but restricted to the photos
uploaded by the users we just crawled.

We then retrieve the tags for each photo, and the clusters for each tag. At
this point, we have all the data we need for constructing our test sample.

### Processing

We use both user-producers and group-producers to generate indexes. For each
producer, we pre-calculate the similarity for each tag in its tag set. We label
the tags with the highest similarities as representative tags, or _rep-tags_.
We also score documents based on which rep-tags are associated with them; the
highest-scored are labelled as representative documents, or _rep-documents_.

We generate content relationships as follows: for each producer $p_0$ we select
the producers whose resource sets contain many of its rep-documents. We call
these the _related producers_. For each related producer $p$, we infer tags to
link to it with, by calculating intersections between the rep-tags of $p$, and
each cluster of the rep-tags of $p_0$.

This (arguably convoluted) method is intended to give a wider-ranging and less
predictable selection of tags, than merely taking the intersection of the
rep-tags of $p_0$ and $p$. This was hoped to be "more realistic", though this
is obviously open to considerable debate.

We then produce indexes according to their resource sets and these content
relationships, using the method described [above](#object-production).

Our data set does not have any natural entities that, we believe, can provide
an adequate naming service, as tgraphs are supposed to. (None of our other
social service candidates had such functionality either.) However, we do have
preconceived ideas of the information tgraphs would contain, and how they would
be structured. So we generate new producers from the existing producers, for
producing tgraphs. Our model aims to satisfy the following properties:

- tgraph producers have a larger view of the network (ie. larger resource set)
  than index producers
- the size of each view follows a power-law distribution (as for indexes)
- the views are interest-oriented (as for indexes)

We generate super-producers by running community detection algorithms on the
indexes network. These were part of the graph library we used, and include
label propagation[REF], greedy max-modularity[REF], and walktrap[REF]. Some of
these return dendrograms rather than membership sets; we just cut these at
various intervals to get multiple membership sets.

Note that this is _not_ intended to have a deep theoretical basis, and we did
not consider the details of each detected algorithm; rather, we only wanted a
quick-and-easy way to achieve the properties listed above.

We construct the resource set of each super-producer from the union of those of
their child producers. We generate content relationships in a similar way to
our original producers, and produce tgraphs similarly too.

Lastly, we generate ptables. We simply have each user-producer link to the
producers for the groups it belongs to (for indexes) and the super-producers
that they in turn belong to (for tgraphs). Social relationships between users
are taken straight from the unprocessed data set.


# Measurements

- ./run.sh -d../z/small -s51114580@N00 -i1000 -n64 -v2 google
- ./run.sh -d../z/small -s8072015@N04 -i1000 -n256 -v3 google

- compare inferred address scheme with true address scheme
- test whether these match our expectations (nodes further out are "more general")
- basically, some measure of how similar two graphs are?

query test:

1. score (f1_score, precision, recall)
2. steps to run the algorithm
3. closeness(id, tag)

(1) is dependent variable, it's what we're interested in
(2) is not that interesting since we expect recall better, and
(3) independent variable

plot (1) against (3) for a small set of (2)
set of (2) we can take to be [32, 64, 128, 256]


more sophisticated score would take into account the distance between results docs and actual docs



