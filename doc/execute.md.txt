% Execution
% Ximin Luo

# Development

## Project infrastructure

From the beginning, we put the entire project under version control. We chose
to use git; we feel that its content-oriented object model and its non-linear
history model is more flexible than traditional centralised systems. It's also
fast and efficient, and we have the most experience with it.

The main compilable component, the search application, was written in Java, and
so we chose Apache Ant for our build system. This is fairly simple and flexible
enough for our purposes. Maven was also considered, but dropped as we didn't
think our project needed such a heavyweight solution.

We spent a moderate amount of our effort creating test code as part of the
development process. We did _not_ attempt to write a unit test for every single
class that was implemented. This is not feasible for many classes, since many
of them are components of a larger system, and cannot function at all without
the entire system in place. However, for utility functions, data structure
classes, and other "standalone" components, we did write fairly extensive tests
for all of these. In our experience, they are the most critical components to
test, and eliminating bugs early helps greatly when finally testing the actual
application-specific logic. We used JUnit as the framework for our tests; this
is easily integrated into the build process via Ant.

Our repository has a fairly simple layout; we have seperate source directories
for application code and test code, and a source directory for documentation.
We used github to publish and backup our repository.

## Search application

We decided to develop the search application in Java. We were aware that our
design had many areas for future improvement, so we wanted our implementation
to be easily maintainable and extensible. We felt that the strict type safety
and class inheritance of Java would aid in achieving such a goal, because it
allows us to design a system architecture (and its component interfaces) to be
both self-enforcing and self-documenting. This is especially useful when coding
individual classes, where it's easy to forget about the overall picture.

### Structure

Our code structure can be divided into the following sections, each roughly
corresponding to a Java package:

`tags.proto`
:	Prototype implementation of the search application
`tags.store`
:	Adapters to provide a consistent interface for different storage layers
`tags.util`, `tags.io`
:	General utilities that the rest of the application depends upon, but is not
	otherwise directly relevant to the theory of our system.
`tags.ui`
:	Interfaces for presenting application information to the user

The prototype implementation code can be divided into:

`tags.proto.*` (objects)
:	The basic objects of our system (`PTable`, `TGraph`, `Index`) and utility
	classes for these. Some of the objects have multiple versions, which help
	to strip out unnecessary features based on use context - e.g. when we only
	need to represent a remote object, we only store outgoing arcs.
`tags.proto.*` (process)
:	The execution architecture of our system. The main classes include:
:	`LayerService`
	:	A template that all layers inherit from, which implements some basic
		functionality (e.g. receiving messages, holding query parameters) as
		described in [architecture - layers](design.html#layers).
	`QueryProcess`
	:	An ongoing query. It holds references to all the state relevant to it,
		including each of the running layers.
	`QueryEnvironment`
	:	The environment of the query - the components that a query process
		needs, but is not specific to the query itself. This includes e.g. an
		`Executor`[^archex] for scheduling jobs, and the interface to the
		storage layer.
`tags.proto.*.*` (layers)
:	The layers as described in [architecture - layers](design.html#layers). The
	"algorithmic components" are represented by Java interfaces (plus basic
	implementations, also as described), to allow for future improvements.

[^archex]: `java.util.concurrent.Executor`[REF]

The general utilities can be further divided into:

`tags.util.*`
:	Data-structure related utilities. Our object specifications are fairly
	abstract, and an easy way to implement them was to construct them out of
	union and tuple types. These aren't a native part of Java, so we built our
	own. We also implemented many methods for performing complex operations on
	maps, needed because our objects are all node-map/arc-map combinations. Our
	crude score-inferer and value-composer are both implemented here, as well
	as Probability and Entropy classes that ensure their values are restricted
	to the correct ranges.
`tags.util.exec.*`
:	Base classes for our execution architecture. `java.util.concurrent` is
	highly abstract and lacking in context-specific implementations. We weren't
	aware of any simple, light, easy-to-learn execution frameworks for Java, so
	we wrote our own. This includes `TaskService`, similar to `Executor` but
	accepts general objects to act on; and `MessageReceiver`, a basic interface
	for a simple execution framework based on message passing.
`tags.io.*`
:	This only contains deserialisation classes for GraphML, and was not needed
	until late on during the development process, when we had to extend a large
	part of JUNG's GraphML reader code - see [below](#data-format).

### Design patterns

The Java Collections Framework[REF] is simple and flexible, and we built most
of our data structures on top of it. In many cases, these provide alternative
views of existing structures, such as combining two maps into one, chaining
many iterators together, etc. It is generally more space-efficient to expose
each item as needed, rather than copying the entire collection and converting
all the items at once. The _proxy_ and _decorator_ patterns are both useful in
implementing such views; we made heavy use of our `ProxyIterable`, `ProxyMap`
classes in constructing the more complex views.

We found good use for the _adapter_ pattern. Our `StoreControl` provides a
consistent interface to interact with different storage layers; implementations
wrap around and hide these differences. We implemented both the path-based
score-inferer and the mean-based value-composer as general utility classes;
adapters were also used to wrap this functionality inside an interface expected
by the layers that used these components.

We also used the _factory_ and _builder_ patterns to make object creation code
more extensible. Use cases include creating local views of remote objects, and
deserialising objects from input streams.

### Generics

We made heavy use of generics in our implementation. In our experience, this is
a useful tool in both enforcing type safety, and in writing re-usable code.

Our design uses several types of objects without making any comment on their
type; these include tags, object addresses (in the storage layer), and social
identities. These are perfect candidates for generic type parameters.

The type of attributes can also be parameterised. Although we use probability
for all of our attributes, this is a part of our specification that is separate
from the overall architecture of the system. A full list of theoretically
distinct types is:

   Source                              Description
-- ----------------------------------- -----------------------------------
1. agent score (social)                social identity trust score
2. resource value (social) (tgraphs)   rating score for tgraphs
3. resource value (social) (indexes)   rating score for indexes
4. agent score (naming)                rating score for tgraphs
5. resource value (naming) (node)      tag size
6. resource value (naming) (arc)       tag-tag similarity
7. agent score (routing)               rating score for indexes
8. resource value (routing)            tag-document similarity

(2,4) are the same, and (3,7) are the same, which leaves us with six attribute
types. In the end we felt it prudent to merge (6,8) into a single arc-attribute
type, and also merge (2,4,3,7) into a single score type. This leaves us with
four distinct attribute types, which arguably is still too much; however, we
believe our code is modular enough to support this level of flexibility.

We end up with seven generic type parameters in total, which are all present in
the unified interface to the storage layer (`StoreControl`): `<I>` identity,
`<T>` tag, `<A>` address, `<U>` node-attribute, `<W>` arc-attribute, `<S>`
score, `<Z>` identity-score.

## Sample generator

We decided to develop the sample generator in Python. We did not need the code
here to be as strictly well-designed as for the search application. Also, by
this point we were already far behind schedule - we felt using Python would
help recover some of this, due to its simplicity and the ease of exploring new
libraries (via the interactive interpreter).

We did run into many problems, such as poor high-level multithreading support,
and performance and memory leak issues which became important at the scales we
were processing data at (we had initially overlooked this). However, ultimately
it probably did save us time, and it's unclear that using a more low-level
language would've helped the performance issues significantly.

### Libraries

We made use of the following external libraries:

flickrapi[REF]
:	Python bindings for Flickr's online API[REF].
python-futures[REF]
:	A high-level multithreading framework, inspired by `java.util.concurrent`.
	We used this to run flickrapi in parallel to reduce network IO waits; this
	is discussed below[LINK].
igraph[REF]
:	A general-purpose graph library written in C, with a python interface
	available. We used this to store and manipulate graph objects, and for
	data serialisation.

### Structure

Our code structure can be divided into the following sections, each roughly
corresponding to a Python module:

`tags.eval.crawl.flickr`
:	An extension of flickrapi, and code for data collection, as described in
	[evaluation - generating data](evaluate.html#application).
`tags.eval.objects`
:	Classes representing single objects, such as `Producer`, `Node`, a class
	for building graphs (`NodeSample`), and various classes for holding
	statistics about a sample or its components.
`tags.eval.sample`
:	Classes for generating, writing, and calculating statistics for samples.
`tags.eval.util`, `tags.eval.util.*`
:	Utility methods and classes that provide support for the rest of the code,
	but which is otherwise not directly relevant to the system.

### Performance

We had to deal with many performance issues during the implementation of the
sample generator. Despite our choice of using python, these weren't CPU-bounded
issues, but actually most involved inefficient use of blocking operations.

#### Network IO

Our Flickr API library, flickrapi, uses blocking IO to send and receive network
data, which results in a few seconds' wait before each API call completes. This
was too slow for our purposes.

To optimise this, we used multithreading to start many IO requests in parallel.
We also extended flickrapi to use persistent HTTP connections (per thread),
instead of opening a new TCP connection for each API call. Since the threads'
major task is only to wait for a system call to return, the lack of true
multithreading in Python (due to the GIL[REF]) is not a problem in this case.

An alternative approach would be to use an asynchronous event-based IO model,
like the popular Twisted library[REF]. However, we never intended the sample
generator to be production code, so we preferred the simplicity of threads,
over the flexibility and performance of event-based models.

#### Serialisation

Since we were dealing with data sizes larger than our RAM, we needed to store
(e.g.) semi-completed objects and request parameters on disk. At first, we used
Python's standard `pickle` module[REF] to de/serialise our objects, and the
standard `shelve` module[REF] to store this data. This proved inadequate, and
we had to optimise heavily.

The first major optimisation was to use a custom `pickle` format for `Graph`
objects. `pickle` was leaking huge amounts of memory when deserialising large
graphs; we tried to debug this, and discovered that storing the graph as a
gzip-compressed GraphML is much more efficient (both in storage space and
serialisation time)[REF] than the standard pickle format, and also solves the
memory leak.

The next major optimisation was to store different components of a large object
separately. The `shelve` module will deserialise an entire object when it's
requested; this turned out to be unnecessary most of the time. We ended up
refactoring the `Producer` class into a state machine, and storing the state
field in a separate table. When we want to check the state of a `Producer`, we
query this other table first. Another optimisation was to store the commonly
accessed parts of a `Graph` in an extra cache, which saves deserialising the
entire `Graph`.

Further optimisations included switching from `shelve`'s default choice of
database to `sqlite3`, and using the RAM-based `tmpfs` (we used a Debian system
for development) to store short-lived temporary files.

## Data format

We needed a common data format for sharing data between the search application
and the sample generator. After a brief look into the formats available, we
decided on GraphML[REF].

There are very few clean, simple, Java graph libraries that have good support
for serialising complex graphs. GraphML seemed to be better supported than the
DOT format[REF], and JUNG is a library that we felt most suited our needs. Even
so, we had to heavily subclass its `GraphMLReader`, which involved much poking
around its implementation details, just to read attributes of the correct type.
This also required reading a significant part of the GraphML specification.

Luckily our graph library on the Python side, `igraph`, is good at both reading
and writing GraphML, and we didn't have to do any extra work here. It can also
write in the DOT format, which meant we could visualise some of the simpler
graphs that were generated, by running them through Graphviz[REF].


# Testing

We had originally planned to do more extensive testing of our system, and for
much larger sets of data. However, unforeseen difficulties (detailed elsewhere)
along with our time constraint, meant that we had to cut back on these.

The original plan was to generate a sample from a crawl of several thousand
users. However, when we tested our generator with successively larger crawl
sizes, it became clear that this was impossible with the resources available,
even with our heavy optimisations.

We did an initial crawl of 16 users starting from a randomly selected seed, to
verify that the sample generator was working correctly. We extended the crawl
to 313 groups, and retrieved 11,000 photos and 6,457 tags. We then generated
329 indexes and 56 tgraphs. We ran the search application on this sample; in
most cases it could find most documents associated with a tag in a reasonable
amount of lookups, just because the network was so small.

The largest complete sample we managed to achieve was from an inital crawl of
256 users. We extended the crawl to 8,597 groups, and retrieved 411,872 photos
and 128,409 tags. We then generated 8,853 indexes and 438 tgraphs.

## Sample integrity

Before proceeding with testing our search application, we first did some basic
integrity tests on our generated sample. It is a common observation that social
networks have a power-law distribution in several properties, the easiest one
to calculate being the node degree.

Firstly, we verified that our crawl of the social network satisfies a power-law
distribution in node-degree:

[DIAG] socgr.svg

This is a relatively small sample size (256), but it is clear that it does
follow the expected distribution.

Then, we tested for a power-law distribution in node-degree over our generated
indexes and tgraphs[^testar]:

[^testar]: we model an arc $p_0 \rightarrow p$ if a resource relationship $p_0
\rightarrow^t p$ exists for any tag $t$.

[DIAG] sprdgr.svg

[DIAG] prodgr.svg

Clearly, the tgraphs network satisfies a power-law distribution. The plot for
the indexes network has a split in the data; this can be explained by the way
we generated arcs. Simply put, if $p_0 \cap p_1$ is similar in size to $p_0$,
but much smaller than $p_1$, then we would generate $p_0 \rightarrow p_1$ but
not vice versa. Most of the time however, links will be bi-directional; this
explains the discrepancy between odd/even degree frequencies[^testdg].

We confirmed this by counting only in-degree / out-degree, which both give
clear power-law distributions:

[DIAG] prodgr_i.svg

[DIAG] prodgr_o.svg

We also tested for a power-law distribution in the document-count for each tag,
over our crawled tags:

[DIAG] tags.svg

This may indicate that our intention on forming a hierarchical partition over
the space of tags is achievable on this data set. However, another reason for
this distribution could be that a few users upload lots of photos with the same
tags, without necessarily indicating any structure over the entire tag space.

[^testdg]: this also applies for the social network sample and the generated
tgraphs network. This discrepancy was not visible in these cases, we think,
because the sample sizes were much smaller.

## Initial tests

Before proceeding with our suite of tests, we did a few initial tests for
randomly-picked queries, to get an idea on how well the system was working.

It became apparent that our mean-based value-composer was not very effective.
To start with, we compared the "ideal"[^explad] address scheme for the sample,
with the address scheme that our search application generated from all the
separate data sources. At first, these differed greatly.

[DIAG] comparing early address schemes

The basic problem seems to be that, at least some agents will give high values
for resources that don't "deserve" them, even when all agents are honest. So,
assuming that most gaps are due to lack of information, will bias the final
result towards this inaccuracy, and sometimes will even supersede a value for
another resource, that many agents agreed with.

A much better way of combining resource values would be to model them as belief
distributions, as suggested previously. We can then use an algorithm that
reduces the variance of an aggregated distribution, formed from many similar
values. (Using the mean would fit this, as per the Central Limit Theorem.) A
recently-developed system for evaulating skill over the result of many games,
uses similar principles.[REF]

We didn't have time to implement this, since it would require redesigning our
prototype to reason about distributions and not probabilities, which is much
more complex. Instead, we just set all our $\alpha$ to be a constant $1$. This
at least allows resources with many "good" value-ratings, to override ones that
have less. (Likewise, YouTube recently switched from a mean-based 5-star rating
system to one that instead counts the number of like/dislike votes.) After we
made this change, the ideal vs. generated address schemes were much more alike.

[^explad]: i.e. according to our model of the [address
space](design.html#structured-address-space), rather than "theoretically
perfect"

Unfortunately, we didn't devise any tests for how well the score-inferer works.
This would involve simulating an attack on the system; doing this properly
would have taken us far beyond our time and resource constraints.

## Full testing

We take a random sample of 16 identities from our social network crawl, limited
to only the users that have between 4 (inclusive) and 64 (exclusive) outgoing
contacts.

This accounts for 118 users, or 0.46 of the total, with 2 above and 136 below
the range. We do this because in our preliminary testing, anything lower ends
up asking the social layer for more friends (which we have not implemented),
and anything higher uses up our RAM as it tries to load all friends' ptables.

We believe that this still gives a useful indication on how our system works;
users with less than 4 friends should be able to obtain results, once we
implement the social layer more fully.

We take a random sample of 16 tags from our tags crawl, limited to only the
tags that have between 4 (inclusive) and 4096 (exclusive) associated documents.

This accounts for 68,808 tags, or 0.54 of the total, with 59,427 above and 174
above the range. We do this for the same reason as above - lower tags give very
little results, whereas higher ones use up our RAM. This goes slightly against
one of the original aims, to find even rare documents, but we have tried to
keep the threshold low, at 4 documents, rather than a higher value.

From these, we constructed 256 queries by pairing each seed identity with each
subject tag.

We calculated the [closeness](evaluate.html#query-rating) for each query,
and also the out-degree of $V_s$ and in-degree of $V_t$.[^fullde]

[^fullde]: definition of "degree" of a set of nodes is given in the glossary.

compare closeness vs those degree measures [MORE]

[DIAG]

Run tests [MORE]

[DIAG]

(tests looks very shaky atm, some give a minor fraction of all available
results, some give nothing, a few go haywire)

## Comments

Our results are mainly poor. Rather than giving meaningful performance trends
as we hoped, we ended up with small result sets for each query, giving a
scattered sample of mostly low scores.

Instead of attempting such a general performance test as we did, we should have
focused on a few distinctive queries, and examined these in detail. In the end,
we got more useful information about our implementation by tracing the process
of a query, rather than by looking at our test results.

It was typical to obtain several dozen seed tgraphs, and several hundred seed
indexes. Our design is to query all of these for each additional tag, which
reduces performance as most of them will be irrelevant. This was the result of
a flawed data generation model; most flickr users do belong to many groups, but
not all of these are suitable as a seed. A better approach would point to most
groups only via resource relationships, rather than in ptables.[EXTN]

Our prototype turned out to be highly dependent on the traversal algorithm in
the routing layer. This seems to be a result of implementing the naming and
contact layers to execute only when requested - the routing layer then has to
make a choice between asking for data from lower layers, or to continue with
itself. The choice algorithm we specified [LINK] turned out to be poor, and we
had to tweak it manually. Even then, we would get sporadic behaviour, such as
choosing to increase the address scheme many times in a row, then choosing to
continue with lookups many times. [DIAG] (give an example in the appendix?)

The system should work better, if we instead ran every layer in parallel. This
would increase the complexity of synchronization, but we wouldn't need to
devise complex choice algorithms in the routing layer. We would need each layer
to pause itself automatically, but this seems much simpler.[EXTN]

A case could certainly be made for the sample data being of poor quality. When
evaluating generated address schemes, we found that even the "ideal" schemes
were not very intuitive (see above). Granted, our sample generation was ad-hoc,
but its products should still largely resemble the source crawl data. This also
raises the question of what a good data sample would be like, which requires a
better understanding of our address space model (or a different model).


# Conclusions

We greatly underestimated the complexity of the problem we set for ourselves.
It took far longer than expected to develop a satisfactory general theoretical
framework, as well as all the low-level specific details that a prototype would
have to implement. The size of our eventual design also added many practical
development obstacles such as finding libraries, debugging, performance, etc.

We don't think the work load could have been signficantly reduced. An aim of
the project was to implement our inital specification; removing any component
would have defeated the whole point. We were also required to collect data to
test the system with. This had to be compatible with our system; cutting down
any component would have rendered this pointless, too.

Our results were not satisfactory. Although initial tests on a tiny sample
seemed promising, results for a significantly-sized network were poor. There
are many possible reasons for this, discussed [above](#comments); however,
ultimately this means that we had far too high an expectation of our system.

On the up side, we have developed a decentralised semantic searching framework
out of fairly simple initial ideas. Although the system as a whole is highly
complex, it is made from the combination of many individual modules. These are
all well-defined and shielded from each other, so that future improvements in
one component should not need a complete restructuring of the entire system.

A full list of directions for taking this project further is available in the
appendix[LINK]. The most important ones, in our view, would be:

- make the layers run fully in parallel
- review the model of the address space
- find a better data source to run tests on
- explore better resource-value metrics

In conclusion, and despite all the setbacks, we think this project has been a
useful endeavour. Even though it has not demonstrably succeeded in its original
aims, it has given us a better understanding and overview of the problems that
must be solved in order to build a complex decentralised system, and the amount
of work involved in doing so. We hope that our project has been interesting to
readers, and that our contribution can be built upon in the future.


