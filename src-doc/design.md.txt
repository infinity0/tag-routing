% Tag-driven routing in a distributed storage network
% Ximin Luo

# Meta

## Terminology

- arc -- a directed relation $e = (v_s, v_t)$ from node $v_s$ to node $v_t$.
- document -- a storage object that can be the target of some query. This could
  include (eg.) multimedia or software; for simplicity, we'll refer to these
  all as "document"
- identity -- a user / node on the social network
- node -- in the context of networks, an object that can be connected to
  another node by an arc.
- semantics -- this term is used in two different senses; hopefully the context
  should give enough disambiguation
	- the meaning of tags, wrt. the end-user
	- the meaning of some concept, wrt. the theory of this system
- tag -- a phrase that can be semantically related to a storage object

## Symbols

With no other context, the following symbols refer to the following object
types; capitalised symbols refer to the set of all objects of the given type.

- $d$ -- document
- $t$ -- tag
- $g$ -- `tgraph`
- $h$ -- `index`
- $p$ -- `ptable`
- $z$ -- identity
- $w$ -- weight, identity score

## Notation

- For any set $E = V_s \times V_t$ of arcs, define:
	- $\src E = \{ v_s \in V_s : \exists v_t . (v_s, v_t) \in E \}$ ie. the set
	  of nodes that the arcs point from.
	- $\dst E = \{ v_t \in V_t : \exists v_s . (v_s, v_t) \in E \}$ ie. the set
	  of nodes that the arcs point to.
	- $\rft E = \src E \cup \dst E$ ie. the set of nodes that E refers to.
- We deal with two types of data structures:
	- A map $V_A : V \to A$ of nodes to their attributes.
	- A map $E_A : E \to A$ of arcs to their attributes. For convenience, define:
		- $\src E_A = \src E$
		- $\dst E_A = \dst E$
		- $\rft E_A = \rft E$
- $f \subseteq X \to Y$ -- $f$ is a partial function from set $X$ to set $Y$.
	- This is non-standard but $f : X \to Y$ is confusing.

### Data structures

In the below notation, $\cdots$ denotes some additional predicate after a
subject, that serves as a specifier or definition for it.

Map $M = \{ x \cdots \mapsto y_x \cdots \} \quad | \quad M = \{ x_i \cdots \mapsto y_i \cdots \}_{i\cdots}$
:	A data structure that maps^[This is equivalent to a partial function: $M =
	\{ x_i \in X : y_i \in Y \}_i$ means the same thing as $M \subseteq X \to
	Y$; however (in my opinion) the former is easier for an implementor to
	read.] $x$ to $y_x$, or $x_i$ to $y_i$. In the second style, $i$ is a dummy
	variable used only to specify $x$, $y$ from the same pair; the actual value
	is unimportant and both $x_i$, $y_i$ are independent from it.
List $L = [ x_i \cdots ]_{i\cdots}$
:	A data structure that imposes an ordering $i$ on a collection of items
	$x_i$.

All data structures discussed from here onwards are implicitly finite in size.

## TODO and related tags

- `DIAG`: draw appropriate diagram here
- `TODO`: open issue; resolve or explain why we leave it open
- `MORE`: expand
- `EXTN`: a pointer to an external research topic, either potential or existing
- `REF`: add references for


# Overview

Distributed network of data structures maintained by multiple independent
parties.

Most of the routing power of the system is delegated to the underlying social
network and the innate information-retrieval heuristics we have as humans,
rather than being inherently built into the system.

How the properties of the system (performance, scalability, robustness) change
with the structure of the network forms the basis of the [analysis](#analysis).

The system has three main networks / planes of information that link to each
other.

- routing plane - see the `index` section
- meta-routing plane - see the `tgraph` section
- social plane - see the `ptable` section


# Specification

## Data structures

### Weight attributes

Structure
:	$w \in W = [0,1]$
Description
:	Aids comparison and composition of multiple nodes or arcs.

The precise meaning of a weight attribute is intrinsically linked with what the
routing and the ranking algorithms do. Specific uses include:

- priority of traversal of the routing and meta-routing networks
- ranking of query results

Probability-based semantics seem quite robust and reliable, since these are
objective properties and can (in theory) be estimated algorithmically. It also
gives rise to a simple way of combining weights (multiplication), as well as a
simple interval of values $[0, 1]$.^[Theoretically, this allows low-probability
weights to be defined for any possible arc. However, the system's algorithms
favour heavy weights over light ones, so the latter should just be omitted,
since they will never be used. TODO rewrite this appropriately]

These weights allow algorithms to achieve a finer level of preference-choosing
than would otherwise be possible. However, care must be taken not to depend on
the precise values too much, or to treat the data as authoritative: even if
malicious data can be excluded, and the weights are defined on some objective
property, there will still be some degree of inconsistency between different
objects, since each publisher can only calculate weights from their subjective
(and limited) view of the entire network.

The best practical solution would be to have an automatic way of calculating
weight attributes. This would reduce the problem of human inaccuracy, as well
as laziness: it's tedious to manually estimate a weight for every object we
add to our collections. [EXTN]

### `index`

DIAG

Structure
:	$$
	h = \left[ \begin{array}{lrr} \\
	  E_h \subseteq & T \times D \cup H & \to W \\
	\end{array} \right]
	$$
Description
:	A map of arcs to their weights, where each arc is a relation from a tag to
	a document or another `index`.
Semantics
:	Probability that the arc target will satisfy a query for the source tag.

This data structure contains information on how to satisfy a search request. It
contains mappings from tags to target documents, or to another index to forward
the request onto.

A mapping indicates some semantic relevance between the tag and the document,
and a subjective judgement that a person searching for the tag did intend to
retrieve information provided by the document.

The analogy to typical navigable networks is that each tag is like an address;
and each `index` is like a routing table, mapping addresses to target entities.
^[Most "addresses" in this network will have multiple targets, but this is an
unimportant detail; eg. the internet has multicast addresses too.] So, we'll
call the network of `index` objects the "routing" plane.

Navigable networks have a well-defined addressing scheme, which enables routing
through a series of addresses to reach the target. For example, numerical
addresses (eg. IP) are naturally hierarchical, and can be prefix-matched upon
in a routing table. Tags however, do not intrinsically encode such information,
so we need an additional layer to provide it.

### `tgraph`

DIAG

Structure
:	$$
	g = \left[ \begin{array}{lrr} \\
	  V_g \subseteq & T & \to W \\
	  E_g \subseteq & T \times T \cup G & \to W \\
	\end{array} \right]
	$$
Description
:	1. A map of nodes to their weights, where each node is a tag; and
	2. A map of arcs to their weights, where each arc is a relation from a tag
	to another tag or `tgraph`.
Semantics
:	1. Saturation of the tag. (see [`tgraph` semantics](#tgraph-semantics))
	2. Relevance of the target tag to the source tag.

This data structure provides information on tags and the relationships between
tags. The presence of a tag indicates an understanding of its meaning; an arc
from a tag to a tag or `tgraph` indicates a semantic relationship between them,
and a judgement that the target can be used in some appropriate way to satisfy
a query for the source tag.

From a given root tag, we can reach other tags and `tgraph`s by following the
arcs between them. Weights on the arcs and nodes help us to adapt the traversal
order to our purposes.

The analogy to typical navigable networks is that the graph of tags provides
the same service as an addressing scheme or a name resolution system: a method
for navigating between related addresses. This works beyond the routing level,
so we'll call the network of `tgraph` objects the "meta-routing" plane.

Conceptually, a tag can be thought of as a "pure" name - ie. the name carries
no information about its referent, such as its location). IP addresses are the
opposite, and DNS names are somewhere in between the two. DNS names need to be
resolved; likewise, we have an additional system here for tags.

### `ptable`

DIAG

Structure
:	$$
	p = \left[ \begin{array}{lrr} \\
	  V_p \subseteq & G \cup H & \to W \\
	\end{array} \right]
	$$
Description
:	A map of nodes to their weights, where each node is a `tgraph` or an
	`index`. Implicitly, each `ptable` "belongs" to some identity on the
	[underlying social network](#underlying-social-network).
Semantics
:	Probability that a ... TODO

This data structure allows an identity to declare a personal list of preferred
indexes and tgraphs to use for routing purposes.

This is intended to provide an attack-resistant "bootstrapping" path onto the
routing and meta-routing planes.

MORE

## Algorithms

Since network latency is far greater than processing latency, we design these
algorithms to be inherently concurrent. Operations involving retrieval of
remote objects are non-blocking and allow other work to be done whilst waiting
for its results.

The system is partitioned into several layers, each dependent on the one below
it. The lower layers (initialisation, meta-routing, routing) correspond to the
data planes introduced previously; the top layer provides the results to the
user's original query, and interacts directly with the user.

Each layer has an interface for interacting with the layer above it, and an
operational framework that directly relates to the substance of this project.
Into this framework fits various independent components that are interesting
problems in themselves, and are too complex to explore in detail. These are
can be implemented and upgraded separately from the other parts of the system.
For now, we only suggest some basic implementations and briefly comment on
their reliability and usefulness.

The lower layers retrieve and process data from the appropriate plane. The
underlying algorithms have no "natural" termination condition; data retrieval
can theoretically proceed until the entire network has been searched. In
practice, each layer does not do this spontaneously; instead, they respond to
requests for data from the layer above. By default, we discontinue sending new
requests in all layers after a given number of results have been returned, and
allow the user to resume from the paused state, resources permitting.

### Combining data from multiple sources

Below, we describe algorithms for combining data from multiple sources. Our
model is relatively simple: we have a list of sources with a weight judgement
on how "good" each source is; in turn, each source specifies their own weight
judgement for each object that it refers to. To avoid confusion, we will call
source-weights "scores" and object-weights "values". Given a target object, we
want to derive a "combined" value for that object, using the given information.

For our prototype, we use a very basic algorithm - the score-weighted mean
value of the object. Generally, the arithmetic mean is susceptible to attacks
and distortions. Let $k$ be the proportion of the total score that the attacker
controls. Weights are [limited](#weight-attributes) to $[0, 1]$, so this will
roughly be the same as the proportion of compromised nodes.

If the value is evenly distributed, the effect of an attack will be limited in
direct proportion to $k$ - the worst thing they can do is give a single value
with $k$ influence in the final result. However, if the value is far from being
evenly distributed, it would be possible for an attacker to introduce a value
that has an extremely small probability $\epsilon$ of occuring, yet which will
still have a $k$ share of influence in the final combined value.

TODO - explain this better. possibly move into the "Design" section.

So the types of weights that are input to this algorithm must be roughly evenly
distributed in order to be secure. Most distributions probably will not cause
too many problems; however, negative-log distributions will.

The algorithms below make use of several $\alpha_i$ functions, which fine-tune
the output based knowledge of on the context of the inputs. There are various
ways of producing such functions; one method is to extract realistic values
for some constant, by collecting training data from a prototype network. More
advanced techniques include collecting data directly from the active network,
to adjust the values dynamically. [EXTN]

However, such methods are likely to be quite complex, and so this project does
not go into them. Instead, we only explore heuristics that give a rough
approximation, based on our intuitions.

#### Mean weight of a node

Given
:	- a map $M = \{ m \mapsto u_m \in W \}$ of node-maps to their weights,
	  where each $m = \{ v \in V \mapsto w_{mv} \in W \}$.
	- a given node $v \in \bigcup \dom m$
Return
:	- the map-weighted mean node-weight $\bar w_v$ of $v$

The mean weight for node $v$, over all maps, is $\frac{\sum_{m} w_{mv}}{|M|}$.
If we take into account the weights of each map, we have $\frac{\sum_{m} u_m
w_{mv}}{\sum_m u_m}$. However, not every map will necessarily contain $v$, so
some $w_{mv}$ may be undefined. In such cases, we estimate a weight instead;
see the section on [zero weights](#zero-weight-attributes) for more details.
Following on from that, we reach:

$$
\bar w_v = \frac{\sum_{m} u_m \hat w_{mv}}{\sum_m u_m}
\quad ; \quad
\hat w_{mv} = \left\{ \begin{array}{llr} \\
  w_{mv} & : v \in \dom m & (0) \\
  \alpha_1(m,v).0 + (1 - \alpha_1(m,v)).\bar w_v & : v \notin \dom m & (1) \\
\end{array} \right
$$

where $\alpha_i(m, v)$ is the probability that the author of $m$ _does_ know
about $v$ (and intentionally excluded it), given $(i)$.

The above definition has $\bar w_v$ on the RHS; after rearranging, we get:

$$
% should be \dfrac but LaTeXMathML doesn't support amsmath commands...
\bar w_v = \frac{\sum_{m:(0)} u_m w_{mv}}{\sum_m u_m \alpha(m,v)}
\quad ; \quad
\alpha(m,v) = \left\{ \begin{array}{ll} \\
  1 & : (0) \\
  \alpha_1(m,v) & : (1) \\
\end{array} \right
$$

Generally, the behaviour of $\alpha_i$ are likely to depend highly on the use
context, so this should be an additional parameter to the algorithm.

#### Mean weight of an arc

Given
:	- a map $M = \{ m \mapsto u_m \in W \}$ of arc-maps to their weights,
	  where each $m = \{ e \in E \mapsto w_{me} \in W \}$.
	- a given arc $e = (v_s, v_t) \in \bigcup \dom m$
Return
:	- the map-weighted mean arc-weight $\bar w_e$ of $e$

The formula is similar to the one from the previous section, but here we are
dealing with arcs, so we can refine our estimate somewhat further; see the
section on [zero weights](#zero-weight-attributes) for more details. Following
on from that, we reach:

$$
\bar w_e = \frac{\sum_{m} u_m \hat w_{me}}{\sum_m u_m}
\quad ; \quad
\hat w_{me} = \left\{ \begin{array}{llr} \\
  w_{me} & : e \in \dom m & (0) \\
  (1 - \alpha_1(m,e)).\bar w_e & : e \notin \dom m \;\wedge\; \{ v_s, v_t \}
    \not\subseteq \rft m & (1) \\
  (1 - \alpha_2(m,e)).\bar w_e & : e \notin \dom m \;\wedge\; \{ v_s, v_t \}
    \subseteq \rft m & (2) \\
\end{array} \right
$$

where $\alpha_i(m, e)$ is the probability that the author of $m$ _does_ know
about $e$ (and intentionally excluded it), given $(i)$.

The above definition has $\bar w_e$ on the RHS; after rearranging, we get:

$$
% should be \dfrac but LaTeXMathML doesn't support amsmath commands...
\bar w_e = \frac{\sum_{m:(0)} u_m w_{me}}{\sum_m u_m \alpha(m,e)}
\quad ; \quad
\alpha(m,e) = \left\{ \begin{array}{ll} \\
  1 & : (0) \\
  \alpha_1(m,e) & : (1) \\
  \alpha_2(m,e) & : (2) \\
\end{array} \right
$$

Generally, the behaviour of $\alpha_i$ are likely to depend highly on the use
context, so this should be an additional parameter to the algorithm.

### Initialisation

MORE blah blah, why etc

MORE only need to do this one, and then keep track of minor updates etc.

#### Obtaining initial ptables

Assume a given identity is already connected to the social network. MORE

Any system which has a social network as a major component, needs a method
of constructing such a network and sustaining its growth. Fortunately, such
systems are already widely deployed on the www; new networks can bootstrap
themselves by extracting social data from existing networks, through invites,
etc. [EXTN] TODO move this probably to "design assumptions".

For our purposes, we need an algorithm which takes our own identity as input,
and outputs a set of seed identities for our routing algorithm, with a score
associated with each identity.

Given
:	- our own identity
Return
:	- a set of seed identities, with corresponding scores

A basic implementation of this would be to return all nodes up to 2 hops away
from our own node. This would grab a decent number of nodes whilst offering
a basic level of protection against the most common and simple attacks. A
variation would be to filter out the 2-hop nodes that share very few mutual
1-hop friends with us (by some standard); this would be slightly harder to
attack.

Inferring attacks from the link structure of a social network is outside the
scope of this project. More advanced algorithms exist; a real implementation of
this system would be able to use the latest available techniques. [REF][EXTN]

#### Combining `ptable` objects

Given
:	- a map $P_s = \{p \mapsto u_p\}$ of `ptable`s to their identity scores
Return
:	- a `ptable` $p_s$ that "combines" this information, with $\dom p_s =
	  \bigcup \dom p$.

Define $G_s = \dom p_s \cap G$ and $H_s = \dom p_s \cap H$. (Note that $G_s +
H_s = \dom p_s$).

A basic implementation is just to calculate the score-weighted average of the
weight of each node. Then, to generate $p_s[v]$ for any $v \in \bigcup \dom p$,
we just pass $P_s$, $v$ and some appropriate $\alpha_1 : P \times V \to [0, 1]$
through the [general algorithm](#mean-weight-of-a-node).

To recap, $\alpha_1(p, v)$ is the probability of $v \notin \dom p$ being due to
"zero-weight". In other words, $p$ does indeed know about $v$, but has chosen
not to inform others of it. We want to make a rough estimate for $\alpha_1$.

Intuitively, the more of our friends that know about a particular item, the
more likely it is that we also know about it. Moreover, this probability is
proportionally greater at lower degrees of saturation, since it only takes one
friend to tell us about an item. Hand-waving furiously, we get:

$$\alpha_1(p, v) = \left(\frac{|F_v(p)|+1}{|F(p)|+2}\right)^\frac{1}{2}$$

where $F(p)$ and $F_v(p)$ have their intuitive meanings, and the additional
constants serve to add some uncertainty in the cases where none or all of $p$'s
friends know about a particular item, and also to avoid division by zero.

TODO revise this formula; for 0/0 friends it gives us 0.7 knowledge which is
ridiculous. TODO note that people might explicitly not point to something their
friends already point to, since that would be redundant.

For a given $p$, this requires retrieving of all of its friends' `ptable`s,
which may not be in $P_s$ depending on our seeding algorithm. To retrieve these
additional `ptable`s for every $p$ we need to resolve could be costly; an even
more rough estimate that involves much less work is just to set $\alpha_1$ to
some constant.

TODO, what constant? and merge/rewrite below paragraph

so to keep things simple for this
project, we use a constant estimate of $\alpha = \frac{1}{2}$.^[Intuitively,
for any given target, $P(like)$ and $P(know)$ are low but slightly correlated,
so $P(\neg like \wedge \neg know)$ is high, but not very high.]

TODO this used to be in "zero weight-attributes"; merge with above stuff
In practice, such methods are hard to get accurate^[A basic heuristic would be to assume that some
fraction of an identity's friends "know about" each object that its `ptable`
points to; however the real fraction is unlikely to be near-constant over all
objects, so the scheme would greatly disadvantage rare objects. We could take
this into account and estimate the "knows about" fraction based on the fraction
of identities that explicitly reference the target, but this is becoming more
and more complicated with increasingly hand-waving arguments.], and probably
won't provide significantly enough benefit to justify the additional complexity
- the social network should mitigate most serious attempts to abuse the system.

### Meta-routing

The purpose of the meta-routing layer is to provide a routing scheme to the
routing layer. This routing scheme is dynamic, and updates are requesting by
the routing layer when it needs more information.

We start from our seed `tgraph` set $G_s$. If $t_0$ does not appear in $G_s$,
we have a few backup options:

- ask the user to supply some related tags. This should not be a significant
  problem; people usually "have an idea" of what they are looking for.
- ask more nodes in the social graph for their `ptable`s. This can be done
  automatically, which makes things more convenient for a user. However, it
  increases the risk of an attack through the social network.

If $t_0$ appears in $G_s$, we proceed with meta-routing. Our query state
consists of these mutable objects, each depending on the previous object:

1. $\breveG$ -- a map of `tgraph`s to their scores, initialised with $G_s$.
2. $\breveg$ -- a partial `tgraph` that [combined](#combining-tgraph-objects)
   data from $\breveG$, initially empty.
	- Only nodes and arcs that have been retrieved from all $g \in \breveG$ are
	  included.
	- Define $t$ is "complete" in $\breveg$ if there is enough information in
	  $\breveg$ to calculate the [distance](#distance-between-tags) between $t$
	  and any of its outgoing neighbours. This will at most be: its weight, the
	  weights of its outgoing arcs, and the weights of its outgoing neighbours.
	- Since `tgraph` nodes inside a `tgraph` do not have any outgoing arcs,
	  these are always "complete".
	- `tgraph` nodes which are in $\dom \breveG$ are excluded from $\breveg$.
3. $\breveT$ -- a directed acyclic subgraph of $\breveg$, initially empty.
	- Nodes are ranked in order of its shortest distance to $t_0$, and each
	  node only has incoming arcs from nodes nearer to $t_0$ than itself.
	- This provides a quick way to obtain the set of short (ie. greedy) paths
	  from $t$ back to $t_0$. MORE explain better

$(\breveg, \breveT)$ represent the routing scheme. To populate $\breveT$ from
$\breveg$, our prototype uses Dijkstra's algorithm. Each step $n$ adds a node
to $\breveT$; for this, we need to know the distances to all the neighbours of
the shortest-path tree so far, ie. $t_i$ must be complete in $\breveg$ for all
$i < n$.

There are two ways for the routing layer to request more information from the
meta-routing layer:

Completing a tag $t$ in $\breveg$

:	If, in calculating shortest paths, we reach a $t$ for which we don't have
	the data to continue, we can pause the algorithm, and return a closure for
	retrieving the data. The routing layer can resume the algorithm when the
	data has been retrieved.

Adding a `tgraph` $g$ to $\breveG$

:	There may be `tgraph`s present in $\breveT$. The routing layer cannot use
	this directly, but can request that $g$ to be added to $\breveG$ (which may
	bring in new tags). When the meta-routing layer receives such a request, it
	needs to maintain consistency between the state objects.

	1. Remove all references to $g$ from $\breveg$.
	2. [Work out](#weight-of-new-tgraph) a weight for $g$, before adding it to
	   $\breveG$.
	3. Retrieve $g$'s weight judgements for all nodes and arcs currently in
	   $\breveg$.
		- This requires at least partially loading $g$ from the network, so it
		  should be done in the background to avoid blocking other parts of the
		  system.
	4. Update $\breveg$ with the information from $g$.
	5. Update $\breveT$ from the new $\breveg$.

#### Combining `tgraph` objects

CURRENTWORK

Given
:	- a working map $G_w = \{(g_i, u_i)\}$ of `tgraph`s to their weights
	- a source tag $t$
Return
:	- a map $\{(x_j, w_j)\}$ from `tgraph` arc targets (ie. a tag or tgraph) to
	  their weights; the source of each arc is implicitly $t$.
	  TODO fix format of this, make it more useful

A basic implementation is just to calculate the `tgraph`-weighted average of
the weight of the arc $(t, x_j)$, ie:

TODO semantics of tag-tgraph arcs

TODO adapt from general algorithms

- combine node-maps: with `tgraph`s, authors are supposed to include low-weight
  tags; we assume that absence of a tag always means "missing information", so
  let $\alpha_1 = 0$
- combine arc-maps: standard; need to work out $\alpha_1, \alpha_2$ - former
  is low, latter is high - see [analysis](#zero-weight-attributes)

then construct the correct output format, as defined in the spec above

For node-weights, the "important" thing is not the value itself, but rather
where the most significant bit occurs. In other words, the outcome-entropy of
(D tagged by v), equal to -log(P(D tagged by v)).

When we combine saturations from various sources, then I think it's more
"correct" to take the mean of the entropy, and not the saturation. I'm not sure
how to justify this rigorously, but intuitively:

- if we have 3 sources giving saturations of (1, epsilon, epsilon), the mean
saturation is 1/3, whereas the actual value is likely to be nearer epsilon.

Is there a more formal way of saying this?

Also, "mean entropy" is theoretically open to manipulation, since entropy can
go all the way to infinity. But I don't think this is too much of a problem,
because:

- there is no incentive to manipulate these node-weights, since a tag must be
well-related to other tags for the search algorithm to be affected; and
arc-weights are not susceptible to this kind of manipulation
- CPUs practically have finite-size representations of numbers, which limits
the entropy to be proportional to the width (in bits) of the representation.
Also, we could do this manually.

#### Distance between tags

TODO resync spec with new meta-routing layer architecture

Given
:	- a working map $G_w = \{(g_i, w_i)\}$ of `tgraph`s to their weights
	- a working tree $T_w$ of tags
Return
:	- an ordered list $[(t_i \in T_w, x_i, w_i)]$ of `tgraph` arcs to add to
	  the tree

see [`tgraph` semantics](#tgraph-semantics) - want to maximise $u_t.w_e$, etc

Transform the graph. For each arc $e = (t_0, t)$, let $w_e\prime = -\log(w_e.u_t)$

A basic implementation can simply return the "nearest" node to the root not
already in the tree, where "distance" of a path is defined as the negative log
of the product of its arc weights.

A alternative implementation, which judges a tag by all the paths leading to
it, could be to transform the graph to use a flow-based model, and rank tags by
the flow going through them. This doesn't seem too complicated an idea, but we
will leave this to one side for now, and concentrate on the "bigger picture".


However, note also that tags with higher $u_t$ are more likely to have their
search queries succeed, independent of $w_e.u_t$.

TODO

If the first element can be added to the tree immediately, we do so, and call
the method recursively, ignoring the other elements returned. In the other case
(`tgraph`) we need to wait for the object to be retrieved from the network; in
the meantime we can go through the list and add the other elements, applying
the same principle to $t_1$, etc...

#### Weight of new `tgraph`

TODO resync spec with new routing layer architecture

Given
:	- a working map $G_w = \{(g_i, w_i)\}$ of `tgraph`s to their weights
	- a new `tgraph` $g_n \notin \dom G_w$
Return
:	- a weight $w_n$ to assign to $g_n$

MORE note the two different semantic meanings for weight attributes being used
here (relevancy vs reliability) TODO explore this and its potential pitfalls

### Routing

The purpose of the routing layer is to retrieve results and provide these to
the ranking layer. The results set is dynamic, and updates are requested by the
ranking layer when it needs more information.

We have read access to $H_s$ from the [initialisation](#initialisation) layer,
and $\breveT$ from the [meta-routing](#meta-routing) layer.

Additionally, we need to keep track of the indexes we search through. Our state
consists of these mutable objects:

1. $\breveS$
	- a map of `index`s to their sources. This keeps track of how we
	  reached a particular index. If the index is in $H_s$, there will be no
	  source. TODO make this more precise...
	- TODO maybe meta-routing layer needs a similar thing?
2. $\breveH = \{ h \mapsto w \}$
	- a map of `index`s to their scores, initialised with $H_s$.
3. $\breveQ = \{ h \mapsto \{ t \} \}$
	- a map of `index`s to "active" tags.
	- Here, we will distinguish between the "query" for the original tag $t_0$,
	  which this system tries to satisfy, and the simpler operation "lookup",
	  which simply returns the results for a tag in a single `index`.
	- An "active" tag is a tag we intend to lookup the index with. This is
	  obtained from $\breveT$.
		- if $h$ is from $H_s$, select all tags in $\breveT$.
		- otherwise, use $\breveT$ to select all "short" paths from $t$ to
		  $t_0$. TODO clarify this, description already in meta-routing
	- An implementation may choose to merge this with $\breveR$; it's easier
	  for our abstract specification to keep these separate.
4. $\breveR = \{ h \mapsto \{ t \mapsto R_{ht} \} \}$
	- a map of `index`s to tags whose lookups have completed, to the results
	  map $R_{ht} = \{ d \mapsto w \}$ for the lookup $q = (h, t)$.
	- this depends on $\breveQ$.

$\breveH, \breveR$ consists of the full results data provided to the ranking
layer. We want to provide as much information to $\breveR$ as possible. We run
the lookups of $\breveQ$, in some particular [order](#deciding-lookup-order).
We can execute lookups in parallel, and update $\breveR$ with the results.

When all lookups have completed, the ranking layer can request more data. We
can respond to this in two different ways.

Adding an index $h$ to $\breveH$

:	If new indexes were found, we can add these to $\breveH$, $\breveS$.

	TODO update $\breveQ$, $\breveR$. weight for the new index, etc

Request data from the meta-routing layer

:	If there are `tgraph`s in $\breveT$, ask the meta-routing layer to import
	them, and wait for the new data to trickle down to $\breveT$.

	TODO update $\breveQ$, $\breveR$.

#### Deciding lookup order

TODO resync spec with new routing layer architecture

Given
:	- a working tree $T_w$ of tags
	- a map $H_w$ of each $t \in T_w$ to its "working `index`" set
	- a map $H_c$ of each $t \in T_w$ to its "queried `index`" set
Return
:	- an ordered list $[(h_i, t_i)] \subseteq Q_p(T_w, H_w) \setminus Q_p(T_w,
	  H_c)$ of queries to perform

Tags further away from the root are less "related" to the original query, but
searches for them are more likely to succeed (for rare searches), so the
algorithm should proceed by searching near the root, then further up the tree.

MORE same principle about concurrency as mentioned before about tag/tgraphs.

#### Weight of new `index`

TODO

### Ranking

#### Combining results from different queries

TODO resync spec with new routing layer architecture

Given
:	- a working tree $T_w$ of tags
	- a map $H_w$ of each $t \in T_w$ to its "working `index`" set
	- a map $H_c$ of each $t \in T_w$ to its "queried `index`" set
	- a map $R_c$ of each $t \in T_w$ to (a map of each "queried `index`" $h
	  \in H_c[t]$ to its results set)
Return
:	- a results map $R = \{(d_i, w_i)\}$ from documents to their weights




## Optimisation

### Data structures

### Querying an `index` or `tgraph` for presence of a source tag

Bloom filters

### Querying an `index` or `tgraph` for weight of an external target

...


## UI

### Components

### Behaviour


# Design

## Objectives

## Assumptions

### Abstract storage network

Our search system assumes that its data objects are located on some abstract
lower-level storage network. Data objects can always be retrieved, instead of
being stored locally with its maintainer, who may be offline. This simplifies
our design, since it allows us to avoid dealing with the issue of churn by
delegating it to an independent component. [EXTN]

This also forces us to consider a iterative routing algorithm rather than a
recursive one, since data objects are "dumb" and cannot respond to dynamic
queries. Instead, a requestor must retrieve the data, process it themselves,
then pull in further data, etc.

Any distributed storage network that has a global address scheme (such as a
binary-key address space) would be suitable, since pointers to objects can be
represented simply as its address. This includes all Distributed Hash Tables
(DHTs), as well as more complex cryptokey-based addresses such as Freenet's
Signed Subspace Keys network.

### Underlying social network

The initialisation algorithm assumes an abstract lower-level social network
where it can look for friends to query for pointers onto the meta-routing and
routing planes.

The network is modelled as a directed weighted graph; arcs represent a degree
of trust by the source node in the target node, and arc weights are restricted
to the interval $[0,1]$.

This model is sufficiently general for our purposes. Many social networking
websites only recognise mutual friendship; this is a degenerate form of the
model, where all weights are constant and all arcs have a reverse-direction
counterpart.

The meta-routing and routing planes also form what is essentially a social
network - `tgraph`s and `index`es effectively contain pointers to each other,
through their arc targets. However, these also carry semantic information (each
arc has a source tag) and are intrinsic to our search system; by constrast, the
social network used for initialisation carries no semantic information and is
independent of our system. To highlight this difference, we refer to a node on
the social network as an "identity", and arc weights as "identity scores".

MORE talk about churn, virtual social networks (WoT on Freenet's SSK), etc

MORE proxy services

## `tgraph` semantics

The routing algorithm needs some way of inferring which tags (out of a related
set) are more "general" or "specific".

Our first thought was to have arcs represent a relationship from a more
"specific" tag to a more "general" tag. However, this is problematic since:

- The relationship is supposed to be transitive, but our graph can potentially
  contain cycles, especially when pulling in data from several sources. We
  don't want to be tied up devising a complex algorithm to resolve this.
- We potentially would like to traverse from general to specific tags, as well
  as vice versa; and arc targets are the only mechanism of doing this between
  `tgraph`s.^[Within a `tgraph`, arc sources ("reverse" pointers) could in
  theory be calculated quickly, depending on the implementation.]

MORE

The following model addresses these shortcomings.

For a given tag $t$, let $D(t) = \{ d \in D : d \textrm{ tagged with } t \}$.
Define the "saturation" of $t$ to be $\frac{|D(t)|}{|D|}$, and the "relevance"
of $t$ to $t_0$ to be $\frac{|D(t) \cap D(t_0)|}{|D(t)|}$. An equivalent model
is to treat $D$ as a random variable, evenly distributed over all documents in
the network. For a given tag $t$, the saturation of $t$ is equivalent to $P(D
\textrm{ tagged with } t)$, which we shorten to $P(t)$ for convenience.
Likewise, the relevance of $t$ to $t_0$ is equivalent to $P(t_0|t)$.

We then use these to define:

- the weight $u_t$ of a node $t$ to be $P(t)$, its saturation.
- the weight $w_e$ of an arc $e = (t_0, t)$ to be $P(t_0|t)$, the relevance of
  the target tag to the source tag.

These values are straightforward to calculate, so this process can (in theory)
be automated. Note also that given $u_{t_0}$, $u_t$ and $w_e$, the weight of
$e^- = (t, t_0)$ can be derived using Bayes' theorem, as $w_e.u_t/u_{t_0}$. If
the `tgraph` implementation allows quick lookups of an arc both by its source
node and its target node, then it might be sensible to not explicitly define
the weight of the reverse arc.

We can think of `tgraph` $g$ (containing tags $t_i$), as a single tag over all
documents tagged by any $t_i$, and define saturation and relevance accordingly.
That is, $P(g) = P(\bigcup t_i)$ and $P(t_0|g) = P(t_0|\bigcup t_i)$. Note that
these values give no information on how well-connected a `tgraph` is within the
meta-routing network. However, a `tgraph` consists of highly compressed data
on a large section of the storage network, so in practice the vast majority of
our meta-routing needs should be satisfied by a small neighbourhood around our
seed set. Choosing between `tgraph`s probably won't be very important, so for
now we ignore this deficiency.[EXTN]

Now for the analogy to typical navigable networks: a tag $t$ can be thought of
as defining a subnetwork - the set of documents tagged with $t$. $P(t)$ tells
us the size of this subnetwork, and $P(t_0|t)$ tells us the proximity between
two subnetworks.

For example, $P(t)$ roughly corresponds to the CIDR routing prefix used by the
current internet. In ring-topology networks, such as some DHTs, neighbours are
matched by the numerical value of the network address; by contrast, internet
routers update each other with their nearest neighbours; and here, we provide
this information in the form of $P(t_0|t)$.

In routing terms, if our end target is $t_0$, then we want to pick a $t$ such
that $P(t_0 \cap t)$ is maximised - for the above model, this is $u_t.w_e$.

This raises the question of why we don't just define $w_e$ as $P(t_0 \cap t)$,
which would also remove the need to define $u_t$. However, this would destroy
our ability to quickly look up $P(t)$ and $P(t_0|t)$. On a distributed network,
we need to be able to combine data from multiple sources; algorithms to do this
may well need knowledge of the separate values.

## Pointers between data structures

### Identity-object pointers

vs. all-data-in-identity and identity-identity pointers

### Object-object pointers vs identity-object pointers

to decrease the number of identity-object pointers

### Mutable-object pointers vs immutable-object pointers

## Weights

### Zero weight-attributes

The current data structures can only represent the presence of an attribute,
and not its absence. This is a problem: any distributed system has to be able
to deal with incomplete information, but here there is no way to tell if the
absence of an object from a collection is due to incomplete information, or an
explicit rejection based on the semantics of the object attributes. This is
important when (eg.) we combine weights from multiple sources - we need to know
whether others disagree with a recommendation, or merely don't know about it.

We can either require that explicit "zero" weight-attributes are defined for
every item encountered, no matter how insignificant; or we can try to generate
heuristics for resolving this ambiguity, based on other implicit information.

**For a node-map**, the former results in linear data growth in the number
of items (nodes) encountered. This is not too bad, but can become inefficient
if the vast majority of items encountered are assigned to zero weights, and the
algorithms that process the data are designed to ignore them. With regards to
our system, this probably affects `ptable`s more than `tgraph`s, where the
algorithmic usefulness of a tag is largely unrelated to its weight.

Within a single node-map, there is no information that could be used to perform
any heuristic: a node not included in the map has no information relating to it
in the map. However, it may be possible to extract information from other
sources, such as the friends for a `ptable`. These are context-specific and
are explored in the appropriate sections. [TODO add link]

**For an arc-map**, the former approach results in quadratic data growth in the
number of items (nodes) encountered, since we need to define _something_
(either a relationship or its absence) between all _pairs_ of nodes. This is
not scalable; fortunately an arc-map holds more information than a node-map,
which allows for better ambiguity-resolution heuristics.

Since the meaning of $e = (v_s, v_t)$ is fully determined by $\{v_s$, $v_t\}$,
the maintainer has enough information to decide whether to include $e$ in $E$
if it "knows about" both $v_s, v_t$. Roughly, we can represent this notion as
$v \in \rft E$. In other words, if $e \notin \dom E$ but $v_s, v_t \in \rft E$,
then either the maintainer has overlooked the relation between $(v_s, v_t)$, or
there is no such relation (ie. a "zero weight"). The latter is more likely,
since people generally review things before publishing, and the mistakes that
do slip through are fixed over time.

Specific examples of this general principle, as well as applications of it,
are explored further in relevant sections of the [Algorithms](#algorithms)
chapter.

### Negative weight-attributes

A related idea is negative weight-attributes, which would represent a judgement
that the subject is malicious (in some sense), rather than neutral or "useless".
In a network where it's possible to act aggressively towards particular nodes,
these weights could provide information on who to attack (or equivalently, set
up defences against), and in what way.

As it stands, our search system is a "dumb" network of data; the only way of
interacting with other nodes is by publishing data objects for other nodes to
read. We are unable to see other nodes' read requests, so it's impossible to
attack a particular node. In this case then, negative weights are not useful.

There are other complications with negative weights in general. These would be
relevant if our search system is adapted into a form where one *can* perform
targeted acts of aggression.

- If positive weights are taken to be probabilities, it's not obvious what
  negative weights should mean. If the semantics of the data structure are not
  well-defined, the algorithms that use it cannot be, either.
- Depending on the algorithm used to combine weights from multiple sources,
  negative weights may allow the weight system itself to be used as an attack
  vector. (Otherwise, this was already possible with positive-only weights and
  the introduction of negative weights makes no difference.)
- MORE

### Corruption of weights

Scoring systems are susceptible to manipulation... the usual path to corruption
is that original semantics of the system are no longer reflected in algorithms
for calculating and evaluating it. When this happens the system breaks...

(eg. google's "trusted ads" are 2x likely to be corrupt)


# Analysis

## Data extraction and processing

last.fm, flickr

social network - relatively easy

### Transforming a simple graph into a `tgraph`

tgraphs arcs have weight and direction

## Test networks

### Inter-node properties

- neighbour count (ie. degree) distribution
- neighbour semantic relation distribution

### Intra-node properties

- semantic unity (how "related" its tags are)
- semantic specialty (how "general" its tags are)

### Generation algorithms

- Use network formed by extracted data ("real world")
- BarabÃ¡si-Albert model (preferential attachment)
  - scale-free
  - not small-world; according to wikipedia:
    - clustering coefficient is power-law, similar to hierarchical networks
    - small-world networks have constant clustering coefficient
- TODO etc. read up on network theory.
- hierarchies
- other structures?

Ideally we want a single algorithm which takes as input, various parameters for
the properties listed in the previous two sections, and outputs a random graph
with those properties.

## Simulation

### Request models

### Network conditions

- perfect conditions
- random failure
- malicious attacks - under the assumptions of "abstract storage network", only
  attacks vs the entire network can occur on the meta-routing / routing planes.
  attacks vs individuals on the social plane is a separate topic, ignore here
	- attacks vs most well-connected nodes
	- MORE


# Java class definitions

Note: I started doing these, then thought I'd leave it for the coding stage;
Here's what I have so far.


~~~{.java}
	interface NetworkStorageAdapter {

		Index getIndex(ObjectID id);

		TGraph getTGraph(ObjectID id);

	}
~~~


~~~{.java}
	interface SocialNetworkAdapter {

		Map<Identity, IdScore> getFriends(Identity id);

		PTable getPTable(Identity id);

	}
~~~


~~~{.java}
	interface SeedSetGetter {

		/**
		** @param self
		**  	A root identity (typically our own)
		** @return
		**  	A set of seed identities, with corresponding weights
		*/
		Map<Identity, IdScore> getSeedSet(Identity self);

	}
~~~


~~~{.java}
	interface TGraphComposer {

		/**
		** @param tables
		**
		** @return
		**  	A set of mapping targets (which can be a tag or tgraph) and
		**  	their weights; the source of each mapping being the source tag
		**
		*/
		Map<TGraphEntry, EntryWeight> compositeTGraphNode(Map<TGraph, EntryWeight> , Tag source);

	}
~~~


~~~{.java}
	interface PTableComposer {

		/**
		** @param tables
		**  	A set of ptables, each being mapped to its identity's score.
		** @return
		**  	A ptable that combines all of the input information, with
		**  	$\dom p = \bigcup \dom p_i$.
		*/
		PTable composite(Map<PTable, IdScore> tables);

	}
~~~
